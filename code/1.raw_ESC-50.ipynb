{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "esc1.[kfold].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN1t6fKqNtuRqVrTvA1cOLt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0be7646b426149d28ae614b548a46174": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_20968f25f5c545629efd3d196075a9e6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2eb7a8fbf57a44b6a9e54220b025a260",
              "IPY_MODEL_d45ef852734c4d2187128108ab6a7972"
            ]
          }
        },
        "20968f25f5c545629efd3d196075a9e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2eb7a8fbf57a44b6a9e54220b025a260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b54fddb63c214edebd958c17b4001c5c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d4e81ee9936a4596b33afa0f3e2e0b73"
          }
        },
        "d45ef852734c4d2187128108ab6a7972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6b931096539446f49443293b65c87849",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2000/? [1:36:22&lt;00:00,  2.89s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0c8348f413e44f019e8b6903f77028ca"
          }
        },
        "b54fddb63c214edebd958c17b4001c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d4e81ee9936a4596b33afa0f3e2e0b73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b931096539446f49443293b65c87849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0c8348f413e44f019e8b6903f77028ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pasquale90/mthesis/blob/master/code/1.raw_ESC-50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcdRRNhz-kce"
      },
      "source": [
        "#prevent from disconnecting --> to console\n",
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_Hcwsrj4Ql_",
        "outputId": "55d58b73-882e-4e57-a8ad-3c8db39e286b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Import Google_drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p-jLQG8HLbH"
      },
      "source": [
        "#Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms, models\n",
        "from google.colab import files as filez"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6v0exbI-zZd",
        "outputId": "33507206-bcdf-4619-c304-d347294e39d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#define device\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device('cuda:0')\n",
        "else:\n",
        "  device=torch.device('cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4Fd1kr_TDlT",
        "outputId": "e454fb8b-6516-4413-aaf2-c1982583bf9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#define experiment mode\n",
        "class mode_class:\n",
        "  def __init__(self, mode_atr):\n",
        "    if (mode_atr == 8):\n",
        "      self.mode='8'\n",
        "    elif (mode_atr == 16):\n",
        "      self.mode='16'\n",
        "    elif (mode_atr == 22):\n",
        "      self.mode = '22'\n",
        "    elif (mode_atr == 32):\n",
        "      self.mode = '32'\n",
        "    else:\n",
        "      print(f'{mode_atr} input attribute is not valid.Please insert 8 or 16 or 22 or 32')\n",
        "  def get_mode(self):\n",
        "    return self.mode\n",
        "\n",
        "mode_id = 8\n",
        "mode_id = 16\n",
        "mode_id = 22\n",
        "mode_id = 32\n",
        "\n",
        "#define mode\n",
        "mode_instance = mode_class(mode_id)\n",
        "mode=mode_instance.get_mode()\n",
        "print('mode:',mode)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mode: 32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqPgG42f48qR"
      },
      "source": [
        "#define expid\n",
        "expid='esc1'\n",
        "\n",
        "#define paths\n",
        "data_path='/content/gdrive/My Drive/dissertation/ESC-50-master/meta/esc50.csv'\n",
        "audio_path='/content/gdrive/My Drive/dissertation/ESC-50-master/audio/'\n",
        "\n",
        "#model\t\n",
        "model_savepath= \"/content/\"+expid+mode+\"/saved_models/\"\n",
        "\n",
        "#store results\n",
        "results_path=\"/content/\"+expid+mode+\"/results/\"\n",
        "compare_results_path =\"/content/\"+expid+mode+\"/k_fold_results/\"\t\n",
        "\n",
        "#attempt.py\n",
        "attempt_path=\"/content/\"+expid+mode+\"/expattempt/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu0-CErT5JTt",
        "outputId": "a9f524d5-84c9-4925-fe85-6fb4b13218b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Import Dataset\n",
        "esc50 = pd.read_csv(data_path)\n",
        "audiofiles = os.listdir(audio_path)\n",
        "print(esc50.shape)\n",
        "print(len(audiofiles))\n",
        "\n",
        "#store_sorted_class_names, in the same way that are returned from dataset_class in data.py\n",
        "esc_classes = sorted(esc50['category'].unique())\n",
        "num_classes = len(esc_classes)\n",
        "print('num_classes: ',num_classes)\n",
        "\n",
        "folds = sorted(esc50['fold'].unique())\n",
        "print('folds: ',num_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 7)\n",
            "2000\n",
            "num_classes:  50\n",
            "folds:  50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOqjKOI6sHbH",
        "outputId": "bfc17341-40d8-4301-bd83-538d22b675d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#AUDIO AUGMENTATION FUNCTIONS_ synthetic data\n",
        "def audio_augmentation(data, sr, class_conditional, shift_time=True, thresshold=0.5):\n",
        "\n",
        "  #add_white_noise to the signal\n",
        "  def white_noise(data): \n",
        "    noiz = np.random.randn(len(data))\n",
        "    mean_intensity = np.sum(np.square(data))/len(data)\n",
        "    data_wn = data + noiz*0.75* mean_intensity\n",
        "    return data_wn\n",
        "    \n",
        "  #Shift the sound wave by a factor value chosen randomly within [0.5,1,1.5,2] seconds\n",
        "  def time_shift(data,sr):\n",
        "    time_step = np.random.choice([sr//2,sr,sr+sr//2,sr*2])\n",
        "    time_shifted = np.roll(data,time_step)\n",
        "    return time_shifted\n",
        "\n",
        "  #Time-stretching the wave by a factor value of 0.9. Permissible : 0 < x < 1.0\n",
        "  def time_stretch(data):#,factor\n",
        "    factor = 0.90\n",
        "    time_streched = librosa.effects.time_stretch(data,factor)\n",
        "    return time_streched[0:len(data)]\n",
        "\n",
        "  #pitch shifting of wave by a random factor value in the space [-1,1].  Permissible : -5 <= x <= 5\n",
        "  def pitch_shift(data,sr):\n",
        "    detune = np.random.uniform(low=-2.5,high=-1.75,size=None)\n",
        "    overtune = np.random.uniform(low=1.75,high=2.5,size=None)\n",
        "    shift_factor = np.random.choice([detune,overtune])\n",
        "    pitch_shifted = librosa.effects.pitch_shift(data,sr,n_steps=shift_factor)\n",
        "    return pitch_shifted\n",
        "  \n",
        "  def soft_pitch_shift(data,sr):\n",
        "    detune = np.random.uniform(low=-1.0,high=-0.5,size=None)\n",
        "    overtune = np.random.uniform(low=0.5,high=1.0,size=None)\n",
        "    shift_factor = np.random.choice([detune,overtune])\n",
        "    pitch_shifted = librosa.effects.pitch_shift(data,sr,n_steps=shift_factor)\n",
        "    return pitch_shifted\n",
        "\n",
        "  '''\n",
        "  #A rough but very simple segmentation mask function\n",
        "  def envelope(y,rate,threshold):\n",
        "    mask = []\n",
        "    y= pd.Series(y).apply(np.abs)\n",
        "    y_mean = y.rolling(window=int(rate/10), min_periods=1, center=True).mean()#/10 means that win=1/10 sec\n",
        "    for mean in y_mean:\n",
        "      if mean > threshold:#if above the threshold keep\n",
        "        mask.append(True)\n",
        "      else:\n",
        "        mask.append(False)#else drop\n",
        "    return mask\n",
        "  '''\n",
        "  \n",
        "  strong_augs = ['airplane','car_horn','cat',#esc\n",
        "                 'chirping_birds','church_bells',\n",
        "                 'cow','crow','crying_baby',\n",
        "                 'door_wood_creaks','insects',\n",
        "                 'rooster','sheep','siren',\n",
        "                 'car_horn','children_playing','siren']#us8k\n",
        "\n",
        "  medium_augs= ['breathing','brushing_teeth',#esc\n",
        "                'clock_alarm','coughing','dog',\n",
        "                'door_wood_knock','fireworks',\n",
        "                'frog','glass_breaking','hand_saw',\n",
        "                'hen','laughing','pig','pouring_water',\n",
        "                'sneezing','snoring',\n",
        "                'dog_bark','gun_shot','street_music']#us8k\n",
        "                \n",
        "  weak_augs = ['can_opening','chainsaw','clapping',#esc\n",
        "               'clock_tick','crackling_fire','crickets',\n",
        "               'drinking_sipping','engine','footsteps',\n",
        "               'helicopter','keyboard_typing','mouse_click',\n",
        "               'rain','sea_waves','thunderstorm',\n",
        "              'toilet_flush','train','vacuum_cleaner',\n",
        "              'washing_machine','water_drops','wind',  \n",
        "               'air_conditioner','drilling','engine_idling','jackhammer']#us8k\n",
        "   \n",
        "  #prob_wn = np.random.uniform(low=0,high=1)\n",
        "  #if prob_wn>thresshold:\n",
        "  data =  white_noise(data)\n",
        "  \n",
        "  #prob_tsh = np.random.uniform(low=0,high=1)\n",
        "  #if prob_tsh>thresshold:\n",
        "\n",
        "  # if it is padded , don 't shift\n",
        "  if shift_time: \n",
        "    data = time_shift(data,sr)\n",
        "\n",
        "  if class_conditional in strong_augs:\n",
        "    \n",
        "    prob_tst = np.random.uniform(low=0,high=1)\n",
        "    if prob_tst>thresshold:\n",
        "      data = time_stretch(data)\n",
        "\n",
        "    prob_psh = np.random.uniform(low=0,high=1)\n",
        "    if prob_psh>thresshold:\n",
        "      data = pitch_shift(data,sr)\n",
        "  \n",
        "  elif class_conditional in medium_augs:\n",
        "    \n",
        "    prob_spsh = np.random.uniform(low=0,high=1)\n",
        "    if prob_spsh>thresshold:\n",
        "      data = soft_pitch_shift(data,sr)\n",
        "\n",
        "  elif class_conditional in weak_augs:\n",
        "    pass\n",
        "\n",
        "  return data\n",
        "\n",
        "#Νο Vision Augmentations in this exp\n",
        "'''\n",
        "train_transforms = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor()\n",
        "        transforms.RandomErasing(p=0.5, scale = (0.05,0.05), ratio = (0.3,0.33), value=0, inplace=False)      \n",
        "    ])\n",
        "valid_transforms = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntrain_transforms = transforms.Compose([\\n        transforms.RandomHorizontalFlip(),\\n        transforms.ToTensor()\\n        transforms.RandomErasing(p=0.5, scale = (0.05,0.05), ratio = (0.3,0.33), value=0, inplace=False)      \\n    ])\\nvalid_transforms = transforms.Compose([\\n        transforms.ToTensor()\\n    ])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VspFPkEDDoOo"
      },
      "source": [
        "#define analysis parameters\n",
        "def analysis_parameters(mode):\n",
        "    sampling_rate=int(mode)*1000  #for instance: if mode 16, sr = 16kHz\n",
        "    if mode == '16':\n",
        "      window_size = sampling_rate\n",
        "      overlap = int(window_size*0.75)\n",
        "    elif mode == '22':\n",
        "      sampling_rate=22050\n",
        "      window_size = sampling_rate\n",
        "      overlap = int(window_size*0.5)\n",
        "    elif mode == '32':\n",
        "      window_size = sampling_rate\n",
        "      overlap = int(window_size*0.5)\n",
        "    \n",
        "    hop_length = (window_size-overlap)\n",
        "    return sampling_rate, window_size, hop_length\n",
        "'''\n",
        "\n",
        "#in case we need other feature shape\n",
        "def analysis_parameters(mode):\n",
        "  sampling_rate=int(mode)*1000  #for instance: if mode 16, sr = 16kHz\n",
        "  \n",
        "  if mode=='16':\n",
        "    window_size = 512\n",
        "    overlap = int(window_size*0.75)\n",
        "    hop_length = (window_size-overlap)\n",
        "  elif mode=='8':\n",
        "    window_size = 256\n",
        "    overlap = int(window_size*0.5)\n",
        "    hop_length = (window_size-overlap)\n",
        "  return sampling_rate, window_size, hop_length\n",
        "'''\n",
        "\n",
        "def preprocess_data(audio_path, df, folds, files, mode):\n",
        "    \n",
        "  def _windows(data, window_size, hop_length):\n",
        "    start = 0\n",
        "    while start < len(data):\n",
        "      yield start, start + window_size\n",
        "      start += hop_length\n",
        "\n",
        "  def normalize(data):#in the space [-1,1]\n",
        "    #return (data - np.min(data)) / (np.max(data) - np.min(data))#[0,1]\n",
        "    return (2*(data-np.min(data))/(np.max(data) - np.min(data)))-1\n",
        "  \n",
        "  def flatten_features(feature):\n",
        "    feature = feature.T.flatten()\n",
        "    return feature\n",
        "\n",
        "  features, labels, folders = [], [], []  \n",
        " \n",
        "  extr = True\n",
        "  if extr == True:\n",
        "    \n",
        "    print('Preprocessing data ........ ')\n",
        "    \n",
        "    sampling_rate, window_size, hop_length = analysis_parameters(mode)\n",
        "    print(f'sampling_rate: {sampling_rate}, window_size: {window_size}, hop_length: {hop_length}')\n",
        "\n",
        "    shape_print=True\n",
        "\n",
        "    #deterministic random augmentation\n",
        "    np.random.seed(3)\n",
        "\n",
        "    for count_files, file in tqdm(enumerate(audiofiles)):\n",
        "      #label = int(df.loc[df['filename']==file]['target'].to_string(index=False))\n",
        "      #folder = int(df.loc[df['filename']==file]['fold'].to_string(index=False))\n",
        "      name = file.split('.wav')[0]\n",
        "      label = int(file.split('-')[-1].split('.')[0])\n",
        "      folder = int(file.split('-')[0])\n",
        "\n",
        "      #readfile\n",
        "      raw, sr =librosa.load(audio_path+file, sr=sampling_rate, mono=True)\n",
        "      #print(f'{file} had length {len(raw)}')\n",
        "\n",
        "      #normalize\n",
        "      raw = normalize(raw)\n",
        "      \n",
        "      frames = []\n",
        "      #get windows out of the raw waveform\n",
        "      for count_frames,(start,end) in enumerate(_windows(raw,window_size,hop_length)):\n",
        "        if(len(raw[start:end]) == window_size):\n",
        "          #print(start,end)\n",
        "          frame = raw[start:end]#rectangular window\n",
        "\n",
        "          #flatten\n",
        "          #implemented inside the model\n",
        "\n",
        "          frames.append(frame)\n",
        "      \n",
        "      features.append(frames)\n",
        "      labels.append(label)\n",
        "      folders.append(folder)\n",
        "\n",
        "      #Synthetic data augmentations\n",
        "      '''Probability of a file being augmented is 100% so as to increase to the double the size of the ESC dataset,and to prevent from imbalanced class distribution issues\n",
        "      #fprob = np.random.uniform(low=0,high=1)\n",
        "      #if fprob>0.75:\n",
        "      '''\n",
        "      category = df.loc[df['filename']==file]['category'].to_string(index=False).lstrip()\n",
        "      augmented = audio_augmentation(data=raw,sr=sr,class_conditional=category)\n",
        "      synth_frames = []\n",
        "      for count_frames,(start,end) in enumerate(_windows(augmented,window_size,hop_length)):\n",
        "        if(len(augmented[start:end]) == window_size):\n",
        "          #print('AUGMENTED',start,end)\n",
        "          synthetic_frame = augmented[start:end]#rectangular window\n",
        "\n",
        "          synth_frames.append(synthetic_frame)\n",
        "      \n",
        "      features.append(synth_frames)\n",
        "      labels.append(label)\n",
        "      folders.append(folder)\n",
        "\n",
        "      if shape_print:\n",
        "            print('\\nFeature Shape Check\\n')\n",
        "            print(f'raw has len:{len(raw)/sampling_rate}')\n",
        "            print(f'Postprocessed feature has shape : {np.asarray(features).shape} with min:{np.asarray(features[0]).min()} and max:{np.asarray(features[0]).max()}]')\n",
        "            print()\n",
        "            shape_print = False\n",
        "\n",
        "  #'''\n",
        "  print('len(features)-features',len(features))\n",
        "  print('len(features[0])-freq_domain',len(features[0]))\n",
        "  print('labels',len(labels))\n",
        "  print('folders',len(folders))\n",
        "  #'''\n",
        "  return features, labels, folders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47eLWqu7EWQz",
        "outputId": "4acbf5f0-f922-4cf4-ec7a-4c47a30f0417",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361,
          "referenced_widgets": [
            "0be7646b426149d28ae614b548a46174",
            "20968f25f5c545629efd3d196075a9e6",
            "2eb7a8fbf57a44b6a9e54220b025a260",
            "d45ef852734c4d2187128108ab6a7972",
            "b54fddb63c214edebd958c17b4001c5c",
            "d4e81ee9936a4596b33afa0f3e2e0b73",
            "6b931096539446f49443293b65c87849",
            "0c8348f413e44f019e8b6903f77028ca"
          ]
        }
      },
      "source": [
        "#get analysis parameters\n",
        "sampling_rate, window_size, hop_length = analysis_parameters(mode)\n",
        "print(f'sampling_rate: {sampling_rate}, window_size: {window_size}, hop_length: {hop_length}')\n",
        "\n",
        "#extract_features       \n",
        "features, labels, folders = preprocess_data(audio_path, esc50, folds, audiofiles, mode)\n",
        "#features, labels, folders = extract_mel_spectogram(audio_path,us8k,folds,audiofiles,sampling_rate,hop_length,fft_points,mel_bands)\n",
        "print(f' feature\\'s len : {len(features)}, labels : {len(labels)}, folders : {len(folders)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sampling_rate: 32000, window_size: 32000, hop_length: 16000\n",
            "Preprocessing data ........ \n",
            "sampling_rate: 32000, window_size: 32000, hop_length: 16000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:67: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0be7646b426149d28ae614b548a46174",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Feature Shape Check\n",
            "\n",
            "raw has len:5.0\n",
            "Postprocessed feature has shape : (2, 9, 32000) with min:-1.0 and max:1.0]\n",
            "\n",
            "\n",
            "len(features)-features 4000\n",
            "len(features[0])-freq_domain 9\n",
            "labels 4000\n",
            "folders 4000\n",
            " feature's len : 4000, labels : 4000, folders : 4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdW13lLQQQUt",
        "outputId": "07b55798-0121-40fd-b50a-6b50be605071",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#train and fold definition\n",
        "vfold = 1\n",
        "train_folds = [folds.index(fold)+1 for i,fold in enumerate(folds) if vfold!=folds.index(fold)+1]\n",
        "#train_folds = [folds.index(fold)+1 for i,fold in enumerate(folds) if (vfold-1)==folds.index(fold)+1]#for debugging\n",
        "valid_folds = [folds.index(fold)+1 for i,fold in enumerate(folds) if vfold==folds.index(fold)+1]\n",
        "print('train_folds: ',train_folds)\n",
        "print('valid_folds: ',valid_folds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_folds:  [2, 3, 4, 5]\n",
            "valid_folds:  [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Yuk74wwQUQ3"
      },
      "source": [
        "class Data(Dataset): \n",
        "  def __init__(self, features,labels,folders,split):\n",
        "    \n",
        "    def convert_to_tensor(data):\n",
        "      tensors = []\n",
        "      for file in data:\n",
        "        tensor = torch.Tensor(list(file))\n",
        "        tensors.append(tensor)\n",
        "      return tensors\n",
        "\n",
        "    print('Loading_features......')\n",
        "\n",
        "    #features, labels, folders\n",
        "    self.indexes = [i for i, val in enumerate(folders) if val in split]\n",
        "    self.data = [features[x] for x in self.indexes]\n",
        "    self.labels = [labels[x] for x in self.indexes]\n",
        "\n",
        "    #convert numpy to tensor\n",
        "    self.data = convert_to_tensor(self.data)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self, idx):#load data on demand\n",
        "    return self.data[idx], self.labels[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vieLPhYMWIuC",
        "outputId": "0f20fd6a-ac43-4025-aa6f-220cd0af6a74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#load features\n",
        "train_data = Data(features,labels,folders,train_folds)\n",
        "valid_data = Data(features,labels,folders,valid_folds)\n",
        "print('features are loaded')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading_features......\n",
            "Loading_features......\n",
            "features are loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcc0TfCJ5TBK",
        "outputId": "9175bd87-0355-4564-f545-9af12fd4f126",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#define batch size\n",
        "batch_size = 16\n",
        "print(f'batch_size: {batch_size}')\n",
        "\n",
        "#data iterator\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_size: 16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0LjD9hPr-Sw",
        "cellView": "both"
      },
      "source": [
        "class R1Dcnn9(nn.Module):\n",
        "    def __init__(self,num_cats):\n",
        "        super(R1Dcnn9, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(1, 32, kernel_size=64, stride=2)#64\n",
        "        self.bn1 = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 32, kernel_size=64, stride=2)#64\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=8, stride=4) \n",
        "        self.conv3 = nn.Conv1d(32, 64, kernel_size=32, stride=2)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.conv4 = nn.Conv1d(64, 64, kernel_size=32, stride=2)\n",
        "        self.bn4 = nn.BatchNorm1d(64)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=8, stride=2) \n",
        "        self.conv5 = nn.Conv1d(64, 128, kernel_size=16, stride=2) \n",
        "        self.bn5 = nn.BatchNorm1d(128)      \n",
        "        self.conv6 = nn.Conv1d(128, 128, kernel_size=8, stride=2) \n",
        "        self.bn6 = nn.BatchNorm1d(128)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=4, stride=2) \n",
        "        self.conv7 = nn.Conv1d(128, 256, kernel_size=4, stride=2) \n",
        "        self.bn7 = nn.BatchNorm1d(256)\n",
        "        self.conv8 = nn.Conv1d(256, 256, kernel_size=4, stride=2) \n",
        "        self.bn8 = nn.BatchNorm1d(256)\n",
        "        self.pool4 = nn.MaxPool1d(kernel_size=2, stride=2) \n",
        "        self.drop1 = nn.Dropout(0.25)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.ap = nn.AdaptiveAvgPool1d(7)\n",
        "        self.fc1 = nn.Linear(256*7, 512)\n",
        "        self.drop2 = nn.Dropout(0.16)       \n",
        "        self.fc2 = nn.Linear(512, num_cats)\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.view(x.shape[0], 1,-1 )\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(self.bn1(x))\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(self.bn2(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(self.bn3(x))\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(self.bn4(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv5(x)\n",
        "        x = F.relu(self.bn5(x))\n",
        "        x = self.conv6(x)\n",
        "        x = F.relu(self.bn6(x))\n",
        "        x = self.pool3(x)\n",
        "        x = self.conv7(x)\n",
        "        x = F.relu(self.bn7(x))\n",
        "        x = self.conv8(x)\n",
        "        x = F.relu(self.bn8(x))\n",
        "        x = self.pool4(x)\n",
        "        x = self.ap(x)\n",
        "        x = x.view(x.shape[0], x.size(1) * x.size(2))\n",
        "        x = self.relu(self.fc1(self.drop1(x)))\n",
        "        x = self.fc2(self.drop2(x))\n",
        "        x = F.log_softmax(x, dim = 1)\n",
        "        return x    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRnL_cO8xM7Y",
        "outputId": "e7ddf207-3fef-4273-8bf0-beafb2a755d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#introduce reproducibility\n",
        "seed = 70\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "#init model\n",
        "model =  R1Dcnn9(num_cats=num_classes).to(device)#.cuda()#\n",
        "modelid='R1Dcnn9'\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'model_{modelid} initialized with total : {total_params} parameters.')\n",
        "\n",
        "#define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-5\n",
        "epochs = 115\n",
        "print(f' learning_rate = {learning_rate}, epochs = {epochs}')\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_R1Dcnn9 initialized with total : 1866098 parameters.\n",
            " learning_rate = 1e-05, epochs = 115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_KKLMHWxaGX"
      },
      "source": [
        "#define metrics\n",
        "def F1_score(trace_y, trace_yhat, classes):\n",
        "  num_classes = len(classes)\n",
        "  #confussion matrix\n",
        "  confmat = confusion_matrix(trace_y, trace_yhat, num_classes)\n",
        "    \n",
        "  TP=pd.Series(confmat.to_numpy().diagonal())\n",
        "  FP=confmat.sum(axis=0)-TP\n",
        "  FN=confmat.sum(axis=1)-TP\n",
        "\n",
        "  TN=confmat.sum().sum()-TP-FP-FN\n",
        "  \n",
        "  #micro\n",
        "  micro_precision = TP.sum()/(TP.sum()+FP.sum())\n",
        "  micro_recall = TP.sum()/(TP.sum()+FN.sum())\n",
        "  F1_micro=2*micro_precision*micro_recall/(micro_precision+micro_recall)\n",
        "\n",
        "  #macro\n",
        "  macro_precision = pd.Series(np.nan)\n",
        "  macro_recall= pd.Series(np.nan)\n",
        "  macro_f1 = pd.Series(np.nan)\n",
        "  #Avoid Zero-Division\n",
        "  for i in range(num_classes):\n",
        "    if (TP[i]==0 and FP[i]==0):\n",
        "      macro_precision[i]=0\n",
        "    else:\n",
        "      macro_precision[i] = TP[i]/(TP[i]+FP[i]) \n",
        "    if (TP[i]==0 and FN[i]==0):\n",
        "      macro_recall[i]=0\n",
        "    else:\n",
        "      macro_recall[i] = TP[i]/(TP[i]+FN[i])\n",
        "    \n",
        "    if (macro_precision[i]+macro_recall[i]==0.0):\n",
        "      macro_f1[i]=0\n",
        "    else:\n",
        "      macro_f1[i] = 2*macro_precision[i]*macro_recall[i]/(macro_precision[i]+macro_recall[i])\n",
        "      \n",
        "  macro_precision = macro_precision.sum()/num_classes\n",
        "  macro_recall=macro_recall.sum()/num_classes\n",
        "  F1_macro = macro_f1.sum()/num_classes\n",
        "\n",
        "  return micro_recall,micro_precision,F1_micro, macro_recall,macro_precision,F1_macro\n",
        "  \n",
        "#labels, preds\n",
        "def confusion_matrix(trace_y,trace_yhat, num_classes):\n",
        "  confmat=pd.DataFrame(data=np.zeros(shape=(num_classes,num_classes)))\n",
        "  predictions=trace_yhat.argmax(axis=1)\n",
        "\n",
        "  for i,pred in enumerate(predictions):\n",
        "    confmat.iat[trace_y[i],pred]+=1\n",
        "\n",
        "  return confmat\n",
        "\n",
        "\n",
        "def F1_Class(trace_y,trace_yhat,classes):\n",
        "  num_classes = len(classes)\n",
        "  confmat = confusion_matrix(trace_y, trace_yhat, num_classes)\n",
        "  \n",
        "  TP=pd.Series(confmat.to_numpy().diagonal())\n",
        "  FP=confmat.sum(axis=0)-TP\n",
        "  FN=confmat.sum(axis=1)-TP\n",
        "  TN=confmat.sum().sum()-TP-FP-FN\n",
        "\n",
        "  #AVOID ZERO DIVISION\n",
        "  class_precision = pd.Series(np.nan)\n",
        "  class_recall = pd.Series(np.nan)\n",
        "  class_f1 = pd.Series(np.nan)\n",
        "  for i in range(num_classes):\n",
        "    if (TP[i]==0 and FP[i]==0):\n",
        "      class_precision[i]=0\n",
        "    else:\n",
        "      class_precision[i] = TP[i]/(TP[i]+FP[i]) \n",
        "    if (TP[i]==0 and FN[i]==0):\n",
        "      class_recall[i]=0\n",
        "    else:\n",
        "      class_recall[i] = TP[i]/(TP[i]+FN[i])\n",
        "    \n",
        "    if (class_precision[i]==0.0 and class_recall[i]==0.0):\n",
        "      class_f1[i]=0\n",
        "    else:\n",
        "      class_f1[i] = 2*class_precision[i]*class_recall[i]/(class_precision[i]+class_recall[i])\n",
        "    \n",
        "  #create a dict_report\n",
        "  f1_class_report = {}\n",
        "  class_report = {}\n",
        "  class_counts = np.asarray(np.unique(trace_y, return_counts=True)).T\n",
        "\n",
        "  for i,c in enumerate(classes):\n",
        "    f1_class_report[c] = {}\n",
        "    f1_class_report[c]['precision'] = class_precision[i]\n",
        "    f1_class_report[c]['recall'] = class_recall[i]\n",
        "    f1_class_report[c]['f1'] = class_f1[i]\n",
        "    f1_class_report[c]['count'] = class_counts[i][1]\n",
        "\n",
        "  return f1_class_report\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def backup_metrics(labels,preds,classes,path,vfold):\n",
        "    if (not os.path.exists(path)):\n",
        "      os.makedirs(path)\n",
        "    report = classification_report(labels, preds.argmax(1), target_names=classes)\n",
        "    report_data = []\n",
        "    lines = report.split('\\n')\n",
        "    for line in lines[2:-5]:\n",
        "        row = {}\n",
        "        row_data = ' '.join(line.split())   \n",
        "        row_data = row_data.split(' ')\n",
        "        row['class'] = row_data[0]\n",
        "        row['precision'] = float(row_data[1])\n",
        "        row['recall'] = float(row_data[2])\n",
        "        row['f1_score'] = float(row_data[3])\n",
        "        row['support'] = float(row_data[4])\n",
        "        report_data.append(row)\n",
        "    dataframe = pd.DataFrame.from_dict(report_data)\n",
        "    dataframe.to_csv(path+'backup_classification_report_'+str(vfold)+'.csv',index=False)\n",
        "\n",
        "    accuracy= f1_score(labels, preds.argmax(1), average='micro', zero_division='warn')\n",
        "    macro_avg= f1_score(labels, preds.argmax(1), average='macro', zero_division='warn')\n",
        "    weightedf1= f1_score(labels, preds.argmax(1), average='weighted', zero_division='warn')\n",
        "\n",
        "    precision,recall,_,_ =precision_recall_fscore_support(labels, preds.argmax(1), average='macro')\n",
        "    general = pd.DataFrame(data = [accuracy,recall,precision,macro_avg,weightedf1],index=['accuracy','recall','precision','macro_f1','weightedf1'])\n",
        "    general.to_csv(path+'backup_general_'+str(vfold)+'.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqchvVZVp5n7"
      },
      "source": [
        "#prevent overfitting custom method\n",
        "class prevent_overfitting:\n",
        "  def __init__(self):\n",
        "    self.tolerance=7  #how many epochs tolerance\n",
        "    self.minloss = 10 \n",
        "    self.lossenvelope =[]\n",
        "    self.thresshold = 0.4\n",
        "    self.avgperformances=[]\n",
        "    self.best_epoch = 0\n",
        "    self.micro_accuracies = []\n",
        "    self.macro_accuracies = []\n",
        "    self.best_curr_model = False#None #model.state_dict save\n",
        "  \n",
        "  #check overfitting by observing 3 last epoch's valid loss\n",
        "  def detect_overfitting(self,validloss,epoch,console_path):\n",
        "    self.lossenvelope.append(validloss)\n",
        "    if validloss<self.minloss:\n",
        "      self.minloss=validloss\n",
        "    print('minloss',self.minloss)\n",
        "    print(f'minloss : {self.minloss}', file=open(console_path, \"a\"))\n",
        "\n",
        "    if (len(self.lossenvelope)>self.tolerance):\n",
        "      self.lossenvelope.pop(0) #reject first value when {tolerance} values are passed\n",
        "\n",
        "    if(len(self.lossenvelope)>=self.tolerance):\n",
        "      overfit=all(earlier <= later for earlier, later in zip(self.lossenvelope, self.lossenvelope[-5:]))#check last 5 values if ascending\n",
        "      if (overfit and min(self.lossenvelope)>self.minloss):\n",
        "        return True\n",
        "    return False\n",
        "  \n",
        "  #store best model's results by observing mean micro and macro accuracies\n",
        "  def store_best_model(self,micro_accuracy,macro_accuracy,console_path):\n",
        "    self.macro_accuracies.append(micro_accuracy)\n",
        "    self.micro_accuracies.append(macro_accuracy)\n",
        "    \n",
        "    meanaccuracy = (micro_accuracy+macro_accuracy)/2.0\n",
        "    self.avgperformances.append(meanaccuracy)\n",
        "    \n",
        "    self.best_epoch = self.avgperformances.index(max(self.avgperformances))+1\n",
        "    \n",
        "    if meanaccuracy >= self.thresshold:\n",
        "      self.best_curr_model = (meanaccuracy >= max(self.avgperformances))\n",
        "      print(f'just saved the best current model in epoch{self.best_epoch}, with acc1:{self.micro_accuracies[self.best_epoch-1]}, and acc2:{self.macro_accuracies[self.best_epoch-1]}')\n",
        "      print(f'just saved the best current model in epoch{self.best_epoch}, with acc1:{self.micro_accuracies[self.best_epoch-1]}, and acc2:{self.macro_accuracies[self.best_epoch-1]}', file=open(console_path, \"a\"))\n",
        "    else:\n",
        "      self.best_curr_model = False\n",
        "\n",
        "    return self.best_curr_model, self.micro_accuracies[self.best_epoch-1],self.macro_accuracies[self.best_epoch-1]#, self.best_epoch\n",
        "  \n",
        "  def early_stopping(self,epochinst):\n",
        "    print('training is terminating so as to prevent further overfitting')\n",
        "    total_epochs = epochinst.set_total(epochinst.get_step())\n",
        "    return total_epochs\n",
        "\n",
        "class epochs_class:\n",
        "  def __init__(self):\n",
        "    self.total_epochs=50#random value\n",
        "    self.step_epoch=1\n",
        "  def set_total(self,num_epochs):\n",
        "    self.total_epochs=num_epochs\n",
        "    return self.get_total()\n",
        "  def get_total(self):\n",
        "    return self.total_epochs\n",
        "  def next_step(self):\n",
        "    self.step_epoch+=1\n",
        "    return self.step_epoch\n",
        "  def get_step(self):\n",
        "    return self.step_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnnZ4vieqgdF"
      },
      "source": [
        "#save results\n",
        "def save_results(content, name):\n",
        "  path = results_path\n",
        "  if (not os.path.exists(path)):\n",
        "    os.mkdir(path)\n",
        "  ovr_results, class_results = content\n",
        "  ovr_results_filename, class_results_filename = name \n",
        "  \n",
        "  ovr_results.to_csv(path+ovr_results_filename) \n",
        "  class_results.to_csv(path + class_results_filename) \n",
        "   \n",
        "#func to define the last argument in the save_results method\n",
        "#PATTERN:dataset&exp_mode_validationfold_attempt_metrics.csv\n",
        "def define_filenames_pattern(expid, mode, vfold, attempt):\n",
        "  classf1_report_filename = expid+'_'+mode+'_v'+str(vfold)+'_a'+str(attempt)+'_classF1.csv'\n",
        "  ovr_results_filename=expid+'_'+mode+'_v'+str(vfold)+'_a'+str(attempt)+'_overalF1.csv'\n",
        "  return ovr_results_filename , classf1_report_filename\n",
        "  \n",
        "  \n",
        "#as the first argument in save_results\n",
        "def define_content(epochs,\n",
        "\t\t               mean_train_losses, \n",
        "                   mean_valid_losses, \n",
        "                   microrecall,\n",
        "                   microprecision,\n",
        "                   microf1, \n",
        "                   macrorecall,\n",
        "                   macroprecision,\n",
        "                   macrof1, \n",
        "\t\t               classf1):\n",
        "                   \n",
        "  #data = [mean_train_losses, mean_valid_losses, accuracies,micro_auroc, macro_auroc microf1, macrof1]#auroc,\n",
        "  data = {'mean_train_loss' : mean_train_losses, 'mean_valid_loss' : mean_valid_losses, \n",
        "  'micro_recall':microrecall, 'micro_precision': microprecision, 'micro_f1' : microf1,\n",
        "  'macro_recall':macrorecall, 'macro_precision': macroprecision, 'macro_f1' : macrof1}\n",
        "  \n",
        "  #index names\n",
        "  epochs_index= (['epoch_'+str(ep+1) for ep in range(epochs)])\n",
        "  \n",
        "  overal_results = pd.DataFrame(data=data, index=epochs_index,  dtype=np.float16)\n",
        "  class_results = pd.DataFrame(data=classf1, index=epochs_index,  dtype=np.float16)\n",
        "  \n",
        "  overal_results.index.name = 'epochs'\n",
        "  class_results.index.name = 'epochs'\n",
        "  \n",
        "  return overal_results, class_results\n",
        "\n",
        " \n",
        "#save general results so as to compare with other systems in a different folder\n",
        "def save_genres(micro, macro, params, validation_fold, filename):\n",
        "  \n",
        "  path = compare_results_path\n",
        "\n",
        "  #if csv exists, overwrite results\n",
        "  if os.path.isfile(path+filename):\n",
        "    general_results=pd.read_csv(path+filename)\n",
        "    general_results.set_index('validation_fold:',inplace=True)\n",
        "    #debug\n",
        "    print(general_results.shape)\n",
        "    new_fold_results = [micro, macro, params]\n",
        "    #debug\n",
        "    print(len(new_fold_results))\n",
        "    general_results[validation_fold] = new_fold_results\n",
        "    general_results.to_csv(path+filename)\n",
        "  #if not, store them into a new csv file\n",
        "  else:\n",
        "    os.makedirs(path)\n",
        "    data = {validation_fold : [micro,macro,params]}\n",
        "    general_results = pd.DataFrame(data=data, index=['micro_f1','macro_f1','params'], dtype=np.float16)\n",
        "    general_results.index.name = 'validation_fold:'\n",
        "    general_results.to_csv(path+filename) \n",
        "    print(general_results)\n",
        "\n",
        "\n",
        "def genres_filename(expid,mode):\n",
        "  filename=expid+'_'+str(mode)+'.csv'\n",
        "  return filename\n",
        "\n",
        "#save the model\n",
        "#PATTERN:exp-num_mode_dataset_'model'_attempt\t\t\n",
        "def save_model(model_name, vfold, state_dict):\n",
        "  path = model_savepath\n",
        "  if not os.path.exists(path):\n",
        "    os.mkdir(path)\n",
        "  torch.save(state_dict, path + model_name+'_'+str(vfold)+'.pt')\n",
        "'''\n",
        "#load model\n",
        "def load_model(model_name, mode):\n",
        "  if (mode=='80'):\n",
        "    model = esc_mel_model_hybrid(input_shape=(1,80,431), batch_size=16, num_cats=50)\n",
        "  elif(mode=='128'):\n",
        "    model = esc_mel_model_hybrid(input_shape=(1,128,431), batch_size=16, num_cats=50)\n",
        "  model.load_state_dict(torch.load(paths.model_savepath+model_name+'.pth'))\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i1-2uEZqnfd"
      },
      "source": [
        "def ZipAnDownload(mode):\n",
        "  if mode == '16':\n",
        "    !zip -r /content/esc116.zip /content/esc116\n",
        "    filez.download(\"/content/esc116.zip\")\n",
        "  elif mode == '22':\n",
        "    !zip -r /content/esc122.zip /content/esc122\n",
        "    filez.download(\"/content/esc122.zip\")\n",
        "  elif mode == '32':\n",
        "    !zip -r /content/esc132.zip /content/esc132\n",
        "    filez.download(\"/content/esc132.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHTSKm-dXJLz"
      },
      "source": [
        "class attempt_class:\n",
        "  #init_attempt\n",
        "  def __init__(self,mode,vfold):\n",
        "\n",
        "    self.attempt_path = attempt_path\n",
        "    self.attempt_file = attempt_path+\"attempts_\"+mode+\"_\"+str(vfold)+\".txt\"\n",
        "\n",
        "    if (not os.path.exists(self.attempt_path)):\n",
        "      os.makedirs(self.attempt_path)\n",
        "      self.init_files()\n",
        "    elif (not os.path.isfile(self.attempt_file)):\n",
        "      self.init_files()\n",
        "    else:\n",
        "      print(f'attempt is already initialized')\n",
        "      print(f'Experiment\\'s _{mode} attempt no_ : {self.get_attempt()}')\n",
        "  \n",
        "  def init_files(self):\n",
        "    attempt = 1\n",
        "    f = open(self.attempt_file,'w')\n",
        "    with open(self.attempt_file, 'a') as out:\n",
        "      out.write(str(attempt))\n",
        "      print(f'Experiment\\'s attempt no_ : {attempt}')\n",
        "  \n",
        "  def add_attempt(self):\n",
        "    with open(self.attempt_file, \"r\") as f:\n",
        "      attempt = f.read()\n",
        "      attempt = int(attempt)\n",
        "      attempt+=1\n",
        "    with open(self.attempt_file, \"w\") as f:\n",
        "      f.write(str(attempt))\n",
        "    print(f'Experiment\\'s attempt changed to : {attempt}')\n",
        "\n",
        "  def get_attempt(self):\n",
        "    with open(self.attempt_file, \"r\") as f:\n",
        "      attempt = int(f.read())\n",
        "    return attempt\n",
        "\n",
        "  def set_attempt(self,attempt_value):\n",
        "    with open(self.attempt_file, \"w\") as f:\n",
        "      f.write(str(attempt_value))\n",
        "    print(f'Experiment\\'s attempt is set to : {attempt_value}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H7GBpumXMj3"
      },
      "source": [
        "#train_model_func\n",
        "def train(model, loss_fn, train_loader, valid_loader,\n",
        "          epochs, optimizer, learning_rate, device, classes, expid, mode, vfold, modelid):\n",
        "\n",
        "  exp_attempt = attempt_class(mode, vfold)\n",
        "  \n",
        "  print('Train started..')\n",
        "\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  \n",
        "  microrecall = []\n",
        "  macrorecall = []\n",
        "  \n",
        "  microprecision = []\n",
        "  macroprecision = []\n",
        "  \n",
        "  microf1 = []\n",
        "  macrof1 = []\n",
        "  \n",
        "  classf1 = []\n",
        "  \n",
        "  #console print redirect - save output of train to a log_file\n",
        "  console_path = \"/content/\"+expid+mode+'/'+expid+'_'+str(mode)+'_'+str(vfold)+\".txt\"\n",
        "  \n",
        "  epoch_instance = epochs_class()\n",
        "  total_epochs = epoch_instance.set_total(epochs)\n",
        "  epoch = epoch_instance.get_step()\n",
        "\n",
        "  prevent_overfit = prevent_overfitting()\n",
        "  \n",
        "  model_name = expid+'_'+mode+'_v'+str(vfold)+'_a'+str(exp_attempt.get_attempt())+'_'+modelid\n",
        "  \n",
        "  #Train step\n",
        "  while (epoch<=total_epochs):\n",
        "\n",
        "    model.train()\n",
        "    batch_losses=[]\n",
        "\n",
        "    for i,data in tqdm(enumerate(train_loader)):\n",
        "      x, y = data\n",
        "      optimizer.zero_grad()\n",
        "      x = x.to(device, dtype=torch.float32)\n",
        "      y = y.to(device, dtype=torch.long) \n",
        "      y_hat = model(x) \n",
        "      \n",
        "      \n",
        "      loss = loss_fn(y_hat, y)\n",
        "      loss.backward()\n",
        "      \n",
        "      batch_losses.append(loss.item())\t\t\t\t\t\t\n",
        "      optimizer.step()\n",
        "    train_losses.append(batch_losses)\n",
        "    mean_train_losses=([np.mean(l) for l in train_losses])\n",
        "    \n",
        "    print()\n",
        "    print(f'Epoch - {epoch} Train-Loss : {np.mean(train_losses[-1])}')\n",
        "    print(f'\\nEpoch - {epoch} Train-Loss : {np.mean(train_losses[-1])}\\n', file=open(console_path, \"a\"))\n",
        "    print()\n",
        "    \n",
        "    #Validation step\n",
        "    model.eval()\n",
        "     \n",
        "    batch_losses=[]\n",
        "    trace_y = []\n",
        "    trace_yhat = []\n",
        "    \n",
        "    for i, data in enumerate(valid_loader):\n",
        "      x, y = data\n",
        "      x = x.to(device, dtype=torch.float32)\n",
        "      y = y.to(device, dtype=torch.long)\n",
        "      y_hat = model(x)\n",
        "      loss = loss_fn(y_hat, y)\n",
        "      trace_y.append(y.cpu().detach().numpy())\n",
        "      trace_yhat.append(y_hat.cpu().detach().numpy())      \n",
        "      batch_losses.append(loss.item())\n",
        "    valid_losses.append(batch_losses)\n",
        "    mean_valid_losses=([np.mean(l) for l in valid_losses])\n",
        "    \n",
        "    trace_y = np.concatenate(trace_y)\n",
        "    trace_yhat = np.concatenate(trace_yhat)\n",
        "\n",
        "    #f1,micro,macro\n",
        "    micro_recall,micro_precision,micro, macro_recall,macro_precision,macro = F1_score(trace_y,trace_yhat,classes)\n",
        "    \n",
        "    microrecall.append(micro_recall)\n",
        "    microprecision.append(micro_precision)\n",
        "    microf1.append(micro) \n",
        "\n",
        "    macrorecall.append(macro_recall)\n",
        "    macroprecision.append(macro_precision)\n",
        "    macrof1.append(macro)\n",
        "    \n",
        "    #f1 for each class\n",
        "    f1_class = F1_Class(trace_y,trace_yhat,classes)\n",
        "    classf1.append(f1_class)\n",
        "\n",
        "    #console prints\n",
        "    print(f'Epoch - {epoch} Valid-Loss : {np.mean(valid_losses[-1])}')\n",
        "    print(f'micro_f1_{epoch} = {micro} ')\n",
        "    print(f'macro_f1_{epoch} = {macro} ')\n",
        "    #append log_file\n",
        "    print(f'Epoch - {epoch} Valid-Loss : {np.mean(valid_losses[-1])}', file=open(console_path, \"a\"))\n",
        "    print(f'micro_f1_{epoch} = {micro} ', file=open(console_path, \"a\"))\n",
        "    print(f'macro_f1_{epoch} = {macro} ', file=open(console_path, \"a\"))\n",
        "    '''\n",
        "    #check if overfitting and reduce lr twice, after that, in case loss is ascending terminate train    \n",
        "    early_stop, optimizer = overfit.detect_overfitting(np.mean(valid_losses[-1]),optimizer, learning_rate, epoch_instance)\n",
        "    if early_stop:\n",
        "      total_epochs=epoch_instance.set_total(epoch_instance.get_step())\n",
        "    epoch=epoch_instance.next_step()\n",
        "    '''\n",
        "    #check if overfitting and reduce lr twice, after that, in case loss is ascending terminate train    \n",
        "    early_stop = prevent_overfit.detect_overfitting(np.mean(valid_losses[-1]), epoch, console_path)\n",
        "    if early_stop:\n",
        "      total_epochs=prevent_overfit.early_stopping(epoch_instance)\n",
        "    \n",
        "    #always store the best according to avg_micro_and_macro_F1\n",
        "    current_best, best_micro, best_macro = prevent_overfit.store_best_model(micro,macro,console_path)#, best_epoch\n",
        "    #If achieves current best mean accuracy, Save the model\n",
        "    if current_best:\n",
        "      save_model(model_name, vfold, model.state_dict())\n",
        "      backup_metrics(trace_y,trace_yhat,classes,results_path+'backup/')\n",
        "    \n",
        "    #next epoch\n",
        "    epoch=epoch_instance.next_step()\n",
        "  \n",
        "  #calc_trainable_params\n",
        "  #params=sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "  params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "      \n",
        "  #Save analytic results \n",
        "  save_results(define_content(total_epochs,\n",
        "\t\t                                     mean_train_losses,\n",
        "                                         mean_valid_losses, \n",
        "                                         microrecall,\n",
        "                                         microprecision,\n",
        "                                         microf1, \n",
        "                                         macrorecall,\n",
        "                                         macroprecision,\n",
        "                                         macrof1,\n",
        "\t\t                                     classf1),\n",
        "                      define_filenames_pattern(expid, mode, vfold, exp_attempt.get_attempt()))\n",
        "  \n",
        "      \n",
        "  #Save general results so as to quickly compare systems\n",
        "  genresfilename = genres_filename(expid,mode)\n",
        "  save_genres(round(best_micro,3), round(best_macro,3), params, vfold, genresfilename)\n",
        "  print('params =', params)\n",
        "  print('params =', params, file=open(console_path, \"a\"))\n",
        "\n",
        "  exp_attempt.add_attempt()\n",
        "  ZipAnDownload(mode)  \n",
        "  return  train_losses,valid_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOYIMR9CX4yQ"
      },
      "source": [
        "#train single-fold\n",
        "train_losses,valid_losses = train(model, loss_fn, train_loader, valid_loader, epochs, optimizer, learning_rate, device, esc_classes, expid, mode, vfold, modelid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DkHUwgmAAGk"
      },
      "source": [
        "if mode =='8':\n",
        "  !rm -r esc18 esc18.zip\n",
        "if mode =='16':\n",
        "  !rm -r esc116 esc116.zip\n",
        "if mode =='22':\n",
        "  !rm -r esc122 esc122.zip\n",
        "if mode =='32':\n",
        "  !rm -r esc132 esc132.zip"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}