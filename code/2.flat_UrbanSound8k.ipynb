{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "us8k2[kfold].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMRFApDn7U3WZtBezZNmOdk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pasquale90/mthesis/blob/master/code/2.flat_UrbanSound8k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ytm7uEhUlgt"
      },
      "source": [
        "!pip uninstall librosa\n",
        "!pip install librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHpKUrcgNqT_"
      },
      "source": [
        "#prevent from disconnecting --> to console\n",
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te91VG5RQ_Cp"
      },
      "source": [
        "#Import Google_drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNri9FNoRLMd"
      },
      "source": [
        "#Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms, models\n",
        "from google.colab import files as filez"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN1TsYQfWQV9",
        "outputId": "5c0ad28b-7699-4332-97b9-7e64375a02f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#define device\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device('cuda:0')\n",
        "else:\n",
        "  device=torch.device('cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5cFRYobRXP4",
        "outputId": "c818f78f-bcfc-4f8c-9a8b-70e476b6f9b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#define experiment mode\n",
        "class mode_class:\n",
        "  def __init__(self, mode_atr):\n",
        "    if (mode_atr == 80):\n",
        "      self.mode='80'\n",
        "    elif (mode_atr == 128):\n",
        "      self.mode = '128'\n",
        "    elif (mode_atr == 360):\n",
        "      self.mode = '360'\n",
        "    else:\n",
        "      print(f'{mode_atr} input attribute is not valid.Please insert 80 or 128 or even 360')\n",
        "  def get_mode(self):\n",
        "    return self.mode\n",
        "\n",
        "mode_id = 80\n",
        "#mode_id = 128\n",
        "#mode_id = 360\n",
        "\n",
        "#define mode\n",
        "mode_instance = mode_class(mode_id)\n",
        "mode=mode_instance.get_mode()\n",
        "print(mode)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVv3xCXipppZ"
      },
      "source": [
        "#define expid\n",
        "expid='us2'\n",
        "\n",
        "#define paths\n",
        "data_path='/content/gdrive/My Drive/dissertation/UrbanSound8K/metadata/UrbanSound8K.csv'\n",
        "audio_path='/content/gdrive/My Drive/dissertation/UrbanSound8K/audio/'\n",
        "\n",
        "#model.py\t\t\n",
        "model_savepath= \"/content/\"+expid+mode+\"/saved_models/\"\n",
        "\n",
        "#store.py\n",
        "results_path=\"/content/\"+expid+mode+\"/results/\"\n",
        "compare_results_path =\"/content/\"+expid+mode+\"/k_fold_results/\"\t\n",
        "\n",
        "#attempt.py\n",
        "attempt_path=\"/content/\"+expid+mode+\"/expattempt/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbqqMgrFsYsH"
      },
      "source": [
        "#read us8k folds method\n",
        "def read_folds(audio_path):\n",
        "  folds=os.listdir(audio_path)\n",
        "  \n",
        "  order={}\n",
        "  files={}\n",
        "  filesum=0\n",
        "  \n",
        "  for f in folds:\n",
        "  \n",
        "    filelist = os.listdir(audio_path+f+'/')\n",
        "   \n",
        "    if (f[-1:]!='0'):\n",
        "      num=f[-1:]\n",
        "    elif(f[-1:]=='0'):#get the value of 10 instead of 0\n",
        "      num=f[-2:]\n",
        "  \n",
        "    order[f]=int(num)\n",
        "    files[f]=filelist\n",
        "\n",
        "    filesum+=len(files[f])\n",
        "  \n",
        "  #folds ordered\n",
        "  folds={}\n",
        "  for key, value in sorted(order.items(), key=lambda item: item[1]):\n",
        "    folds[value]= key\n",
        "  folds=list(folds.values())\n",
        "  \n",
        "  return folds, files, order, filesum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEazv5VURdjc"
      },
      "source": [
        "#Import Dataset\n",
        "us8k = pd.read_csv(data_path)\n",
        "print(us8k.shape)\n",
        "\n",
        "folds, audiofiles, order, filesum = read_folds(audio_path)\n",
        "for f in folds:\n",
        "    print('fold no_%d contains %d audiofiles'%(order[f],len(audiofiles[f]))) \n",
        "print('All in all there are %d audio files found in 8k Urban Sound dataset folders'%filesum)\n",
        "\n",
        "print(us8k.columns)\n",
        "us8k.rename(columns={'class':'Class'}, inplace=True)\n",
        "print('\\n\\ncolumn <class> became... <%s>'%us8k.columns[-1])\n",
        "\n",
        "#store_sorted_classes in the same way as defined in data.py\n",
        "us8k_classes = sorted(us8k['Class'].unique())\n",
        "num_classes = len(us8k_classes)\n",
        "print('num_classes: ',num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKfc5OrPu2Sm",
        "outputId": "a7476251-022e-4474-9aa6-3bfbcdc86657",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#AUDIO AUGMENTATION FUNCTIONS_ synthetic data\n",
        "def audio_augmentation(data, sr, class_conditional, shift_time, thresshold=0.5):\n",
        "\n",
        "  #add_white_noise to the signal\n",
        "  def white_noise(data): \n",
        "    noiz = np.random.randn(len(data))\n",
        "    mean_intensity = np.sum(np.square(data))/len(data)\n",
        "    data_wn = data + noiz*0.75* mean_intensity\n",
        "    return data_wn\n",
        "    \n",
        "  #Shift the sound wave by a factor value chosen randomly within [0.5,1,1.5,2] seconds\n",
        "  def time_shift(data,sr):\n",
        "    time_step = np.random.choice([sr//2,sr,sr+sr//2,sr*2])\n",
        "    time_shifted = np.roll(data,time_step)\n",
        "    return time_shifted\n",
        "\n",
        "  #Time-stretching the wave by a factor value of 0.9. Permissible : 0 < x < 1.0\n",
        "  def time_stretch(data):#,factor\n",
        "    factor = 0.90\n",
        "    time_streched = librosa.effects.time_stretch(data,factor)\n",
        "    return time_streched[0:len(data)]\n",
        "\n",
        "  #pitch shifting of wave by a random factor value in the space [-1,1].  Permissible : -5 <= x <= 5\n",
        "  def pitch_shift(data,sr):\n",
        "    detune = np.random.uniform(low=-2.5,high=-1.75,size=None)\n",
        "    overtune = np.random.uniform(low=1.75,high=2.5,size=None)\n",
        "    shift_factor = np.random.choice([detune,overtune])\n",
        "    pitch_shifted = librosa.effects.pitch_shift(data,sr,n_steps=shift_factor)\n",
        "    return pitch_shifted\n",
        "  \n",
        "  def soft_pitch_shift(data,sr):\n",
        "    detune = np.random.uniform(low=-1.0,high=-0.5,size=None)\n",
        "    overtune = np.random.uniform(low=0.5,high=1.0,size=None)\n",
        "    shift_factor = np.random.choice([detune,overtune])\n",
        "    pitch_shifted = librosa.effects.pitch_shift(data,sr,n_steps=shift_factor)\n",
        "    return pitch_shifted\n",
        "\n",
        "  '''\n",
        "  #A rough but very simple segmentation mask function\n",
        "  def envelope(y,rate,threshold):\n",
        "    mask = []\n",
        "    y= pd.Series(y).apply(np.abs)\n",
        "    y_mean = y.rolling(window=int(rate/10), min_periods=1, center=True).mean()#/10 means that win=1/10 sec\n",
        "    for mean in y_mean:\n",
        "      if mean > threshold:#if above the threshold keep\n",
        "        mask.append(True)\n",
        "      else:\n",
        "        mask.append(False)#else drop\n",
        "    return mask\n",
        "  '''\n",
        "  \n",
        "  strong_augs = ['airplane','car_horn','cat',#esc\n",
        "                 'chirping_birds','church_bells',\n",
        "                 'cow','crow','crying_baby',\n",
        "                 'door_wood_creaks','insects',\n",
        "                 'rooster','sheep','siren',\n",
        "                 'car_horn','children_playing','siren']#us8k\n",
        "\n",
        "  medium_augs= ['breathing','brushing_teeth',#esc\n",
        "                'clock_alarm','coughing','dog',\n",
        "                'door_wood_knock','fireworks',\n",
        "                'frog','glass_breaking','hand_saw',\n",
        "                'hen','laughing','pig','pouring_water',\n",
        "                'sneezing','snoring',\n",
        "                'dog_bark','gun_shot','street_music']#us8k\n",
        "                \n",
        "  weak_augs = ['can_opening','chainsaw','clapping',#esc\n",
        "               'clock_tick','crackling_fire','crickets',\n",
        "               'drinking_sipping','engine','footsteps',\n",
        "               'helicopter','keyboard_typing','mouse_click',\n",
        "               'rain','sea_waves','thunderstorm',\n",
        "              'toilet_flush','train','vacuum_cleaner',\n",
        "              'washing_machine','water_drops','wind',  \n",
        "               'air_conditioner','drilling','engine_idling','jackhammer']#us8k\n",
        "   \n",
        "  #prob_wn = np.random.uniform(low=0,high=1)\n",
        "  #if prob_wn>thresshold:\n",
        "  data =  white_noise(data)\n",
        "  \n",
        "  #prob_tsh = np.random.uniform(low=0,high=1)\n",
        "  #if prob_tsh>thresshold:\n",
        "\n",
        "  # if it is padded , don 't shift\n",
        "  if shift_time: \n",
        "    data = time_shift(data,sr)\n",
        "\n",
        "  if class_conditional in strong_augs:\n",
        "    \n",
        "    prob_tst = np.random.uniform(low=0,high=1)\n",
        "    if prob_tst>thresshold:\n",
        "      data = time_stretch(data)\n",
        "\n",
        "    prob_psh = np.random.uniform(low=0,high=1)\n",
        "    if prob_psh>thresshold:\n",
        "      data = pitch_shift(data,sr)\n",
        "  \n",
        "  elif class_conditional in medium_augs:\n",
        "    \n",
        "    prob_spsh = np.random.uniform(low=0,high=1)\n",
        "    if prob_spsh>thresshold:\n",
        "      data = soft_pitch_shift(data,sr)\n",
        "\n",
        "  elif class_conditional in weak_augs:\n",
        "    pass\n",
        "\n",
        "  return data\n",
        "\n",
        "#Νο Vision Augmentations in this exp\n",
        "'''\n",
        "train_transforms = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor()\n",
        "        transforms.RandomErasing(p=0.5, scale = (0.05,0.05), ratio = (0.3,0.33), value=0, inplace=False)      \n",
        "    ])\n",
        "valid_transforms = transforms.Compose([\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ntrain_transforms = transforms.Compose([\\n        transforms.RandomHorizontalFlip(),\\n        transforms.ToTensor()\\n        transforms.RandomErasing(p=0.5, scale = (0.05,0.05), ratio = (0.3,0.33), value=0, inplace=False)      \\n    ])\\nvalid_transforms = transforms.Compose([\\n        transforms.ToTensor()\\n    ])\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2O158qkubLb"
      },
      "source": [
        "#define analysis parameters\n",
        "def analysis_parameters(mode):\n",
        "  sampling_rate=44100\n",
        "  hop_length=512\n",
        "  fft_points=2048\n",
        "  mel_bands=mode  #80x321 or 128x321\n",
        "  return sampling_rate, hop_length, fft_points, mel_bands\n",
        "  \n",
        "def zero_pad(signal,fs):\n",
        "  shift_time = True\n",
        "  if len(signal)>(4*fs):\n",
        "    signal = signal[0:4*fs]\n",
        "  elif len(signal)<(4*fs):\n",
        "    shift_time = False\n",
        "  num_zeros=4*fs-len(signal)\n",
        "  zp=np.zeros(num_zeros,dtype=float)\n",
        "  padded_signal = np.concatenate((signal,zp),axis=0)\n",
        "  return padded_signal, shift_time\n",
        "\n",
        "#extract features\n",
        "def extract_mel_spectogram(audio_path, df, folds, audiofiles, sr, hop, nfft, nmels):   \n",
        "\n",
        "  def compute_mel_spectogram(raw,sr,hop,nfft,nmels,window='hann'):\n",
        "    S=librosa.feature.melspectrogram(y=raw.astype(float),\n",
        "                                      sr=sr,S=None,\n",
        "                                      n_fft=nfft,\n",
        "                                      hop_length=hop,\n",
        "                                      window=window, \n",
        "                                      power=2,\n",
        "                                      n_mels=nmels) \n",
        "    S = librosa.power_to_db(S, ref=np.max)\n",
        "    return S\n",
        "\n",
        "  def scale_image(spec, eps=1e-6):\n",
        "    mean = spec.mean()\n",
        "    std = spec.std()\n",
        "    spec_norm = (spec - mean) / (std + eps)\n",
        "    spec_min, spec_max = spec_norm.min(), spec_norm.max()\n",
        "    spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n",
        "    spec_scaled = spec_scaled.astype(np.uint8)\n",
        "    return spec_scaled\n",
        "\n",
        "  def flatten_features(feature):\n",
        "    feature = feature.T.flatten()\n",
        "    return feature\n",
        "\n",
        "  features, labels, folders = [], [], []  \n",
        " \n",
        "  extr = True\n",
        "  if extr == True:\n",
        "\n",
        "    print('Extracting features ........ ')\n",
        "    \n",
        "    shape_print = True\n",
        "    pad_print = True\n",
        "\n",
        "    #deterministic random augmentation\n",
        "    np.random.seed(77)\n",
        "\n",
        "    for foldname, filesinfold in audiofiles.items():\n",
        "      \n",
        "     #if foldname=='fold9' or foldname=='fold10':#test\n",
        "\n",
        "      path = audio_path+foldname+'/'\n",
        "\n",
        "      for file in tqdm(filesinfold):\t\t\t\t\t\t#[0:200]: test\n",
        "        name = file.split('.wav')[0]\n",
        "        label = np.int8(file.split('-')[1])\n",
        "        folder = np.int8(folds.index(foldname)+1)\n",
        "\n",
        "        raw,_ = librosa.load(path+file, sr=sr, mono=True)\n",
        "        #zero pad signal to 5 seconds\n",
        "        padded, shift_time = zero_pad(raw,sr)\n",
        "        #extract mel spectogram\n",
        "        S = compute_mel_spectogram(padded,sr,hop,nfft,nmels)\n",
        "        #flip image\n",
        "        flipped = np.flipud(S)\n",
        "        #to gray scale\n",
        "        greyscale = scale_image(flipped)\n",
        "        #flatten features\n",
        "        flattened = flatten_features(greyscale)\n",
        "      \n",
        "        features.append(flattened)\n",
        "        labels.append(label)\n",
        "        folders.append(folder)\n",
        "\n",
        "        #Synthetic data augmentations\n",
        "        prob = np.random.uniform(low=0,high=1)\n",
        "        if prob<=1:#100% of the files\n",
        "          category = df.loc[df['slice_file_name']==file]['Class'].to_string(index=False).lstrip()\n",
        "          augmented = audio_augmentation(data=padded,sr=sr,class_conditional=category,shift_time=shift_time,thresshold=0.5)\n",
        "          synthetic = scale_image(np.flipud(compute_mel_spectogram(augmented,sr,hop,nfft,nmels)))\n",
        "          flatsynth = flatten_features(synthetic)\n",
        "\n",
        "          features.append(flatsynth)\n",
        "          labels.append(label)\n",
        "          folders.append(folder)\n",
        "\n",
        "        #test_shapes of raw data and feature representation\n",
        "        if shape_print:\n",
        "          print('\\nFeature Shape Check\\n')\n",
        "          print(f'raw had len:{len(raw)/sr}, and padded has len:{len(padded)/sr}')\n",
        "          print(f'Spectogram has shape : {flattened.shape} with min:{flattened.min()} and max:{flattened.max()}]')\n",
        "          shape_print = False\n",
        "        if (not shift_time) and pad_print:\n",
        "          print('\\nPadded Feature Shape Check\\n')\n",
        "          print(f'raw had len:{len(raw)/sr}, and padded has len:{len(padded)/sr}')\n",
        "          print(f'Spectogram has shape : {flatsynth.shape} with min:{flatsynth.min()} and max:{flatsynth.max()}')\n",
        "          pad_print = False\n",
        "\n",
        "  '''\n",
        "  print('len(features)-features',len(features))\n",
        "  print('len(features[0])-freq_domain',len(features[0]))\n",
        "  print('labels',len(labels))\n",
        "  print('folders',len(folders))\n",
        "  '''\n",
        "  \n",
        "  print('Features are extracted!')\n",
        "\n",
        "  return features, labels, folders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM31BwEDvExz"
      },
      "source": [
        "#get analysis parameters\n",
        "sampling_rate, hop_length, fft_points, mel_bands = analysis_parameters(mode)\n",
        "print(f'sampling_rate: {sampling_rate}, hop_length: {hop_length}, fft_points: {fft_points}, mel_bands: {mel_bands}')\n",
        "\n",
        "#extract_features\n",
        "features, labels, folders = extract_mel_spectogram(audio_path,us8k,folds,audiofiles,sampling_rate,hop_length,fft_points,mel_bands)\n",
        "print(f' feature\\'s len : {len(features)}, labels : {len(labels)}, folders : {len(folders)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-w227wJuTNO"
      },
      "source": [
        "#train and fold definition\n",
        "vfold = 10\n",
        "train_folds = [folds.index(fold)+1 for i,fold in enumerate(folds) if vfold!=folds.index(fold)+1]\n",
        "#train_folds = [folds.index(fold)+1 for i,fold in enumerate(folds) if (vfold-1)==folds.index(fold)+1]#for testing\n",
        "valid_folds = [folds.index(fold)+1 for i,fold in enumerate(folds) if vfold==folds.index(fold)+1]\n",
        "print('train_folds: ',train_folds)\n",
        "print('valid_folds: ',valid_folds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u6cJWE9vakP",
        "outputId": "4a35a6a4-8816-497d-c495-538e9a07801c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "class Data(Dataset): \n",
        "  def __init__(self, features,labels,folders,split):\n",
        "    \n",
        "    def convert_to_tensor(data,transform):\n",
        "      tensors = []\n",
        "      for file in data:\n",
        "        tensor = torch.Tensor(list(file))\n",
        "        tensors.append(tensor)\n",
        "      return tensors\n",
        "\n",
        "    print('Loading_features......')\n",
        "\n",
        "    #features, labels, folders\n",
        "    self.indexes = [i for i, val in enumerate(folders) if val in split]\n",
        "    self.data = [features[x] for x in self.indexes]\n",
        "    self.labels = [labels[x] for x in self.indexes]\n",
        "\n",
        "    #normalize\n",
        "    self.data = np.asarray(self.data, dtype=np.float32)/255.0\n",
        "    print(type(self.data))\n",
        "    print(self.data.shape)\n",
        "\n",
        "    #convert numpy to tensor\n",
        "    self.data = convert_to_tensor(self.data,transforms)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self, idx):#load data on demand\n",
        "    return self.data[idx], self.labels[idx]\n",
        "\n",
        "  \n",
        "\n",
        "#load features\n",
        "train_data = Data(features,labels,folders,train_folds)\n",
        "valid_data = Data(features,labels,folders,valid_folds)\n",
        "print('features are loaded')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading_features......\n",
            "<class 'numpy.ndarray'>\n",
            "(15790, 27600)\n",
            "Loading_features......\n",
            "<class 'numpy.ndarray'>\n",
            "(1674, 27600)\n",
            "features are loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEO5zNhwveab"
      },
      "source": [
        "batch_size = 16\t\n",
        "print(f'batch_size: {batch_size}')\n",
        "\n",
        "#data iterator\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_AArTfAx-3W"
      },
      "source": [
        "class Fcnn7(nn.Module):\n",
        "    def __init__(self, num_cats):\n",
        "        super(Fcnn7, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(1, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv1d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(4, stride=3))\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU())\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv1d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(4, stride=3))\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv1d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU())\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv1d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(4,stride=3))\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv1d(256, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU())\n",
        "        self.conv8 = nn.Sequential(\n",
        "            nn.Conv1d(512, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(4,stride=3))\n",
        "        self.ap = nn.AdaptiveMaxPool1d(4)\n",
        "        self.fc = nn.Linear(512*4,512) \n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.fc2 = nn.Linear(512,512)\n",
        "        self.fc3 = nn.Linear(512,num_cats)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], 1,-1 )\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.conv5(x) \n",
        "        x = self.conv6(x)        \n",
        "        x = self.conv7(x)\n",
        "        x = self.conv8(x)\n",
        "        x = self.ap(x)\n",
        "        x = x.view(x.shape[0], x.size(1) * x.size(2))\n",
        "        x = self.dropout(F.relu(self.fc(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhK_zA9_yLxO",
        "outputId": "a9118f3a-d9dd-4bdd-f6e9-82b2fe214357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#introduce reproducibility\n",
        "seed = 700\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "#init model\n",
        "#model =  Fcnn1(num_cats=num_classes).cuda()#to(device)\n",
        "model =  Fcnn7(num_cats=num_classes).to(device)\n",
        "modelid='Fcnn7'\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'model_{modelid} initialized with total : {total_params} parameters.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model_Fcnn7 initialized with total : 2883402 parameters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hCULJGmCnqu"
      },
      "source": [
        "#define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 1e-5\n",
        "epochs = 34#30\n",
        "print(f' learning_rate = {learning_rate}, epochs = {epochs}')\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPcC5tRB0xkJ"
      },
      "source": [
        "#define metrics\n",
        "def F1_score(trace_y, trace_yhat, classes):\n",
        "  num_classes = len(classes)\n",
        "  #confussion matrix\n",
        "  confmat = confusion_matrix(trace_y, trace_yhat, num_classes)\n",
        "    \n",
        "  TP=pd.Series(confmat.to_numpy().diagonal())\n",
        "  FP=confmat.sum(axis=0)-TP\n",
        "  FN=confmat.sum(axis=1)-TP\n",
        "\n",
        "  TN=confmat.sum().sum()-TP-FP-FN\n",
        "  \n",
        "  #micro\n",
        "  micro_precision = TP.sum()/(TP.sum()+FP.sum())\n",
        "  micro_recall = TP.sum()/(TP.sum()+FN.sum())\n",
        "  F1_micro=2*micro_precision*micro_recall/(micro_precision+micro_recall)\n",
        "\n",
        "  #macro\n",
        "  macro_precision = pd.Series(np.nan)\n",
        "  macro_recall= pd.Series(np.nan)\n",
        "  macro_f1 = pd.Series(np.nan)\n",
        "  #Avoid Zero-Division\n",
        "  for i in range(num_classes):\n",
        "    if (TP[i]==0 and FP[i]==0):\n",
        "      macro_precision[i]=0\n",
        "    else:\n",
        "      macro_precision[i] = TP[i]/(TP[i]+FP[i]) \n",
        "    if (TP[i]==0 and FN[i]==0):\n",
        "      macro_recall[i]=0\n",
        "    else:\n",
        "      macro_recall[i] = TP[i]/(TP[i]+FN[i])\n",
        "    \n",
        "    if (macro_precision[i]+macro_recall[i]==0.0):\n",
        "      macro_f1[i]=0\n",
        "    else:\n",
        "      macro_f1[i] = 2*macro_precision[i]*macro_recall[i]/(macro_precision[i]+macro_recall[i])\n",
        "      \n",
        "  macro_precision = macro_precision.sum()/num_classes\n",
        "  macro_recall=macro_recall.sum()/num_classes\n",
        "  F1_macro = macro_f1.sum()/num_classes\n",
        "\n",
        "  return micro_recall,micro_precision,F1_micro, macro_recall,macro_precision,F1_macro\n",
        "  \n",
        "#labels, preds\n",
        "def confusion_matrix(trace_y,trace_yhat, num_classes):\n",
        "  confmat=pd.DataFrame(data=np.zeros(shape=(num_classes,num_classes)))\n",
        "  predictions=trace_yhat.argmax(axis=1)\n",
        "\n",
        "  for i,pred in enumerate(predictions):\n",
        "    confmat.iat[trace_y[i],pred]+=1\n",
        "\n",
        "  return confmat\n",
        "\n",
        "\n",
        "def F1_Class(trace_y,trace_yhat,classes):\n",
        "  num_classes = len(classes)\n",
        "  confmat = confusion_matrix(trace_y, trace_yhat, num_classes)\n",
        "  \n",
        "  TP=pd.Series(confmat.to_numpy().diagonal())\n",
        "  FP=confmat.sum(axis=0)-TP\n",
        "  FN=confmat.sum(axis=1)-TP\n",
        "  TN=confmat.sum().sum()-TP-FP-FN\n",
        "\n",
        "  #AVOID ZERO DIVISION\n",
        "  class_precision = pd.Series(np.nan)\n",
        "  class_recall = pd.Series(np.nan)\n",
        "  class_f1 = pd.Series(np.nan)\n",
        "  for i in range(num_classes):\n",
        "    if (TP[i]==0 and FP[i]==0):\n",
        "      class_precision[i]=0\n",
        "    else:\n",
        "      class_precision[i] = TP[i]/(TP[i]+FP[i]) \n",
        "    if (TP[i]==0 and FN[i]==0):\n",
        "      class_recall[i]=0\n",
        "    else:\n",
        "      class_recall[i] = TP[i]/(TP[i]+FN[i])\n",
        "    \n",
        "    if (class_precision[i]==0.0 and class_recall[i]==0.0):\n",
        "      class_f1[i]=0\n",
        "    else:\n",
        "      class_f1[i] = 2*class_precision[i]*class_recall[i]/(class_precision[i]+class_recall[i])\n",
        "    \n",
        "  #create a dict_report\n",
        "  f1_class_report = {}\n",
        "  class_report = {}\n",
        "  class_counts = np.asarray(np.unique(trace_y, return_counts=True)).T\n",
        "\n",
        "  for i,c in enumerate(classes):\n",
        "    f1_class_report[c] = {}\n",
        "    f1_class_report[c]['precision'] = class_precision[i]\n",
        "    f1_class_report[c]['recall'] = class_recall[i]\n",
        "    f1_class_report[c]['f1'] = class_f1[i]\n",
        "    f1_class_report[c]['count'] = class_counts[i][1]\n",
        "\n",
        "  return f1_class_report\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "def backup_metrics(labels,preds,classes,path):\n",
        "    if (not os.path.exists(path)):\n",
        "      os.makedirs(path)\n",
        "    report = classification_report(labels, preds.argmax(1), target_names=classes)\n",
        "    report_data = []\n",
        "    lines = report.split('\\n')\n",
        "    for line in lines[2:-5]:\n",
        "        row = {}\n",
        "        row_data = ' '.join(line.split())   \n",
        "        row_data = row_data.split(' ')\n",
        "        row['class'] = row_data[0]\n",
        "        row['precision'] = float(row_data[1])\n",
        "        row['recall'] = float(row_data[2])\n",
        "        row['f1_score'] = float(row_data[3])\n",
        "        row['support'] = float(row_data[4])\n",
        "        report_data.append(row)\n",
        "    dataframe = pd.DataFrame.from_dict(report_data)\n",
        "    dataframe.to_csv(path+'backup_classification_report.csv',index=False)\n",
        "\n",
        "    accuracy= f1_score(labels, preds.argmax(1), average='micro', zero_division='warn')\n",
        "    macro_avg= f1_score(labels, preds.argmax(1), average='macro', zero_division='warn')\n",
        "    weightedf1= f1_score(labels, preds.argmax(1), average='weighted', zero_division='warn')\n",
        "\n",
        "    precision,recall,_,_ =precision_recall_fscore_support(labels, preds.argmax(1), average='macro')\n",
        "    general = pd.DataFrame(data = [accuracy,recall,precision,macro_avg,weightedf1],index=['accuracy','recall','precision','macro_f1','weightedf1'])\n",
        "    general.to_csv(path+'backup_general.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nt_nMNG1II_"
      },
      "source": [
        "#prevent overfitting custom method\n",
        "class prevent_overfitting:\n",
        "  def __init__(self):\n",
        "    self.tolerance=5  #how many epochs tolerance\n",
        "    self.minloss = 10 \n",
        "    self.lossenvelope =[]\n",
        "    self.thresshold = 0.5\n",
        "    self.avgperformances=[]\n",
        "    self.best_epoch = 0\n",
        "    self.micro_accuracies = []\n",
        "    self.macro_accuracies = []\n",
        "    self.best_curr_model = False#None #model.state_dict save\n",
        "  \n",
        "  #check overfitting by observing 3 last epoch's valid loss\n",
        "  def detect_overfitting(self,validloss,epoch,console_path):\n",
        "    self.lossenvelope.append(validloss)\n",
        "    if validloss<self.minloss:\n",
        "      self.minloss=validloss\n",
        "    print('minloss',self.minloss)\n",
        "    print('minloss',self.minloss,file=open(console_path, \"a\"))\n",
        "\n",
        "    if (len(self.lossenvelope)>self.tolerance):\n",
        "      self.lossenvelope.pop(0) #reject first value when {tolerance} values are passed\n",
        "\n",
        "    if(len(self.lossenvelope)>=self.tolerance):\n",
        "      overfit=all(earlier <= later for earlier, later in zip(self.lossenvelope, self.lossenvelope[-3:]))#check if descending\n",
        "      if (overfit and min(self.lossenvelope)>self.minloss):\n",
        "        return True\n",
        "    return False\n",
        "  \n",
        "  #store best model's results by observing mean micro and macro accuracies\n",
        "  def store_best_model(self,micro_accuracy,macro_accuracy,console_path):\n",
        "    self.macro_accuracies.append(micro_accuracy)\n",
        "    self.micro_accuracies.append(macro_accuracy)\n",
        "    \n",
        "    meanaccuracy = (micro_accuracy+macro_accuracy)/2.0\n",
        "    self.avgperformances.append(meanaccuracy)\n",
        "    \n",
        "    self.best_epoch = self.avgperformances.index(max(self.avgperformances))+1\n",
        "    \n",
        "    if meanaccuracy >= self.thresshold:\n",
        "      self.best_curr_model = (meanaccuracy >= max(self.avgperformances))\n",
        "      print(f'just saved the best current model in epoch{self.best_epoch}, with acc1:{self.micro_accuracies[self.best_epoch-1]}, and acc2:{self.macro_accuracies[self.best_epoch-1]}')\n",
        "      print(f'just saved the best current model in epoch{self.best_epoch}, with acc1:{self.micro_accuracies[self.best_epoch-1]}, and acc2:{self.macro_accuracies[self.best_epoch-1]}', file=open(console_path, \"a\"))\n",
        "    else:\n",
        "      self.best_curr_model = False\n",
        "\n",
        "    return self.best_curr_model, self.micro_accuracies[self.best_epoch-1],self.macro_accuracies[self.best_epoch-1]#, self.best_epoch\n",
        "  \n",
        "  def early_stopping(self,epochinst):\n",
        "    print('training is terminating so as to prevent further overfitting')\n",
        "    total_epochs = epochinst.set_total(epochinst.get_step())\n",
        "    return total_epochs\n",
        "\n",
        "class epochs_class:\n",
        "  def __init__(self):\n",
        "    self.total_epochs=50#random value\n",
        "    self.step_epoch=1\n",
        "  def set_total(self,num_epochs):\n",
        "    self.total_epochs=num_epochs\n",
        "    return self.get_total()\n",
        "  def get_total(self):\n",
        "    return self.total_epochs\n",
        "  def next_step(self):\n",
        "    self.step_epoch+=1\n",
        "    return self.step_epoch\n",
        "  def get_step(self):\n",
        "    return self.step_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgHWTSWO1fEE"
      },
      "source": [
        "#save results\n",
        "def save_results(content, name):\n",
        "  path = results_path\n",
        "  if (not os.path.exists(path)):\n",
        "    os.mkdir(path)\n",
        "  ovr_results, class_results = content\n",
        "  ovr_results_filename, class_results_filename = name \n",
        "  \n",
        "  ovr_results.to_csv(path+ovr_results_filename) \n",
        "  class_results.to_csv(path + class_results_filename) \n",
        "   \n",
        "#func to define the last argument in the save_results method\n",
        "#PATTERN:dataset&exp_mode_validationfold_attempt_metrics.csv\n",
        "def define_filenames_pattern(expid, mode, vfold, attempt):\n",
        "  classf1_report_filename = expid+'_'+mode+'_v'+str(vfold)+'_a'+str(attempt)+'_classF1.csv'\n",
        "  ovr_results_filename=expid+'_'+mode+'_v'+str(vfold)+'_a'+str(attempt)+'_overalF1.csv'\n",
        "  return ovr_results_filename , classf1_report_filename\n",
        "  \n",
        "  \n",
        "#as the first argument in save_results\n",
        "def define_content(epochs,\n",
        "\t\t               mean_train_losses, \n",
        "                   mean_valid_losses, \n",
        "                   microrecall,\n",
        "                   microprecision,\n",
        "                   microf1, \n",
        "                   macrorecall,\n",
        "                   macroprecision,\n",
        "                   macrof1, \n",
        "\t\t               classf1):\n",
        "                   \n",
        "  #data = [mean_train_losses, mean_valid_losses, accuracies,micro_auroc, macro_auroc microf1, macrof1]#auroc,\n",
        "  data = {'mean_train_loss' : mean_train_losses, 'mean_valid_loss' : mean_valid_losses, \n",
        "  'micro_recall':microrecall, 'micro_precision': microprecision, 'micro_f1' : microf1,\n",
        "  'macro_recall':macrorecall, 'macro_precision': macroprecision, 'macro_f1' : macrof1}\n",
        "  \n",
        "  #index names\n",
        "  epochs_index= (['epoch_'+str(ep+1) for ep in range(epochs)])\n",
        "  \n",
        "  overal_results = pd.DataFrame(data=data, index=epochs_index,  dtype=np.float16)\n",
        "  class_results = pd.DataFrame(data=classf1, index=epochs_index,  dtype=np.float16)\n",
        "  \n",
        "  overal_results.index.name = 'epochs'\n",
        "  class_results.index.name = 'epochs'\n",
        "  \n",
        "  return overal_results, class_results\n",
        "\n",
        " \n",
        "#save general results so as to compare with other systems in a different folder\n",
        "def save_genres(micro, macro, params, validation_fold, filename):\n",
        "  \n",
        "  path = compare_results_path\n",
        "\n",
        "  #if csv exists, overwrite results\n",
        "  if os.path.isfile(path+filename):\n",
        "    general_results=pd.read_csv(path+filename)\n",
        "    general_results.set_index('validation_fold:',inplace=True)\n",
        "    #debug\n",
        "    print(general_results.shape)\n",
        "    new_fold_results = [micro, macro, params]\n",
        "    #debug\n",
        "    print(len(new_fold_results))\n",
        "    general_results[validation_fold] = new_fold_results\n",
        "    general_results.to_csv(path+filename)\n",
        "  #if not, store them into a new csv file\n",
        "  else:\n",
        "    os.makedirs(path)\n",
        "    data = {validation_fold : [micro,macro,params]}\n",
        "    general_results = pd.DataFrame(data=data, index=['micro_f1','macro_f1','params'], dtype=np.float16)\n",
        "    general_results.index.name = 'validation_fold:'\n",
        "    general_results.to_csv(path+filename) \n",
        "    print(general_results)\n",
        "\n",
        "\n",
        "def genres_filename(expid,mode):\n",
        "  filename=expid+'_'+str(mode)+'.csv'\n",
        "  return filename\n",
        "\n",
        "#save the model\n",
        "#PATTERN:exp-num_mode_dataset_'model'_attempt\t\t\n",
        "def save_model(model_name, vfold, state_dict):\n",
        "  path = model_savepath\n",
        "  if not os.path.exists(path):\n",
        "    os.mkdir(path)\n",
        "  torch.save(state_dict, path + model_name+'_'+str(vfold)+'.pt')\n",
        "'''\n",
        "#load model\n",
        "def load_model(model_name, mode):\n",
        "  if (mode=='80'):\n",
        "    model = esc_mel_model_hybrid(input_shape=(1,80,431), batch_size=16, num_cats=50)\n",
        "  elif(mode=='128'):\n",
        "    model = esc_mel_model_hybrid(input_shape=(1,128,431), batch_size=16, num_cats=50)\n",
        "  model.load_state_dict(torch.load(paths.model_savepath+model_name+'.pth'))\n",
        "'''\n",
        "\n",
        "def ZipAnDownload(mode):\n",
        "  if mode == '80':\n",
        "    !zip -r /content/us280.zip /content/us280\n",
        "    filez.download(\"/content/us280.zip\")\n",
        "  elif mode == '128':\n",
        "    !zip -r /content/us2128.zip /content/us2128\n",
        "    filez.download(\"/content/us2128.zip\")\n",
        "  elif mode == '360':\n",
        "    !zip -r /content/us2360.zip /content/us2360\n",
        "    filez.download(\"/content/us2360.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9IsuPcN0ERQ"
      },
      "source": [
        "class attempt_class:\n",
        "  #init_attempt\n",
        "  def __init__(self,mode,vfold):\n",
        "\n",
        "    self.attempt_path = attempt_path\n",
        "    self.attempt_file = attempt_path+\"attempts_\"+mode+\"_\"+str(vfold)+\".txt\"\n",
        "\n",
        "    if (not os.path.exists(self.attempt_path)):\n",
        "      os.makedirs(self.attempt_path)\n",
        "      self.init_files()\n",
        "    elif (not os.path.isfile(self.attempt_file)):\n",
        "      self.init_files()\n",
        "    else:\n",
        "      print(f'attempt is already initialized')\n",
        "      print(f'Experiment\\'s _{mode} attempt no_ : {self.get_attempt()}')\n",
        "  \n",
        "  def init_files(self):\n",
        "    attempt = 1\n",
        "    f = open(self.attempt_file,'w')\n",
        "    with open(self.attempt_file, 'a') as out:\n",
        "      out.write(str(attempt))\n",
        "      print(f'Experiment\\'s attempt no_ : {attempt}')\n",
        "  \n",
        "  def add_attempt(self):\n",
        "    with open(self.attempt_file, \"r\") as f:\n",
        "      attempt = f.read()\n",
        "      attempt = int(attempt)\n",
        "      attempt+=1\n",
        "    with open(self.attempt_file, \"w\") as f:\n",
        "      f.write(str(attempt))\n",
        "    print(f'Experiment\\'s attempt changed to : {attempt}')\n",
        "\n",
        "  def get_attempt(self):\n",
        "    with open(self.attempt_file, \"r\") as f:\n",
        "      attempt = int(f.read())\n",
        "    return attempt\n",
        "\n",
        "  def set_attempt(self,attempt_value):\n",
        "    with open(self.attempt_file, \"w\") as f:\n",
        "      f.write(str(attempt_value))\n",
        "    print(f'Experiment\\'s attempt is set to : {attempt_value}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oc5mj2Jy55J"
      },
      "source": [
        "#train_model_func\n",
        "def train(model, loss_fn, train_loader, valid_loader,\n",
        "          epochs, optimizer, learning_rate, device, classes, expid, mode, vfold, modelid):\n",
        "\n",
        "  exp_attempt = attempt_class(mode, vfold)\n",
        "  \n",
        "  print('Train started..')\n",
        "\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  \n",
        "  microrecall = []\n",
        "  macrorecall = []\n",
        "  \n",
        "  microprecision = []\n",
        "  macroprecision = []\n",
        "  \n",
        "  microf1 = []\n",
        "  macrof1 = []\n",
        "  \n",
        "  classf1 = []\n",
        "  \n",
        "  #console print redirect - save output of train to a log_file\n",
        "  console_path = \"/content/\"+expid+mode+'/'+expid+'_'+str(mode)+'_'+str(vfold)+\".txt\"\n",
        "  \n",
        "  epoch_instance = epochs_class()\n",
        "  total_epochs = epoch_instance.set_total(epochs)\n",
        "  epoch = epoch_instance.get_step()\n",
        "\n",
        "  prevent_overfit = prevent_overfitting()\n",
        "  \n",
        "  model_name = expid+'_'+mode+'_v'+str(vfold)+'_a'+str(exp_attempt.get_attempt())+'_'+modelid\n",
        "  \n",
        "  while (epoch<=total_epochs):\n",
        "\n",
        "    model.train()\n",
        "    batch_losses=[]\n",
        "\n",
        "    for i,data in tqdm(enumerate(train_loader)):\n",
        "      x, y = data\n",
        "      optimizer.zero_grad()\n",
        "      x = x.to(device, dtype=torch.float32)\n",
        "      y = y.to(device, dtype=torch.long) \n",
        "      y_hat = model(x) \n",
        "      loss = loss_fn(y_hat, y)\n",
        "      loss.backward()\n",
        "      batch_losses.append(loss.item())\t\t\t\t\t\t\n",
        "      optimizer.step()\n",
        "    train_losses.append(batch_losses)\n",
        "    mean_train_losses=([np.mean(l) for l in train_losses])\n",
        "    \n",
        "    print()\n",
        "    print(f'Epoch - {epoch} Train-Loss : {np.mean(train_losses[-1])}')\n",
        "    print(f'\\nEpoch - {epoch} Train-Loss : {np.mean(train_losses[-1])}\\n', file=open(console_path, \"a\"))\n",
        "    print()\n",
        "    \n",
        "    #Validation step\n",
        "    model.eval()\n",
        "     \n",
        "    batch_losses=[]\n",
        "    trace_y = []\n",
        "    trace_yhat = []\n",
        "    \n",
        "    for i, data in enumerate(valid_loader):\n",
        "      x, y = data\n",
        "      x = x.to(device, dtype=torch.float32)\n",
        "      y = y.to(device, dtype=torch.long)\n",
        "      y_hat = model(x)\n",
        "      loss = loss_fn(y_hat, y)\n",
        "      trace_y.append(y.cpu().detach().numpy())\n",
        "      trace_yhat.append(y_hat.cpu().detach().numpy())      \n",
        "      batch_losses.append(loss.item())\n",
        "    valid_losses.append(batch_losses)\n",
        "    mean_valid_losses=([np.mean(l) for l in valid_losses])\n",
        "    \n",
        "    trace_y = np.concatenate(trace_y)\n",
        "    trace_yhat = np.concatenate(trace_yhat)\n",
        "\n",
        "    #f1,micro,macro\n",
        "    micro_recall,micro_precision,micro, macro_recall,macro_precision,macro = F1_score(trace_y,trace_yhat,classes)\n",
        "    \n",
        "    microrecall.append(micro_recall)\n",
        "    microprecision.append(micro_precision)\n",
        "    microf1.append(micro) \n",
        "\n",
        "    macrorecall.append(macro_recall)\n",
        "    macroprecision.append(macro_precision)\n",
        "    macrof1.append(macro)\n",
        "    \n",
        "    #f1 for each class\n",
        "    f1_class = F1_Class(trace_y,trace_yhat,classes)\n",
        "    classf1.append(f1_class)\n",
        "\n",
        "    #console prints\n",
        "    print(f'Epoch - {epoch} Valid-Loss : {np.mean(valid_losses[-1])}')\n",
        "    print(f'micro_f1_{epoch} = {micro} ')\n",
        "    print(f'macro_f1_{epoch} = {macro} ')\n",
        "    #append log_file\n",
        "    print(f'Epoch - {epoch} Valid-Loss : {np.mean(valid_losses[-1])}', file=open(console_path, \"a\"))\n",
        "    print(f'micro_f1_{epoch} = {micro} ', file=open(console_path, \"a\"))\n",
        "    print(f'macro_f1_{epoch} = {macro} ', file=open(console_path, \"a\"))\n",
        "    '''\n",
        "    #check if overfitting and reduce lr twice, after that, in case loss is ascending terminate train    \n",
        "    early_stop, optimizer = overfit.detect_overfitting(np.mean(valid_losses[-1]),optimizer, learning_rate, epoch_instance)\n",
        "    if early_stop:\n",
        "      total_epochs=epoch_instance.set_total(epoch_instance.get_step())\n",
        "    epoch=epoch_instance.next_step()\n",
        "    '''\n",
        "    #check if overfitting and reduce lr twice, after that, in case loss is ascending terminate train    \n",
        "    early_stop = prevent_overfit.detect_overfitting(np.mean(valid_losses[-1]), epoch, console_path)\n",
        "    if early_stop:\n",
        "      total_epochs=prevent_overfit.early_stopping(epoch_instance)\n",
        "    \n",
        "    #always store the best according to avg_micro_and_macro_F1\n",
        "    current_best, best_micro, best_macro = prevent_overfit.store_best_model(micro,macro,console_path)#, best_epoch\n",
        "    #If achieves current best mean accuracy, Save the model\n",
        "    if current_best:\n",
        "      save_model(model_name, vfold, model.state_dict())\n",
        "      backup_metrics(trace_y,trace_yhat,classes,results_path+'backup/')\n",
        "    \n",
        "    #next epoch\n",
        "    epoch=epoch_instance.next_step()\n",
        "  \n",
        "  #calc_trainable_params\n",
        "  #params=sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "  params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "      \n",
        "  #Save analytic results \n",
        "  save_results(define_content(total_epochs,\n",
        "\t\t                          mean_train_losses,\n",
        "                              mean_valid_losses, \n",
        "                              microrecall,\n",
        "                              microprecision,\n",
        "                              microf1, \n",
        "                              macrorecall,\n",
        "                              macroprecision,\n",
        "                              macrof1,\n",
        "\t\t                          classf1),\n",
        "                      define_filenames_pattern(expid, mode, vfold, exp_attempt.get_attempt()))\n",
        "  \n",
        "      \n",
        "  #Save general results so as to quickly compare systems\n",
        "  genresfilename = genres_filename(expid,mode)\n",
        "  save_genres(round(best_micro,3), round(best_macro,3), params, vfold, genresfilename)\n",
        "  print('params =', params)\n",
        "  print('params =', params, file=open(console_path, \"a\") )\n",
        "  \n",
        "  #Save the model\n",
        "  #model_name = expid+'_'+mode+'_v'+str(vfold)+'_a'+str(exp_attempt.get_attempt())+'_'+modelid\n",
        "  #save_model(model_name, model.state_dict())\n",
        "\n",
        "  exp_attempt.add_attempt()\n",
        "  ZipAnDownload(mode) \n",
        "  return  train_losses,valid_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-6EZKe3Rcrc"
      },
      "source": [
        "#single fold train\n",
        "train_losses,valid_losses = train(model, loss_fn, train_loader, valid_loader, epochs, optimizer, learning_rate, device, us8k_classes, expid, mode, vfold, modelid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTpyxfwwObcD"
      },
      "source": [
        "if mode =='80':\n",
        "  !rm -r us280 us280.zip\n",
        "elif mode =='128':\n",
        "  !rm -r us2128 us2128.zip\n",
        "elif mode =='360':\n",
        "  !rm -r esc2360 esc2360.zip"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}