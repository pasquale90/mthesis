/mnt/scratch_a/users/m/melissap/melEnv/lib/python3.8/site-packages/torchvision/transforms/functional.py:92: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729061180/work/torch/csrc/utils/tensor_numpy.cpp:141.)
  img = torch.from_numpy(np.array(pic, np.float32, copy=False))
/mnt/scratch_a/users/m/melissap/melEnv/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
360
dataset's shape :  (2000, 7)
audio_files length : 2000
num_classes:  50
folds :  [1, 2, 3, 4, 5]
sampling_rate: 44100, hop_length: 512, fft_points: 2048, mel_bands: 360
Extracting features ........ 

Feature Shape Check

Spectogram has shape : (360, 431) with min:0 and max:255
Features are extracted!
 feature's len : 4000, labels : 4000, folders : 4000
device :  cuda:0
train_folds:  [2, 3, 4, 5]
valid_fold:  [1]
Loading_features......
Loading_features......
features are loaded
batch_size: 16
model_M2Dcnn2 initialized with total : 3343378 parameters.
 learning_rate = 1e-05, epochs = 135
 epochs = 135
Experiment's attempt no_ : 1
Train started..

Epoch - 1 Train-Loss : 3.8311913120746612

Epoch - 1 Valid-Loss : 3.7016036462783815
micro_f1_1 = 0.11125 
macro_f1_1 = 0.06857271264354768 
minloss 3.7016036462783815

Epoch - 2 Train-Loss : 3.627883882522583

Epoch - 2 Valid-Loss : 3.490609712600708
micro_f1_2 = 0.1725 
macro_f1_2 = 0.11797164713962725 
minloss 3.490609712600708

Epoch - 3 Train-Loss : 3.4240064775943755

Epoch - 3 Valid-Loss : 3.285228900909424
micro_f1_3 = 0.19875 
macro_f1_3 = 0.13593833196234764 
minloss 3.285228900909424

Epoch - 4 Train-Loss : 3.2192365503311158

Epoch - 4 Valid-Loss : 3.085072021484375
micro_f1_4 = 0.25125 
macro_f1_4 = 0.20067178534467195 
minloss 3.085072021484375

Epoch - 5 Train-Loss : 3.0380100870132445

Epoch - 5 Valid-Loss : 2.908855800628662
micro_f1_5 = 0.28375 
macro_f1_5 = 0.22212125266135732 
minloss 2.908855800628662

Epoch - 6 Train-Loss : 2.8741708827018737

Epoch - 6 Valid-Loss : 2.7735224533081055
micro_f1_6 = 0.3175 
macro_f1_6 = 0.27400309246517823 
minloss 2.7735224533081055

Epoch - 7 Train-Loss : 2.7421582198143004

Epoch - 7 Valid-Loss : 2.626751925945282
micro_f1_7 = 0.3525 
macro_f1_7 = 0.3121444453014836 
minloss 2.626751925945282

Epoch - 8 Train-Loss : 2.60033727645874

Epoch - 8 Valid-Loss : 2.4975462412834166
micro_f1_8 = 0.4000000000000001 
macro_f1_8 = 0.36162020750659396 
minloss 2.4975462412834166

Epoch - 9 Train-Loss : 2.5028952944278715

Epoch - 9 Valid-Loss : 2.388033993244171
micro_f1_9 = 0.4000000000000001 
macro_f1_9 = 0.3655582402047351 
minloss 2.388033993244171

Epoch - 10 Train-Loss : 2.39436385512352

Epoch - 10 Valid-Loss : 2.320916051864624
micro_f1_10 = 0.4125 
macro_f1_10 = 0.386764179168136 
minloss 2.320916051864624

Epoch - 11 Train-Loss : 2.3163892674446105

Epoch - 11 Valid-Loss : 2.1927437806129455
micro_f1_11 = 0.4425 
macro_f1_11 = 0.4172723834959983 
minloss 2.1927437806129455

Epoch - 12 Train-Loss : 2.19812489926815

Epoch - 12 Valid-Loss : 2.140212314128876
micro_f1_12 = 0.47125 
macro_f1_12 = 0.44165981132625504 
minloss 2.140212314128876

Epoch - 13 Train-Loss : 2.1434117102622987

Epoch - 13 Valid-Loss : 2.081933126449585
micro_f1_13 = 0.46375 
macro_f1_13 = 0.43112118917264247 
minloss 2.081933126449585

Epoch - 14 Train-Loss : 2.046586534380913

Epoch - 14 Valid-Loss : 1.9845443630218507
micro_f1_14 = 0.47625 
macro_f1_14 = 0.45679792702023647 
minloss 1.9845443630218507

Epoch - 15 Train-Loss : 1.9936922705173492

Epoch - 15 Valid-Loss : 2.020735385417938
micro_f1_15 = 0.48 
macro_f1_15 = 0.44519567470180227 
minloss 1.9845443630218507

Epoch - 16 Train-Loss : 1.9141859233379364

Epoch - 16 Valid-Loss : 1.9267507362365723
micro_f1_16 = 0.48875 
macro_f1_16 = 0.45955056247595644 
minloss 1.9267507362365723

Epoch - 17 Train-Loss : 1.886222664117813

Epoch - 17 Valid-Loss : 1.8798130941390991
micro_f1_17 = 0.49875 
macro_f1_17 = 0.46914067142086036 
minloss 1.8798130941390991

Epoch - 18 Train-Loss : 1.8087677264213562

Epoch - 18 Valid-Loss : 1.8227849555015565
micro_f1_18 = 0.5025 
macro_f1_18 = 0.47818321941336905 
minloss 1.8227849555015565

Epoch - 19 Train-Loss : 1.7583380150794983

Epoch - 19 Valid-Loss : 1.7747760581970216
micro_f1_19 = 0.51625 
macro_f1_19 = 0.4949590864815025 
minloss 1.7747760581970216
just saved the best current model in epoch19, with acc1:0.4949590864815025, and acc2:0.51625

Epoch - 20 Train-Loss : 1.7002814245223998

Epoch - 20 Valid-Loss : 1.739806433916092
micro_f1_20 = 0.51375 
macro_f1_20 = 0.4916050200356664 
minloss 1.739806433916092
just saved the best current model in epoch19, with acc1:0.4949590864815025, and acc2:0.51625

Epoch - 21 Train-Loss : 1.6554892176389695

Epoch - 21 Valid-Loss : 1.684166020154953
micro_f1_21 = 0.52625 
macro_f1_21 = 0.5108371155066554 
minloss 1.684166020154953
just saved the best current model in epoch21, with acc1:0.5108371155066554, and acc2:0.52625

Epoch - 22 Train-Loss : 1.6260689777135848

Epoch - 22 Valid-Loss : 1.6696930873394011
micro_f1_22 = 0.5275 
macro_f1_22 = 0.5050755715304123 
minloss 1.6696930873394011
just saved the best current model in epoch21, with acc1:0.5108371155066554, and acc2:0.52625

Epoch - 23 Train-Loss : 1.567376991212368

Epoch - 23 Valid-Loss : 1.6156098222732544
micro_f1_23 = 0.52 
macro_f1_23 = 0.5013087520177748 
minloss 1.6156098222732544
just saved the best current model in epoch21, with acc1:0.5108371155066554, and acc2:0.52625

Epoch - 24 Train-Loss : 1.5454842793941497

Epoch - 24 Valid-Loss : 1.6017572689056396
micro_f1_24 = 0.55375 
macro_f1_24 = 0.5362175865833874 
minloss 1.6017572689056396
just saved the best current model in epoch24, with acc1:0.5362175865833874, and acc2:0.55375

Epoch - 25 Train-Loss : 1.493839281499386

Epoch - 25 Valid-Loss : 1.6266025519371032
micro_f1_25 = 0.5525 
macro_f1_25 = 0.5356014349617869 
minloss 1.6017572689056396
just saved the best current model in epoch24, with acc1:0.5362175865833874, and acc2:0.55375

Epoch - 26 Train-Loss : 1.469021221101284

Epoch - 26 Valid-Loss : 1.593486750125885
micro_f1_26 = 0.55375 
macro_f1_26 = 0.5337447828395958 
minloss 1.593486750125885
just saved the best current model in epoch24, with acc1:0.5362175865833874, and acc2:0.55375

Epoch - 27 Train-Loss : 1.4264494839310646

Epoch - 27 Valid-Loss : 1.5456633687019348
micro_f1_27 = 0.5725 
macro_f1_27 = 0.5547147523303595 
minloss 1.5456633687019348
just saved the best current model in epoch27, with acc1:0.5547147523303595, and acc2:0.5725

Epoch - 28 Train-Loss : 1.383446254134178

Epoch - 28 Valid-Loss : 1.5392575931549073
micro_f1_28 = 0.55625 
macro_f1_28 = 0.5419294553711165 
minloss 1.5392575931549073
just saved the best current model in epoch27, with acc1:0.5547147523303595, and acc2:0.5725

Epoch - 29 Train-Loss : 1.361744867861271

Epoch - 29 Valid-Loss : 1.5147404611110686
micro_f1_29 = 0.57375 
macro_f1_29 = 0.5571640037716447 
minloss 1.5147404611110686
just saved the best current model in epoch29, with acc1:0.5571640037716447, and acc2:0.57375

Epoch - 30 Train-Loss : 1.340794025361538

Epoch - 30 Valid-Loss : 1.4806173980236053
micro_f1_30 = 0.57 
macro_f1_30 = 0.5500353891818586 
minloss 1.4806173980236053
just saved the best current model in epoch29, with acc1:0.5571640037716447, and acc2:0.57375

Epoch - 31 Train-Loss : 1.3237515133619309

Epoch - 31 Valid-Loss : 1.4598187077045441
micro_f1_31 = 0.57375 
macro_f1_31 = 0.5576532528741805 
minloss 1.4598187077045441
just saved the best current model in epoch31, with acc1:0.5576532528741805, and acc2:0.57375

Epoch - 32 Train-Loss : 1.2915703013539315

Epoch - 32 Valid-Loss : 1.4713769364356994
micro_f1_32 = 0.57375 
macro_f1_32 = 0.5576455812992741 
minloss 1.4598187077045441
just saved the best current model in epoch31, with acc1:0.5576532528741805, and acc2:0.57375

Epoch - 33 Train-Loss : 1.268477641940117

Epoch - 33 Valid-Loss : 1.4438875198364258
micro_f1_33 = 0.5775 
macro_f1_33 = 0.5589850658995144 
minloss 1.4438875198364258
just saved the best current model in epoch33, with acc1:0.5589850658995144, and acc2:0.5775

Epoch - 34 Train-Loss : 1.246954481601715

Epoch - 34 Valid-Loss : 1.4340799951553345
micro_f1_34 = 0.58375 
macro_f1_34 = 0.5660721198369485 
minloss 1.4340799951553345
just saved the best current model in epoch34, with acc1:0.5660721198369485, and acc2:0.58375

Epoch - 35 Train-Loss : 1.2096512204408645

Epoch - 35 Valid-Loss : 1.3908550250530243
micro_f1_35 = 0.6 
macro_f1_35 = 0.5925557331951695 
minloss 1.3908550250530243
just saved the best current model in epoch35, with acc1:0.5925557331951695, and acc2:0.6

Epoch - 36 Train-Loss : 1.160542371571064

Epoch - 36 Valid-Loss : 1.3716098606586455
micro_f1_36 = 0.5975 
macro_f1_36 = 0.5828748056785044 
minloss 1.3716098606586455
just saved the best current model in epoch35, with acc1:0.5925557331951695, and acc2:0.6

Epoch - 37 Train-Loss : 1.1268725907802581

Epoch - 37 Valid-Loss : 1.362968029975891
micro_f1_37 = 0.60125 
macro_f1_37 = 0.5896325431127906 
minloss 1.362968029975891
just saved the best current model in epoch35, with acc1:0.5925557331951695, and acc2:0.6

Epoch - 38 Train-Loss : 1.1246335992217065

Epoch - 38 Valid-Loss : 1.373067415356636
micro_f1_38 = 0.60125 
macro_f1_38 = 0.5859235969471094 
minloss 1.362968029975891
just saved the best current model in epoch35, with acc1:0.5925557331951695, and acc2:0.6

Epoch - 39 Train-Loss : 1.107632066011429

Epoch - 39 Valid-Loss : 1.345445671081543
micro_f1_39 = 0.61375 
macro_f1_39 = 0.6101272299386908 
minloss 1.345445671081543
just saved the best current model in epoch39, with acc1:0.6101272299386908, and acc2:0.61375

Epoch - 40 Train-Loss : 1.0913513335585594

Epoch - 40 Valid-Loss : 1.3324221670627594
micro_f1_40 = 0.6175 
macro_f1_40 = 0.610780552162594 
minloss 1.3324221670627594
just saved the best current model in epoch40, with acc1:0.610780552162594, and acc2:0.6175

Epoch - 41 Train-Loss : 1.0453344401717186

Epoch - 41 Valid-Loss : 1.3297064924240112
micro_f1_41 = 0.6175 
macro_f1_41 = 0.6034772593521232 
minloss 1.3297064924240112
just saved the best current model in epoch40, with acc1:0.610780552162594, and acc2:0.6175

Epoch - 42 Train-Loss : 1.031701264679432

Epoch - 42 Valid-Loss : 1.3037973350286485
micro_f1_42 = 0.615 
macro_f1_42 = 0.6002694840606716 
minloss 1.3037973350286485
just saved the best current model in epoch40, with acc1:0.610780552162594, and acc2:0.6175

Epoch - 43 Train-Loss : 1.0074843008816243

Epoch - 43 Valid-Loss : 1.2938816571235656
micro_f1_43 = 0.63375 
macro_f1_43 = 0.6201167948640383 
minloss 1.2938816571235656
just saved the best current model in epoch43, with acc1:0.6201167948640383, and acc2:0.63375

Epoch - 44 Train-Loss : 1.0023930433392525

Epoch - 44 Valid-Loss : 1.2811137449741363
micro_f1_44 = 0.6375 
macro_f1_44 = 0.6257600600406213 
minloss 1.2811137449741363
just saved the best current model in epoch44, with acc1:0.6257600600406213, and acc2:0.6375

Epoch - 45 Train-Loss : 0.981728490293026

Epoch - 45 Valid-Loss : 1.2723542821407319
micro_f1_45 = 0.6325 
macro_f1_45 = 0.623275591737675 
minloss 1.2723542821407319
just saved the best current model in epoch44, with acc1:0.6257600600406213, and acc2:0.6375

Epoch - 46 Train-Loss : 0.9434388235211373

Epoch - 46 Valid-Loss : 1.2813196635246278
micro_f1_46 = 0.6275 
macro_f1_46 = 0.6144157528284266 
minloss 1.2723542821407319
just saved the best current model in epoch44, with acc1:0.6257600600406213, and acc2:0.6375

Epoch - 47 Train-Loss : 0.9325396114587784

Epoch - 47 Valid-Loss : 1.2462492591142655
micro_f1_47 = 0.63 
macro_f1_47 = 0.6191299053020414 
minloss 1.2462492591142655
just saved the best current model in epoch44, with acc1:0.6257600600406213, and acc2:0.6375

Epoch - 48 Train-Loss : 0.9286250703036785

Epoch - 48 Valid-Loss : 1.2283426666259765
micro_f1_48 = 0.63875 
macro_f1_48 = 0.6309529535920969 
minloss 1.2283426666259765
just saved the best current model in epoch48, with acc1:0.6309529535920969, and acc2:0.63875

Epoch - 49 Train-Loss : 0.907133166640997

Epoch - 49 Valid-Loss : 1.271835423707962
micro_f1_49 = 0.63625 
macro_f1_49 = 0.6293046730213085 
minloss 1.2283426666259765
just saved the best current model in epoch48, with acc1:0.6309529535920969, and acc2:0.63875

Epoch - 50 Train-Loss : 0.8847651396691799

Epoch - 50 Valid-Loss : 1.2015607595443725
micro_f1_50 = 0.6575 
macro_f1_50 = 0.6522577814722339 
minloss 1.2015607595443725
just saved the best current model in epoch50, with acc1:0.6522577814722339, and acc2:0.6575

Epoch - 51 Train-Loss : 0.8591502051055432

Epoch - 51 Valid-Loss : 1.2243833738565444
micro_f1_51 = 0.6525 
macro_f1_51 = 0.6412051363377919 
minloss 1.2015607595443725
just saved the best current model in epoch50, with acc1:0.6522577814722339, and acc2:0.6575

Epoch - 52 Train-Loss : 0.8454058292508125

Epoch - 52 Valid-Loss : 1.2269205284118652
micro_f1_52 = 0.6525 
macro_f1_52 = 0.6426068899145636 
minloss 1.2015607595443725
just saved the best current model in epoch50, with acc1:0.6522577814722339, and acc2:0.6575

Epoch - 53 Train-Loss : 0.8145626913011074

Epoch - 53 Valid-Loss : 1.1936073541641234
micro_f1_53 = 0.66 
macro_f1_53 = 0.6523799919577042 
minloss 1.1936073541641234
just saved the best current model in epoch53, with acc1:0.6523799919577042, and acc2:0.66

Epoch - 54 Train-Loss : 0.8180350099503993

Epoch - 54 Valid-Loss : 1.1938454538583756
micro_f1_54 = 0.66 
macro_f1_54 = 0.6531832168745493 
minloss 1.1936073541641234
just saved the best current model in epoch54, with acc1:0.6531832168745493, and acc2:0.66

Epoch - 55 Train-Loss : 0.7947405776381493

Epoch - 55 Valid-Loss : 1.20594340801239
micro_f1_55 = 0.66375 
macro_f1_55 = 0.6532168386379427 
minloss 1.1936073541641234
just saved the best current model in epoch55, with acc1:0.6532168386379427, and acc2:0.66375

Epoch - 56 Train-Loss : 0.7693458586931229

Epoch - 56 Valid-Loss : 1.1502874433994292
micro_f1_56 = 0.66125 
macro_f1_56 = 0.6512786640747069 
minloss 1.1502874433994292
just saved the best current model in epoch55, with acc1:0.6532168386379427, and acc2:0.66375

Epoch - 57 Train-Loss : 0.7759139581024647

Epoch - 57 Valid-Loss : 1.1739905768632888
micro_f1_57 = 0.66625 
macro_f1_57 = 0.6558974518663417 
minloss 1.1502874433994292
just saved the best current model in epoch57, with acc1:0.6558974518663417, and acc2:0.66625

Epoch - 58 Train-Loss : 0.7650717163085937

Epoch - 58 Valid-Loss : 1.1764866989850997
micro_f1_58 = 0.65625 
macro_f1_58 = 0.6512089347094638 
minloss 1.1502874433994292
just saved the best current model in epoch57, with acc1:0.6558974518663417, and acc2:0.66625

Epoch - 59 Train-Loss : 0.7209140557050705

Epoch - 59 Valid-Loss : 1.1870315593481064
micro_f1_59 = 0.65375 
macro_f1_59 = 0.6399220960664774 
minloss 1.1502874433994292
just saved the best current model in epoch57, with acc1:0.6558974518663417, and acc2:0.66625

Epoch - 60 Train-Loss : 0.7299358524382115

Epoch - 60 Valid-Loss : 1.1770884257555008
micro_f1_60 = 0.65375 
macro_f1_60 = 0.6411362893540022 
minloss 1.1502874433994292
just saved the best current model in epoch57, with acc1:0.6558974518663417, and acc2:0.66625

Epoch - 61 Train-Loss : 0.7155676880478858

Epoch - 61 Valid-Loss : 1.1309236258268356
micro_f1_61 = 0.67375 
macro_f1_61 = 0.6685300620366276 
minloss 1.1309236258268356
just saved the best current model in epoch61, with acc1:0.6685300620366276, and acc2:0.67375

Epoch - 62 Train-Loss : 0.7182080692052841

Epoch - 62 Valid-Loss : 1.1301880937814712
micro_f1_62 = 0.6725 
macro_f1_62 = 0.6661484403979537 
minloss 1.1301880937814712
just saved the best current model in epoch61, with acc1:0.6685300620366276, and acc2:0.67375

Epoch - 63 Train-Loss : 0.6908482931554317

Epoch - 63 Valid-Loss : 1.1914407619833947
micro_f1_63 = 0.64625 
macro_f1_63 = 0.6362735602582601 
minloss 1.1301880937814712
just saved the best current model in epoch61, with acc1:0.6685300620366276, and acc2:0.67375

Epoch - 64 Train-Loss : 0.6723564241826534

Epoch - 64 Valid-Loss : 1.1373748451471328
micro_f1_64 = 0.67875 
macro_f1_64 = 0.6672667269280218 
minloss 1.1301880937814712
just saved the best current model in epoch64, with acc1:0.6672667269280218, and acc2:0.67875

Epoch - 65 Train-Loss : 0.6617130967974663

Epoch - 65 Valid-Loss : 1.180572264790535
micro_f1_65 = 0.67 
macro_f1_65 = 0.6562297142569044 
minloss 1.1301880937814712
just saved the best current model in epoch64, with acc1:0.6672667269280218, and acc2:0.67875

Epoch - 66 Train-Loss : 0.6496388755738736

Epoch - 66 Valid-Loss : 1.1181783723831176
micro_f1_66 = 0.685 
macro_f1_66 = 0.6750230813942829 
minloss 1.1181783723831176
just saved the best current model in epoch66, with acc1:0.6750230813942829, and acc2:0.685

Epoch - 67 Train-Loss : 0.6322103916853666

Epoch - 67 Valid-Loss : 1.1326205509901046
micro_f1_67 = 0.67875 
macro_f1_67 = 0.6688181696832932 
minloss 1.1181783723831176
just saved the best current model in epoch66, with acc1:0.6750230813942829, and acc2:0.685

Epoch - 68 Train-Loss : 0.6141573443263769

Epoch - 68 Valid-Loss : 1.1050065284967423
micro_f1_68 = 0.6925 
macro_f1_68 = 0.6837208500861688 
minloss 1.1050065284967423
just saved the best current model in epoch68, with acc1:0.6837208500861688, and acc2:0.6925

Epoch - 69 Train-Loss : 0.6152515953779221

Epoch - 69 Valid-Loss : 1.1116783809661865
micro_f1_69 = 0.68375 
macro_f1_69 = 0.6757403531278022 
minloss 1.1050065284967423
just saved the best current model in epoch68, with acc1:0.6837208500861688, and acc2:0.6925

Epoch - 70 Train-Loss : 0.5855736620724201

Epoch - 70 Valid-Loss : 1.1198960030078888
micro_f1_70 = 0.685 
macro_f1_70 = 0.6768841073321138 
minloss 1.1050065284967423
just saved the best current model in epoch68, with acc1:0.6837208500861688, and acc2:0.6925

Epoch - 71 Train-Loss : 0.5949009278416634

Epoch - 71 Valid-Loss : 1.064224070906639
micro_f1_71 = 0.69375 
macro_f1_71 = 0.6905890170580379 
minloss 1.064224070906639
just saved the best current model in epoch71, with acc1:0.6905890170580379, and acc2:0.69375

Epoch - 72 Train-Loss : 0.5811916802823544

Epoch - 72 Valid-Loss : 1.0886347568035126
micro_f1_72 = 0.69375 
macro_f1_72 = 0.6884068371388067 
minloss 1.064224070906639
just saved the best current model in epoch71, with acc1:0.6905890170580379, and acc2:0.69375

Epoch - 73 Train-Loss : 0.5618100787699223

Epoch - 73 Valid-Loss : 1.1006702977418898
micro_f1_73 = 0.705 
macro_f1_73 = 0.6993096248585384 
minloss 1.064224070906639
just saved the best current model in epoch73, with acc1:0.6993096248585384, and acc2:0.705

Epoch - 74 Train-Loss : 0.5521597713977099

Epoch - 74 Valid-Loss : 1.0544226503372192
micro_f1_74 = 0.6925 
macro_f1_74 = 0.684682247726879 
minloss 1.0544226503372192
just saved the best current model in epoch73, with acc1:0.6993096248585384, and acc2:0.705

Epoch - 75 Train-Loss : 0.5315497659146786

Epoch - 75 Valid-Loss : 1.1229428604245186
micro_f1_75 = 0.66875 
macro_f1_75 = 0.6613758750271306 
minloss 1.0544226503372192
just saved the best current model in epoch73, with acc1:0.6993096248585384, and acc2:0.705

Epoch - 76 Train-Loss : 0.5425626516342164

Epoch - 76 Valid-Loss : 1.1203273341059685
micro_f1_76 = 0.69375 
macro_f1_76 = 0.6809981433527018 
minloss 1.0544226503372192
just saved the best current model in epoch73, with acc1:0.6993096248585384, and acc2:0.705

Epoch - 77 Train-Loss : 0.5132951726019382

Epoch - 77 Valid-Loss : 1.120060321688652
micro_f1_77 = 0.68875 
macro_f1_77 = 0.677465162130636 
minloss 1.0544226503372192
just saved the best current model in epoch73, with acc1:0.6993096248585384, and acc2:0.705

Epoch - 78 Train-Loss : 0.523997935950756

Epoch - 78 Valid-Loss : 1.0593356209993363
micro_f1_78 = 0.7 
macro_f1_78 = 0.6936277688718553 
minloss 1.0544226503372192
just saved the best current model in epoch73, with acc1:0.6993096248585384, and acc2:0.705

Epoch - 79 Train-Loss : 0.5020203748345375

Epoch - 79 Valid-Loss : 1.0624040931463241
micro_f1_79 = 0.69875 
macro_f1_79 = 0.692830202811586 
minloss 1.0544226503372192
just saved the best current model in epoch73, with acc1:0.6993096248585384, and acc2:0.705

Epoch - 80 Train-Loss : 0.4773946564644575

Epoch - 80 Valid-Loss : 1.068969356417656
micro_f1_80 = 0.7025 
macro_f1_80 = 0.7001733424207549 
minloss 1.0544226503372192
just saved the best current model in epoch73, with acc1:0.6993096248585384, and acc2:0.705

Epoch - 81 Train-Loss : 0.48100113324821

Epoch - 81 Valid-Loss : 1.1050367045402527
micro_f1_81 = 0.69875 
macro_f1_81 = 0.6928028114005491 
minloss 1.0544226503372192
training is terminating so as to prevent further overfitting
just saved the best current model in epoch73, with acc1:0.6993096248585384, and acc2:0.705
                         1
validation_fold:          
micro_f1          0.699219
macro_f1          0.705078
params                 inf
params = 3343378
Experiment's attempt changed to : 2


.......................................................................................................




.......................................................................................................


train_folds:  [1, 3, 4, 5]
valid_fold:  [2]
Loading_features......
Loading_features......
features are loaded
batch_size: 16
model_M2Dcnn2 initialized with total : 3343378 parameters.
 learning_rate = 1e-05, epochs = 135
 epochs = 135
Experiment's attempt no_ : 1
Train started..

Epoch - 1 Train-Loss : 3.8337149012088774

Epoch - 1 Valid-Loss : 3.6976978397369384
micro_f1_1 = 0.095 
macro_f1_1 = 0.05685282803462613 
minloss 3.6976978397369384

Epoch - 2 Train-Loss : 3.633161838054657

Epoch - 2 Valid-Loss : 3.5068342781066892
micro_f1_2 = 0.1425 
macro_f1_2 = 0.09950642690319722 
minloss 3.5068342781066892

Epoch - 3 Train-Loss : 3.4384540164470674

Epoch - 3 Valid-Loss : 3.3131936597824097
micro_f1_3 = 0.21499999999999997 
macro_f1_3 = 0.15357173709843047 
minloss 3.3131936597824097

Epoch - 4 Train-Loss : 3.2459371733665465

Epoch - 4 Valid-Loss : 3.0892623043060303
micro_f1_4 = 0.22375 
macro_f1_4 = 0.16170473474387456 
minloss 3.0892623043060303

Epoch - 5 Train-Loss : 3.070691936016083

Epoch - 5 Valid-Loss : 2.9071927452087403
micro_f1_5 = 0.30875 
macro_f1_5 = 0.24121014890501763 
minloss 2.9071927452087403

Epoch - 6 Train-Loss : 2.900877295732498

Epoch - 6 Valid-Loss : 2.7776882362365725
micro_f1_6 = 0.3425 
macro_f1_6 = 0.27359961663640153 
minloss 2.7776882362365725

Epoch - 7 Train-Loss : 2.7615166866779326

Epoch - 7 Valid-Loss : 2.609805116653442
micro_f1_7 = 0.38375 
macro_f1_7 = 0.33551409920164504 
minloss 2.609805116653442

Epoch - 8 Train-Loss : 2.629194648861885

Epoch - 8 Valid-Loss : 2.5151100659370424
micro_f1_8 = 0.39375 
macro_f1_8 = 0.3422815572372411 
minloss 2.5151100659370424

Epoch - 9 Train-Loss : 2.520196823477745

Epoch - 9 Valid-Loss : 2.392341723442078
micro_f1_9 = 0.435 
macro_f1_9 = 0.3864494636089171 
minloss 2.392341723442078

Epoch - 10 Train-Loss : 2.400298418402672

Epoch - 10 Valid-Loss : 2.2973886704444886
micro_f1_10 = 0.42999999999999994 
macro_f1_10 = 0.3929980454389146 
minloss 2.2973886704444886

Epoch - 11 Train-Loss : 2.3383408153057097

Epoch - 11 Valid-Loss : 2.21357928276062
micro_f1_11 = 0.45625 
macro_f1_11 = 0.41324728095123553 
minloss 2.21357928276062

Epoch - 12 Train-Loss : 2.24712080180645

Epoch - 12 Valid-Loss : 2.1128832626342775
micro_f1_12 = 0.4925 
macro_f1_12 = 0.456808275197741 
minloss 2.1128832626342775

Epoch - 13 Train-Loss : 2.160473684668541

Epoch - 13 Valid-Loss : 2.0541745734214785
micro_f1_13 = 0.4925 
macro_f1_13 = 0.4599114626706654 
minloss 2.0541745734214785

Epoch - 14 Train-Loss : 2.0702986162900925

Epoch - 14 Valid-Loss : 2.0304647636413575
micro_f1_14 = 0.5175 
macro_f1_14 = 0.474230832720252 
minloss 2.0304647636413575

Epoch - 15 Train-Loss : 2.016477058529854

Epoch - 15 Valid-Loss : 1.9679148292541504
micro_f1_15 = 0.50375 
macro_f1_15 = 0.4795084204212759 
minloss 1.9679148292541504

Epoch - 16 Train-Loss : 1.9512983310222625

Epoch - 16 Valid-Loss : 1.8693335008621217
micro_f1_16 = 0.515 
macro_f1_16 = 0.48457000059418276 
minloss 1.8693335008621217

Epoch - 17 Train-Loss : 1.918780762553215

Epoch - 17 Valid-Loss : 1.8666530132293702
micro_f1_17 = 0.51375 
macro_f1_17 = 0.48802453762447184 
minloss 1.8666530132293702
just saved the best current model in epoch17, with acc1:0.48802453762447184, and acc2:0.51375

Epoch - 18 Train-Loss : 1.8171305221319198

Epoch - 18 Valid-Loss : 1.8297670698165893
micro_f1_18 = 0.5475 
macro_f1_18 = 0.5258067239421574 
minloss 1.8297670698165893
just saved the best current model in epoch18, with acc1:0.5258067239421574, and acc2:0.5475

Epoch - 19 Train-Loss : 1.779967755675316

Epoch - 19 Valid-Loss : 1.741437520980835
micro_f1_19 = 0.55625 
macro_f1_19 = 0.5294434393740312 
minloss 1.741437520980835
just saved the best current model in epoch19, with acc1:0.5294434393740312, and acc2:0.55625

Epoch - 20 Train-Loss : 1.7490190523862839

Epoch - 20 Valid-Loss : 1.755393476486206
micro_f1_20 = 0.55125 
macro_f1_20 = 0.5285145413788246 
minloss 1.741437520980835
just saved the best current model in epoch19, with acc1:0.5294434393740312, and acc2:0.55625

Epoch - 21 Train-Loss : 1.67415717959404

Epoch - 21 Valid-Loss : 1.6460368049144745
micro_f1_21 = 0.56125 
macro_f1_21 = 0.5427292881608086 
minloss 1.6460368049144745
just saved the best current model in epoch21, with acc1:0.5427292881608086, and acc2:0.56125

Epoch - 22 Train-Loss : 1.6533932399749756

Epoch - 22 Valid-Loss : 1.636241317987442
micro_f1_22 = 0.5675 
macro_f1_22 = 0.5424060635955491 
minloss 1.636241317987442
just saved the best current model in epoch22, with acc1:0.5424060635955491, and acc2:0.5675

Epoch - 23 Train-Loss : 1.62240444034338

Epoch - 23 Valid-Loss : 1.621338288784027
micro_f1_23 = 0.5375 
macro_f1_23 = 0.5149263227363919 
minloss 1.621338288784027
just saved the best current model in epoch22, with acc1:0.5424060635955491, and acc2:0.5675

Epoch - 24 Train-Loss : 1.5635579326748847

Epoch - 24 Valid-Loss : 1.5666462588310242
micro_f1_24 = 0.59875 
macro_f1_24 = 0.5795723053137843 
minloss 1.5666462588310242
just saved the best current model in epoch24, with acc1:0.5795723053137843, and acc2:0.59875

Epoch - 25 Train-Loss : 1.5296948421001435

Epoch - 25 Valid-Loss : 1.556081725358963
micro_f1_25 = 0.595 
macro_f1_25 = 0.5799836029461497 
minloss 1.556081725358963
just saved the best current model in epoch24, with acc1:0.5795723053137843, and acc2:0.59875

Epoch - 26 Train-Loss : 1.4980517101287842

Epoch - 26 Valid-Loss : 1.515939223766327
micro_f1_26 = 0.6025 
macro_f1_26 = 0.5822696322908335 
minloss 1.515939223766327
just saved the best current model in epoch26, with acc1:0.5822696322908335, and acc2:0.6025

Epoch - 27 Train-Loss : 1.4591966810822488

Epoch - 27 Valid-Loss : 1.4809194588661194
micro_f1_27 = 0.59375 
macro_f1_27 = 0.5764963467669183 
minloss 1.4809194588661194
just saved the best current model in epoch26, with acc1:0.5822696322908335, and acc2:0.6025

Epoch - 28 Train-Loss : 1.4166773399710655

Epoch - 28 Valid-Loss : 1.4221696186065673
micro_f1_28 = 0.61375 
macro_f1_28 = 0.5984523297137909 
minloss 1.4221696186065673
just saved the best current model in epoch28, with acc1:0.5984523297137909, and acc2:0.61375

Epoch - 29 Train-Loss : 1.37987135887146

Epoch - 29 Valid-Loss : 1.445808025598526
micro_f1_29 = 0.6175 
macro_f1_29 = 0.5966300745499707 
minloss 1.4221696186065673
just saved the best current model in epoch29, with acc1:0.5966300745499707, and acc2:0.6175

Epoch - 30 Train-Loss : 1.3666223472356795

Epoch - 30 Valid-Loss : 1.4394509172439576
micro_f1_30 = 0.62 
macro_f1_30 = 0.6029125412189351 
minloss 1.4221696186065673
just saved the best current model in epoch30, with acc1:0.6029125412189351, and acc2:0.62

Epoch - 31 Train-Loss : 1.3400888094305992

Epoch - 31 Valid-Loss : 1.4075519859790802
micro_f1_31 = 0.61875 
macro_f1_31 = 0.6045192996445324 
minloss 1.4075519859790802
just saved the best current model in epoch31, with acc1:0.6045192996445324, and acc2:0.61875

Epoch - 32 Train-Loss : 1.3143000367283821

Epoch - 32 Valid-Loss : 1.3739438033103943
micro_f1_32 = 0.61375 
macro_f1_32 = 0.595333143258133 
minloss 1.3739438033103943
just saved the best current model in epoch31, with acc1:0.6045192996445324, and acc2:0.61875

Epoch - 33 Train-Loss : 1.266214158833027

Epoch - 33 Valid-Loss : 1.3836130964756013
micro_f1_33 = 0.59875 
macro_f1_33 = 0.5865585683448544 
minloss 1.3739438033103943
just saved the best current model in epoch31, with acc1:0.6045192996445324, and acc2:0.61875

Epoch - 34 Train-Loss : 1.2484604012966156

Epoch - 34 Valid-Loss : 1.345946000814438
micro_f1_34 = 0.63 
macro_f1_34 = 0.6116282219788954 
minloss 1.345946000814438
just saved the best current model in epoch34, with acc1:0.6116282219788954, and acc2:0.63

Epoch - 35 Train-Loss : 1.2016661274433136

Epoch - 35 Valid-Loss : 1.3234742045402528
micro_f1_35 = 0.6325 
macro_f1_35 = 0.6137813292075304 
minloss 1.3234742045402528
just saved the best current model in epoch35, with acc1:0.6137813292075304, and acc2:0.6325

Epoch - 36 Train-Loss : 1.204623045027256

Epoch - 36 Valid-Loss : 1.3376489806175231
micro_f1_36 = 0.6275 
macro_f1_36 = 0.6047706397180836 
minloss 1.3234742045402528
just saved the best current model in epoch35, with acc1:0.6137813292075304, and acc2:0.6325

Epoch - 37 Train-Loss : 1.1824593341350556

Epoch - 37 Valid-Loss : 1.3122815847396851
micro_f1_37 = 0.62 
macro_f1_37 = 0.6045208334429395 
minloss 1.3122815847396851
just saved the best current model in epoch35, with acc1:0.6137813292075304, and acc2:0.6325

Epoch - 38 Train-Loss : 1.1533240488171577

Epoch - 38 Valid-Loss : 1.283292316198349
micro_f1_38 = 0.6475 
macro_f1_38 = 0.6351485082186941 
minloss 1.283292316198349
just saved the best current model in epoch38, with acc1:0.6351485082186941, and acc2:0.6475

Epoch - 39 Train-Loss : 1.114274082481861

Epoch - 39 Valid-Loss : 1.2974296689033509
micro_f1_39 = 0.63375 
macro_f1_39 = 0.6131175227148841 
minloss 1.283292316198349
just saved the best current model in epoch38, with acc1:0.6351485082186941, and acc2:0.6475

Epoch - 40 Train-Loss : 1.0803234297037125

Epoch - 40 Valid-Loss : 1.2850780057907105
micro_f1_40 = 0.64375 
macro_f1_40 = 0.6297806399861959 
minloss 1.283292316198349
just saved the best current model in epoch38, with acc1:0.6351485082186941, and acc2:0.6475

Epoch - 41 Train-Loss : 1.072630133330822

Epoch - 41 Valid-Loss : 1.232935985326767
micro_f1_41 = 0.655 
macro_f1_41 = 0.6442731142069859 
minloss 1.232935985326767
just saved the best current model in epoch41, with acc1:0.6442731142069859, and acc2:0.655

Epoch - 42 Train-Loss : 1.061678221821785

Epoch - 42 Valid-Loss : 1.267580019235611
micro_f1_42 = 0.625 
macro_f1_42 = 0.6036752895557312 
minloss 1.232935985326767
just saved the best current model in epoch41, with acc1:0.6442731142069859, and acc2:0.655

Epoch - 43 Train-Loss : 1.0250630432367325

Epoch - 43 Valid-Loss : 1.2108046972751618
micro_f1_43 = 0.6675 
macro_f1_43 = 0.6515058100989057 
minloss 1.2108046972751618
just saved the best current model in epoch43, with acc1:0.6515058100989057, and acc2:0.6675

Epoch - 44 Train-Loss : 1.0128709974884986

Epoch - 44 Valid-Loss : 1.2027436232566833
micro_f1_44 = 0.66375 
macro_f1_44 = 0.6486039907803306 
minloss 1.2027436232566833
just saved the best current model in epoch43, with acc1:0.6515058100989057, and acc2:0.6675

Epoch - 45 Train-Loss : 1.0125904801487922

Epoch - 45 Valid-Loss : 1.2008503293991089
micro_f1_45 = 0.65 
macro_f1_45 = 0.6353599696187645 
minloss 1.2008503293991089
just saved the best current model in epoch43, with acc1:0.6515058100989057, and acc2:0.6675

Epoch - 46 Train-Loss : 0.981498823016882

Epoch - 46 Valid-Loss : 1.1717255520820617
micro_f1_46 = 0.6675 
macro_f1_46 = 0.6577345953090353 
minloss 1.1717255520820617
just saved the best current model in epoch46, with acc1:0.6577345953090353, and acc2:0.6675

Epoch - 47 Train-Loss : 0.9422355081140995

Epoch - 47 Valid-Loss : 1.154123221039772
micro_f1_47 = 0.66375 
macro_f1_47 = 0.6517022869124146 
minloss 1.154123221039772
just saved the best current model in epoch46, with acc1:0.6577345953090353, and acc2:0.6675

Epoch - 48 Train-Loss : 0.9276654426753521

Epoch - 48 Valid-Loss : 1.1723588979244233
micro_f1_48 = 0.66375 
macro_f1_48 = 0.6518963011876961 
minloss 1.154123221039772
just saved the best current model in epoch46, with acc1:0.6577345953090353, and acc2:0.6675

Epoch - 49 Train-Loss : 0.9165470451116562

Epoch - 49 Valid-Loss : 1.1527177345752717
micro_f1_49 = 0.65375 
macro_f1_49 = 0.6455287579946534 
minloss 1.1527177345752717
just saved the best current model in epoch46, with acc1:0.6577345953090353, and acc2:0.6675

Epoch - 50 Train-Loss : 0.8906234747171402

Epoch - 50 Valid-Loss : 1.134624788761139
micro_f1_50 = 0.6775 
macro_f1_50 = 0.6655748806115155 
minloss 1.134624788761139
just saved the best current model in epoch50, with acc1:0.6655748806115155, and acc2:0.6775

Epoch - 51 Train-Loss : 0.8988028188049794

Epoch - 51 Valid-Loss : 1.131454187631607
micro_f1_51 = 0.67625 
macro_f1_51 = 0.6641572381992324 
minloss 1.131454187631607
just saved the best current model in epoch50, with acc1:0.6655748806115155, and acc2:0.6775

Epoch - 52 Train-Loss : 0.8544451889395713

Epoch - 52 Valid-Loss : 1.1759675776958465
micro_f1_52 = 0.655 
macro_f1_52 = 0.6423527702640588 
minloss 1.131454187631607
just saved the best current model in epoch50, with acc1:0.6655748806115155, and acc2:0.6775

Epoch - 53 Train-Loss : 0.8572892878949642

Epoch - 53 Valid-Loss : 1.1578877174854278
micro_f1_53 = 0.665 
macro_f1_53 = 0.6520553971957221 
minloss 1.131454187631607
just saved the best current model in epoch50, with acc1:0.6655748806115155, and acc2:0.6775

Epoch - 54 Train-Loss : 0.8364527127146721

Epoch - 54 Valid-Loss : 1.1484514546394349
micro_f1_54 = 0.65625 
macro_f1_54 = 0.6417077444232423 
minloss 1.131454187631607
just saved the best current model in epoch50, with acc1:0.6655748806115155, and acc2:0.6775

Epoch - 55 Train-Loss : 0.8152484227716923

Epoch - 55 Valid-Loss : 1.1559094840288162
micro_f1_55 = 0.67125 
macro_f1_55 = 0.6616535980655547 
minloss 1.131454187631607
just saved the best current model in epoch50, with acc1:0.6655748806115155, and acc2:0.6775

Epoch - 56 Train-Loss : 0.7898929570615292

Epoch - 56 Valid-Loss : 1.0937560510635376
micro_f1_56 = 0.67875 
macro_f1_56 = 0.6681427282520533 
minloss 1.0937560510635376
just saved the best current model in epoch56, with acc1:0.6681427282520533, and acc2:0.67875

Epoch - 57 Train-Loss : 0.7790405075252056

Epoch - 57 Valid-Loss : 1.0992675858736038
micro_f1_57 = 0.675 
macro_f1_57 = 0.6716732597040811 
minloss 1.0937560510635376
just saved the best current model in epoch56, with acc1:0.6681427282520533, and acc2:0.67875

Epoch - 58 Train-Loss : 0.756199770718813

Epoch - 58 Valid-Loss : 1.1124726998806
micro_f1_58 = 0.6725 
macro_f1_58 = 0.6658808880820429 
minloss 1.0937560510635376
just saved the best current model in epoch56, with acc1:0.6681427282520533, and acc2:0.67875

Epoch - 59 Train-Loss : 0.7505462104082108

Epoch - 59 Valid-Loss : 1.0787424886226653
micro_f1_59 = 0.68625 
macro_f1_59 = 0.6766451867222675 
minloss 1.0787424886226653
just saved the best current model in epoch59, with acc1:0.6766451867222675, and acc2:0.68625

Epoch - 60 Train-Loss : 0.7391689138114452

Epoch - 60 Valid-Loss : 1.0741748052835465
micro_f1_60 = 0.6925 
macro_f1_60 = 0.6816057911290979 
minloss 1.0741748052835465
just saved the best current model in epoch60, with acc1:0.6816057911290979, and acc2:0.6925

Epoch - 61 Train-Loss : 0.6992735780030489

Epoch - 61 Valid-Loss : 1.0764801758527756
micro_f1_61 = 0.6775 
macro_f1_61 = 0.6732965810508529 
minloss 1.0741748052835465
just saved the best current model in epoch60, with acc1:0.6816057911290979, and acc2:0.6925

Epoch - 62 Train-Loss : 0.7163843532651663

Epoch - 62 Valid-Loss : 1.0504177504777907
micro_f1_62 = 0.6925 
macro_f1_62 = 0.6829599976103709 
minloss 1.0504177504777907
just saved the best current model in epoch62, with acc1:0.6829599976103709, and acc2:0.6925

Epoch - 63 Train-Loss : 0.7078641051054001

Epoch - 63 Valid-Loss : 1.084322257041931
micro_f1_63 = 0.69 
macro_f1_63 = 0.680594182142108 
minloss 1.0504177504777907
just saved the best current model in epoch62, with acc1:0.6829599976103709, and acc2:0.6925

Epoch - 64 Train-Loss : 0.6705392757058144

Epoch - 64 Valid-Loss : 1.076032007932663
micro_f1_64 = 0.6825 
macro_f1_64 = 0.6738347073572208 
minloss 1.0504177504777907
just saved the best current model in epoch62, with acc1:0.6829599976103709, and acc2:0.6925

Epoch - 65 Train-Loss : 0.6654742935299873

Epoch - 65 Valid-Loss : 1.0406092131137847
micro_f1_65 = 0.68875 
macro_f1_65 = 0.6816694652116818 
minloss 1.0406092131137847
just saved the best current model in epoch62, with acc1:0.6829599976103709, and acc2:0.6925

Epoch - 66 Train-Loss : 0.6507980290055275

Epoch - 66 Valid-Loss : 1.0585582959651947
micro_f1_66 = 0.6775 
macro_f1_66 = 0.6695575232833693 
minloss 1.0406092131137847
just saved the best current model in epoch62, with acc1:0.6829599976103709, and acc2:0.6925

Epoch - 67 Train-Loss : 0.6549841037392616

Epoch - 67 Valid-Loss : 1.0260075581073762
micro_f1_67 = 0.695 
macro_f1_67 = 0.6869150921697067 
minloss 1.0260075581073762
just saved the best current model in epoch67, with acc1:0.6869150921697067, and acc2:0.695

Epoch - 68 Train-Loss : 0.651392767727375

Epoch - 68 Valid-Loss : 1.0833396536111832
micro_f1_68 = 0.6825 
macro_f1_68 = 0.6666407018123777 
minloss 1.0260075581073762
just saved the best current model in epoch67, with acc1:0.6869150921697067, and acc2:0.695

Epoch - 69 Train-Loss : 0.6266044710576534

Epoch - 69 Valid-Loss : 1.128476976752281
micro_f1_69 = 0.65875 
macro_f1_69 = 0.6532740157808787 
minloss 1.0260075581073762
just saved the best current model in epoch67, with acc1:0.6869150921697067, and acc2:0.695

Epoch - 70 Train-Loss : 0.6081976967304945

Epoch - 70 Valid-Loss : 1.042311081290245
micro_f1_70 = 0.705 
macro_f1_70 = 0.6974889998772644 
minloss 1.0260075581073762
just saved the best current model in epoch70, with acc1:0.6974889998772644, and acc2:0.705

Epoch - 71 Train-Loss : 0.6078190359473229

Epoch - 71 Valid-Loss : 1.0377512204647064
micro_f1_71 = 0.69625 
macro_f1_71 = 0.6846663992589354 
minloss 1.0260075581073762
just saved the best current model in epoch70, with acc1:0.6974889998772644, and acc2:0.705

Epoch - 72 Train-Loss : 0.602951367944479

Epoch - 72 Valid-Loss : 1.0363484394550324
micro_f1_72 = 0.68375 
macro_f1_72 = 0.6703262840641159 
minloss 1.0260075581073762
just saved the best current model in epoch70, with acc1:0.6974889998772644, and acc2:0.705

Epoch - 73 Train-Loss : 0.5773829193413258

Epoch - 73 Valid-Loss : 1.0584599107503891
micro_f1_73 = 0.7 
macro_f1_73 = 0.6926179198278247 
minloss 1.0260075581073762
just saved the best current model in epoch70, with acc1:0.6974889998772644, and acc2:0.705

Epoch - 74 Train-Loss : 0.5845713191479445

Epoch - 74 Valid-Loss : 1.0883675301074982
micro_f1_74 = 0.685 
macro_f1_74 = 0.6688715232861466 
minloss 1.0260075581073762
just saved the best current model in epoch70, with acc1:0.6974889998772644, and acc2:0.705

Epoch - 75 Train-Loss : 0.5766245775669813

Epoch - 75 Valid-Loss : 1.1064626902341843
micro_f1_75 = 0.7 
macro_f1_75 = 0.6923611531067231 
minloss 1.0260075581073762
training is terminating so as to prevent further overfitting
just saved the best current model in epoch70, with acc1:0.6974889998772644, and acc2:0.705
(3, 1)
3
params = 3343378
Experiment's attempt changed to : 2


.......................................................................................................




.......................................................................................................


train_folds:  [1, 2, 4, 5]
valid_fold:  [3]
Loading_features......
Loading_features......
features are loaded
batch_size: 16
model_M2Dcnn2 initialized with total : 3343378 parameters.
 learning_rate = 1e-05, epochs = 135
 epochs = 135
Experiment's attempt no_ : 1
Train started..

Epoch - 1 Train-Loss : 3.838170738220215

Epoch - 1 Valid-Loss : 3.7081957626342774
micro_f1_1 = 0.09875 
macro_f1_1 = 0.0513487462437205 
minloss 3.7081957626342774

Epoch - 2 Train-Loss : 3.625362284183502

Epoch - 2 Valid-Loss : 3.5038739252090454
micro_f1_2 = 0.14375 
macro_f1_2 = 0.07824529164165503 
minloss 3.5038739252090454

Epoch - 3 Train-Loss : 3.446868963241577

Epoch - 3 Valid-Loss : 3.3242461824417116
micro_f1_3 = 0.19125 
macro_f1_3 = 0.12444065161807594 
minloss 3.3242461824417116

Epoch - 4 Train-Loss : 3.2529818546772002

Epoch - 4 Valid-Loss : 3.087625093460083
micro_f1_4 = 0.24375 
macro_f1_4 = 0.1814921607670698 
minloss 3.087625093460083

Epoch - 5 Train-Loss : 3.0800344932079313

Epoch - 5 Valid-Loss : 2.9459851217269897
micro_f1_5 = 0.24875 
macro_f1_5 = 0.19195233752188134 
minloss 2.9459851217269897

Epoch - 6 Train-Loss : 2.9135355138778687

Epoch - 6 Valid-Loss : 2.773578362464905
micro_f1_6 = 0.2875 
macro_f1_6 = 0.23945843838475483 
minloss 2.773578362464905

Epoch - 7 Train-Loss : 2.755576961040497

Epoch - 7 Valid-Loss : 2.6401571798324586
micro_f1_7 = 0.335 
macro_f1_7 = 0.2820912798496864 
minloss 2.6401571798324586

Epoch - 8 Train-Loss : 2.60249685883522

Epoch - 8 Valid-Loss : 2.5190388131141663
micro_f1_8 = 0.35625 
macro_f1_8 = 0.30783765611048136 
minloss 2.5190388131141663

Epoch - 9 Train-Loss : 2.4924662721157076

Epoch - 9 Valid-Loss : 2.36857257604599
micro_f1_9 = 0.3575 
macro_f1_9 = 0.3123336665285569 
minloss 2.36857257604599

Epoch - 10 Train-Loss : 2.3951436161994932

Epoch - 10 Valid-Loss : 2.285436222553253
micro_f1_10 = 0.37875 
macro_f1_10 = 0.3310142422319599 
minloss 2.285436222553253

Epoch - 11 Train-Loss : 2.309378609061241

Epoch - 11 Valid-Loss : 2.195700509548187
micro_f1_11 = 0.4000000000000001 
macro_f1_11 = 0.3613483499734461 
minloss 2.195700509548187

Epoch - 12 Train-Loss : 2.2421659767627715

Epoch - 12 Valid-Loss : 2.155018403530121
micro_f1_12 = 0.40625 
macro_f1_12 = 0.36895438308002754 
minloss 2.155018403530121

Epoch - 13 Train-Loss : 2.1563995629549026

Epoch - 13 Valid-Loss : 2.088974621295929
micro_f1_13 = 0.4375 
macro_f1_13 = 0.4066212738937574 
minloss 2.088974621295929

Epoch - 14 Train-Loss : 2.085802646279335

Epoch - 14 Valid-Loss : 1.9845069670677185
micro_f1_14 = 0.44375 
macro_f1_14 = 0.4120245378743228 
minloss 1.9845069670677185

Epoch - 15 Train-Loss : 1.9862978965044022

Epoch - 15 Valid-Loss : 1.9599625492095947
micro_f1_15 = 0.4475 
macro_f1_15 = 0.41963931561136286 
minloss 1.9599625492095947

Epoch - 16 Train-Loss : 1.9603375071287155

Epoch - 16 Valid-Loss : 1.8586687111854554
micro_f1_16 = 0.46625 
macro_f1_16 = 0.4287020814738572 
minloss 1.8586687111854554

Epoch - 17 Train-Loss : 1.876614317893982

Epoch - 17 Valid-Loss : 1.8826908230781556
micro_f1_17 = 0.48625 
macro_f1_17 = 0.45709901956418186 
minloss 1.8586687111854554

Epoch - 18 Train-Loss : 1.8040966141223906

Epoch - 18 Valid-Loss : 1.8165496563911439
micro_f1_18 = 0.50125 
macro_f1_18 = 0.47153408371437855 
minloss 1.8165496563911439

Epoch - 19 Train-Loss : 1.7797073489427566

Epoch - 19 Valid-Loss : 1.7521279239654541
micro_f1_19 = 0.49 
macro_f1_19 = 0.4644624453307016 
minloss 1.7521279239654541

Epoch - 20 Train-Loss : 1.7363538020849227

Epoch - 20 Valid-Loss : 1.7078985524177552
micro_f1_20 = 0.52 
macro_f1_20 = 0.49640629756774607 
minloss 1.7078985524177552
just saved the best current model in epoch20, with acc1:0.49640629756774607, and acc2:0.52

Epoch - 21 Train-Loss : 1.6653060555458068

Epoch - 21 Valid-Loss : 1.6595725333690643
micro_f1_21 = 0.525 
macro_f1_21 = 0.4924412223722356 
minloss 1.6595725333690643
just saved the best current model in epoch21, with acc1:0.4924412223722356, and acc2:0.525

Epoch - 22 Train-Loss : 1.6317987036705017

Epoch - 22 Valid-Loss : 1.719797316789627
micro_f1_22 = 0.5075 
macro_f1_22 = 0.4935807979970127 
minloss 1.6595725333690643
just saved the best current model in epoch21, with acc1:0.4924412223722356, and acc2:0.525

Epoch - 23 Train-Loss : 1.5916945070028305

Epoch - 23 Valid-Loss : 1.6475843811035156
micro_f1_23 = 0.525 
macro_f1_23 = 0.5112765662044112 
minloss 1.6475843811035156
just saved the best current model in epoch23, with acc1:0.5112765662044112, and acc2:0.525

Epoch - 24 Train-Loss : 1.5373142898082732

Epoch - 24 Valid-Loss : 1.5931419289112092
micro_f1_24 = 0.54 
macro_f1_24 = 0.5286052966830163 
minloss 1.5931419289112092
just saved the best current model in epoch24, with acc1:0.5286052966830163, and acc2:0.54

Epoch - 25 Train-Loss : 1.5026192489266395

Epoch - 25 Valid-Loss : 1.5603514683246613
micro_f1_25 = 0.53375 
macro_f1_25 = 0.5132977657994108 
minloss 1.5603514683246613
just saved the best current model in epoch24, with acc1:0.5286052966830163, and acc2:0.54

Epoch - 26 Train-Loss : 1.4787136703729629

Epoch - 26 Valid-Loss : 1.5174039113521576
micro_f1_26 = 0.56125 
macro_f1_26 = 0.5429315763572226 
minloss 1.5174039113521576
just saved the best current model in epoch26, with acc1:0.5429315763572226, and acc2:0.56125

Epoch - 27 Train-Loss : 1.4478126642107965

Epoch - 27 Valid-Loss : 1.499958897829056
micro_f1_27 = 0.56875 
macro_f1_27 = 0.5528998972152617 
minloss 1.499958897829056
just saved the best current model in epoch27, with acc1:0.5528998972152617, and acc2:0.56875

Epoch - 28 Train-Loss : 1.4108050882816314

Epoch - 28 Valid-Loss : 1.4826527893543244
micro_f1_28 = 0.575 
macro_f1_28 = 0.5656909083663707 
minloss 1.4826527893543244
just saved the best current model in epoch28, with acc1:0.5656909083663707, and acc2:0.575

Epoch - 29 Train-Loss : 1.3806635841727257

Epoch - 29 Valid-Loss : 1.5361872327327728
micro_f1_29 = 0.5725 
macro_f1_29 = 0.5533478546563296 
minloss 1.4826527893543244
just saved the best current model in epoch28, with acc1:0.5656909083663707, and acc2:0.575

Epoch - 30 Train-Loss : 1.3446907913684845

Epoch - 30 Valid-Loss : 1.4586904537677765
micro_f1_30 = 0.57 
macro_f1_30 = 0.5554841826563023 
minloss 1.4586904537677765
just saved the best current model in epoch28, with acc1:0.5656909083663707, and acc2:0.575

Epoch - 31 Train-Loss : 1.3075036227703094

Epoch - 31 Valid-Loss : 1.4198810589313506
micro_f1_31 = 0.6 
macro_f1_31 = 0.5874498704034713 
minloss 1.4198810589313506
just saved the best current model in epoch31, with acc1:0.5874498704034713, and acc2:0.6

Epoch - 32 Train-Loss : 1.2744594812393188

Epoch - 32 Valid-Loss : 1.4053463697433473
micro_f1_32 = 0.59 
macro_f1_32 = 0.5737259920691837 
minloss 1.4053463697433473
just saved the best current model in epoch31, with acc1:0.5874498704034713, and acc2:0.6

Epoch - 33 Train-Loss : 1.2709625923633576

Epoch - 33 Valid-Loss : 1.3720332622528075
micro_f1_33 = 0.60375 
macro_f1_33 = 0.5970455750939274 
minloss 1.3720332622528075
just saved the best current model in epoch33, with acc1:0.5970455750939274, and acc2:0.60375

Epoch - 34 Train-Loss : 1.2285098928213118

Epoch - 34 Valid-Loss : 1.343426274061203
micro_f1_34 = 0.60875 
macro_f1_34 = 0.6004676778255429 
minloss 1.343426274061203
just saved the best current model in epoch34, with acc1:0.6004676778255429, and acc2:0.60875

Epoch - 35 Train-Loss : 1.2158407413959502

Epoch - 35 Valid-Loss : 1.352114313840866
micro_f1_35 = 0.615 
macro_f1_35 = 0.6034759176765071 
minloss 1.343426274061203
just saved the best current model in epoch35, with acc1:0.6034759176765071, and acc2:0.615

Epoch - 36 Train-Loss : 1.1675157457590104

Epoch - 36 Valid-Loss : 1.3420653676986694
micro_f1_36 = 0.63 
macro_f1_36 = 0.6147332134764129 
minloss 1.3420653676986694
just saved the best current model in epoch36, with acc1:0.6147332134764129, and acc2:0.63

Epoch - 37 Train-Loss : 1.1579351380467415

Epoch - 37 Valid-Loss : 1.310537747144699
micro_f1_37 = 0.60375 
macro_f1_37 = 0.5943633593647877 
minloss 1.310537747144699
just saved the best current model in epoch36, with acc1:0.6147332134764129, and acc2:0.63

Epoch - 38 Train-Loss : 1.1376028615236282

Epoch - 38 Valid-Loss : 1.2877025586366653
micro_f1_38 = 0.63 
macro_f1_38 = 0.624435775790471 
minloss 1.2877025586366653
just saved the best current model in epoch38, with acc1:0.624435775790471, and acc2:0.63

Epoch - 39 Train-Loss : 1.1056258523464202

Epoch - 39 Valid-Loss : 1.282674862742424
micro_f1_39 = 0.63125 
macro_f1_39 = 0.6180163160112963 
minloss 1.282674862742424
just saved the best current model in epoch38, with acc1:0.624435775790471, and acc2:0.63

Epoch - 40 Train-Loss : 1.0789417481422425

Epoch - 40 Valid-Loss : 1.2824897783994675
micro_f1_40 = 0.64375 
macro_f1_40 = 0.6334570435596817 
minloss 1.2824897783994675
just saved the best current model in epoch40, with acc1:0.6334570435596817, and acc2:0.64375

Epoch - 41 Train-Loss : 1.063399550318718

Epoch - 41 Valid-Loss : 1.254680078625679
micro_f1_41 = 0.655 
macro_f1_41 = 0.6481852280240301 
minloss 1.254680078625679
just saved the best current model in epoch41, with acc1:0.6481852280240301, and acc2:0.655

Epoch - 42 Train-Loss : 1.0400678196549415

Epoch - 42 Valid-Loss : 1.2396798372268676
micro_f1_42 = 0.64125 
macro_f1_42 = 0.6327094627248154 
minloss 1.2396798372268676
just saved the best current model in epoch41, with acc1:0.6481852280240301, and acc2:0.655

Epoch - 43 Train-Loss : 1.02132564291358

Epoch - 43 Valid-Loss : 1.2731206607818604
micro_f1_43 = 0.63625 
macro_f1_43 = 0.6305517006673031 
minloss 1.2396798372268676
just saved the best current model in epoch41, with acc1:0.6481852280240301, and acc2:0.655

Epoch - 44 Train-Loss : 0.9915246227383614

Epoch - 44 Valid-Loss : 1.2300700652599335
micro_f1_44 = 0.64625 
macro_f1_44 = 0.6390757668126552 
minloss 1.2300700652599335
just saved the best current model in epoch41, with acc1:0.6481852280240301, and acc2:0.655

Epoch - 45 Train-Loss : 0.9773139423131942

Epoch - 45 Valid-Loss : 1.2117377746105193
micro_f1_45 = 0.65125 
macro_f1_45 = 0.644396319182503 
minloss 1.2117377746105193
just saved the best current model in epoch41, with acc1:0.6481852280240301, and acc2:0.655

Epoch - 46 Train-Loss : 0.94848822042346

Epoch - 46 Valid-Loss : 1.212516920566559
micro_f1_46 = 0.65 
macro_f1_46 = 0.6425788230386229 
minloss 1.2117377746105193
just saved the best current model in epoch41, with acc1:0.6481852280240301, and acc2:0.655

Epoch - 47 Train-Loss : 0.9423913632333278

Epoch - 47 Valid-Loss : 1.1748887550830842
micro_f1_47 = 0.665 
macro_f1_47 = 0.6567419870916693 
minloss 1.1748887550830842
just saved the best current model in epoch47, with acc1:0.6567419870916693, and acc2:0.665

Epoch - 48 Train-Loss : 0.9355563786625862

Epoch - 48 Valid-Loss : 1.202958202958107
micro_f1_48 = 0.65625 
macro_f1_48 = 0.6484083686121713 
minloss 1.1748887550830842
just saved the best current model in epoch47, with acc1:0.6567419870916693, and acc2:0.665

Epoch - 49 Train-Loss : 0.9194136832654476

Epoch - 49 Valid-Loss : 1.1711209923028947
micro_f1_49 = 0.6625 
macro_f1_49 = 0.6581587096054713 
minloss 1.1711209923028947
just saved the best current model in epoch47, with acc1:0.6567419870916693, and acc2:0.665

Epoch - 50 Train-Loss : 0.8854351775348186

Epoch - 50 Valid-Loss : 1.161394291818142
micro_f1_50 = 0.67125 
macro_f1_50 = 0.6659863387904125 
minloss 1.161394291818142
just saved the best current model in epoch50, with acc1:0.6659863387904125, and acc2:0.67125

Epoch - 51 Train-Loss : 0.8711758331954479

Epoch - 51 Valid-Loss : 1.1623491710424423
micro_f1_51 = 0.6575 
macro_f1_51 = 0.6549564959016635 
minloss 1.161394291818142
just saved the best current model in epoch50, with acc1:0.6659863387904125, and acc2:0.67125

Epoch - 52 Train-Loss : 0.8513988922536373

Epoch - 52 Valid-Loss : 1.1530877938866615
micro_f1_52 = 0.67375 
macro_f1_52 = 0.6684578994850869 
minloss 1.1530877938866615
just saved the best current model in epoch52, with acc1:0.6684578994850869, and acc2:0.67375

Epoch - 53 Train-Loss : 0.8367750035226345

Epoch - 53 Valid-Loss : 1.1636183750629425
micro_f1_53 = 0.6725 
macro_f1_53 = 0.6668859070572588 
minloss 1.1530877938866615
just saved the best current model in epoch52, with acc1:0.6684578994850869, and acc2:0.67375

Epoch - 54 Train-Loss : 0.826678109318018

Epoch - 54 Valid-Loss : 1.1510096061229707
micro_f1_54 = 0.6675 
macro_f1_54 = 0.6655014496535544 
minloss 1.1510096061229707
just saved the best current model in epoch52, with acc1:0.6684578994850869, and acc2:0.67375

Epoch - 55 Train-Loss : 0.7999361485987901

Epoch - 55 Valid-Loss : 1.2151187318563461
micro_f1_55 = 0.66125 
macro_f1_55 = 0.6464979119038097 
minloss 1.1510096061229707
just saved the best current model in epoch52, with acc1:0.6684578994850869, and acc2:0.67375

Epoch - 56 Train-Loss : 0.7892084638774395

Epoch - 56 Valid-Loss : 1.1854343277215957
micro_f1_56 = 0.6425 
macro_f1_56 = 0.6358854588413039 
minloss 1.1510096061229707
just saved the best current model in epoch52, with acc1:0.6684578994850869, and acc2:0.67375

Epoch - 57 Train-Loss : 0.7728583978116512

Epoch - 57 Valid-Loss : 1.1225076222419739
micro_f1_57 = 0.66625 
macro_f1_57 = 0.6600759487296552 
minloss 1.1225076222419739
just saved the best current model in epoch52, with acc1:0.6684578994850869, and acc2:0.67375

Epoch - 58 Train-Loss : 0.7523060777783394

Epoch - 58 Valid-Loss : 1.2258565533161163
micro_f1_58 = 0.63125 
macro_f1_58 = 0.6253234941834266 
minloss 1.1225076222419739
just saved the best current model in epoch52, with acc1:0.6684578994850869, and acc2:0.67375

Epoch - 59 Train-Loss : 0.7509244510531425

Epoch - 59 Valid-Loss : 1.104156824350357
micro_f1_59 = 0.67875 
macro_f1_59 = 0.6673046378859927 
minloss 1.104156824350357
just saved the best current model in epoch59, with acc1:0.6673046378859927, and acc2:0.67875

Epoch - 60 Train-Loss : 0.7312379501760006

Epoch - 60 Valid-Loss : 1.094950137734413
micro_f1_60 = 0.68 
macro_f1_60 = 0.6733613761334766 
minloss 1.094950137734413
just saved the best current model in epoch60, with acc1:0.6733613761334766, and acc2:0.68

Epoch - 61 Train-Loss : 0.7390154895186424

Epoch - 61 Valid-Loss : 1.094828031361103
micro_f1_61 = 0.685 
macro_f1_61 = 0.6830839294332578 
minloss 1.094828031361103
just saved the best current model in epoch61, with acc1:0.6830839294332578, and acc2:0.685

Epoch - 62 Train-Loss : 0.714413153976202

Epoch - 62 Valid-Loss : 1.0749609795212747
micro_f1_62 = 0.69625 
macro_f1_62 = 0.6891871109505006 
minloss 1.0749609795212747
just saved the best current model in epoch62, with acc1:0.6891871109505006, and acc2:0.69625

Epoch - 63 Train-Loss : 0.6920180438458919

Epoch - 63 Valid-Loss : 1.1258722963929175
micro_f1_63 = 0.68125 
macro_f1_63 = 0.6769345709928287 
minloss 1.0749609795212747
just saved the best current model in epoch62, with acc1:0.6891871109505006, and acc2:0.69625

Epoch - 64 Train-Loss : 0.6807292027771473

Epoch - 64 Valid-Loss : 1.0572016343474389
micro_f1_64 = 0.69625 
macro_f1_64 = 0.6924221206114699 
minloss 1.0572016343474389
just saved the best current model in epoch64, with acc1:0.6924221206114699, and acc2:0.69625

Epoch - 65 Train-Loss : 0.6512247982621193

Epoch - 65 Valid-Loss : 1.0821439599990845
micro_f1_65 = 0.69625 
macro_f1_65 = 0.6891628526840692 
minloss 1.0572016343474389
just saved the best current model in epoch64, with acc1:0.6924221206114699, and acc2:0.69625

Epoch - 66 Train-Loss : 0.6471244974434376

Epoch - 66 Valid-Loss : 1.0381149631738662
micro_f1_66 = 0.69375 
macro_f1_66 = 0.6889644824923566 
minloss 1.0381149631738662
just saved the best current model in epoch64, with acc1:0.6924221206114699, and acc2:0.69625

Epoch - 67 Train-Loss : 0.6240515989065171

Epoch - 67 Valid-Loss : 1.0363808119297027
micro_f1_67 = 0.6975 
macro_f1_67 = 0.6900802808175642 
minloss 1.0363808119297027
just saved the best current model in epoch64, with acc1:0.6924221206114699, and acc2:0.69625

Epoch - 68 Train-Loss : 0.6366085111349821

Epoch - 68 Valid-Loss : 1.0407002562284469
micro_f1_68 = 0.7 
macro_f1_68 = 0.6988863611549809 
minloss 1.0363808119297027
just saved the best current model in epoch68, with acc1:0.6988863611549809, and acc2:0.7

Epoch - 69 Train-Loss : 0.6187958858907223

Epoch - 69 Valid-Loss : 1.015733228623867
micro_f1_69 = 0.7 
macro_f1_69 = 0.6917146300918655 
minloss 1.015733228623867
just saved the best current model in epoch68, with acc1:0.6988863611549809, and acc2:0.7

Epoch - 70 Train-Loss : 0.6087284927815199

Epoch - 70 Valid-Loss : 1.0079201090335845
micro_f1_70 = 0.7075 
macro_f1_70 = 0.7002268458948567 
minloss 1.0079201090335845
just saved the best current model in epoch70, with acc1:0.7002268458948567, and acc2:0.7075

Epoch - 71 Train-Loss : 0.5966053945571184

Epoch - 71 Valid-Loss : 1.0279076270759107
micro_f1_71 = 0.6975 
macro_f1_71 = 0.6948838774982237 
minloss 1.0079201090335845
just saved the best current model in epoch70, with acc1:0.7002268458948567, and acc2:0.7075

Epoch - 72 Train-Loss : 0.5828002637624741

Epoch - 72 Valid-Loss : 1.0186146356165409
micro_f1_72 = 0.69875 
macro_f1_72 = 0.6978275158228644 
minloss 1.0079201090335845
just saved the best current model in epoch70, with acc1:0.7002268458948567, and acc2:0.7075

Epoch - 73 Train-Loss : 0.6013723994046449

Epoch - 73 Valid-Loss : 1.0586706095933913
micro_f1_73 = 0.6775 
macro_f1_73 = 0.6751549639504055 
minloss 1.0079201090335845
just saved the best current model in epoch70, with acc1:0.7002268458948567, and acc2:0.7075

Epoch - 74 Train-Loss : 0.558521994203329

Epoch - 74 Valid-Loss : 1.034850483685732
micro_f1_74 = 0.695 
macro_f1_74 = 0.68955135524678 
minloss 1.0079201090335845
just saved the best current model in epoch70, with acc1:0.7002268458948567, and acc2:0.7075

Epoch - 75 Train-Loss : 0.5250948521494866

Epoch - 75 Valid-Loss : 0.9893327036499977
micro_f1_75 = 0.7137500000000001 
macro_f1_75 = 0.7126549240846574 
minloss 0.9893327036499977
just saved the best current model in epoch75, with acc1:0.7126549240846574, and acc2:0.7137500000000001

Epoch - 76 Train-Loss : 0.5396347571164369

Epoch - 76 Valid-Loss : 0.9959187991917133
micro_f1_76 = 0.69625 
macro_f1_76 = 0.6883273365654179 
minloss 0.9893327036499977
just saved the best current model in epoch75, with acc1:0.7126549240846574, and acc2:0.7137500000000001

Epoch - 77 Train-Loss : 0.5386457455158233

Epoch - 77 Valid-Loss : 1.0155574209988116
micro_f1_77 = 0.69875 
macro_f1_77 = 0.6943638551961826 
minloss 0.9893327036499977
just saved the best current model in epoch75, with acc1:0.7126549240846574, and acc2:0.7137500000000001

Epoch - 78 Train-Loss : 0.5096521206572652

Epoch - 78 Valid-Loss : 0.9742710195481777
micro_f1_78 = 0.69875 
macro_f1_78 = 0.6917488898564274 
minloss 0.9742710195481777
just saved the best current model in epoch75, with acc1:0.7126549240846574, and acc2:0.7137500000000001

Epoch - 79 Train-Loss : 0.5077425906062126

Epoch - 79 Valid-Loss : 1.005537355840206
micro_f1_79 = 0.7125 
macro_f1_79 = 0.7058439056346566 
minloss 0.9742710195481777
just saved the best current model in epoch75, with acc1:0.7126549240846574, and acc2:0.7137500000000001

Epoch - 80 Train-Loss : 0.4980671539157629

Epoch - 80 Valid-Loss : 0.9979244408011436
micro_f1_80 = 0.7137500000000001 
macro_f1_80 = 0.7067270596206997 
minloss 0.9742710195481777
just saved the best current model in epoch75, with acc1:0.7126549240846574, and acc2:0.7137500000000001

Epoch - 81 Train-Loss : 0.4901718721538782

Epoch - 81 Valid-Loss : 0.973625950217247
micro_f1_81 = 0.7237499999999999 
macro_f1_81 = 0.7180044669208827 
minloss 0.973625950217247
just saved the best current model in epoch81, with acc1:0.7180044669208827, and acc2:0.7237499999999999

Epoch - 82 Train-Loss : 0.4928831385821104

Epoch - 82 Valid-Loss : 0.9628122499585152
micro_f1_82 = 0.7250000000000001 
macro_f1_82 = 0.7237891429111388 
minloss 0.9628122499585152
just saved the best current model in epoch82, with acc1:0.7237891429111388, and acc2:0.7250000000000001

Epoch - 83 Train-Loss : 0.49570421129465103

Epoch - 83 Valid-Loss : 0.9865063743293285
micro_f1_83 = 0.72125 
macro_f1_83 = 0.7183693787540387 
minloss 0.9628122499585152
just saved the best current model in epoch82, with acc1:0.7237891429111388, and acc2:0.7250000000000001

Epoch - 84 Train-Loss : 0.46412785045802596

Epoch - 84 Valid-Loss : 1.0467031566798688
micro_f1_84 = 0.705 
macro_f1_84 = 0.694565123982051 
minloss 0.9628122499585152
just saved the best current model in epoch82, with acc1:0.7237891429111388, and acc2:0.7250000000000001

Epoch - 85 Train-Loss : 0.44609540946781634

Epoch - 85 Valid-Loss : 0.9706038895249367
micro_f1_85 = 0.72125 
macro_f1_85 = 0.712904177289356 
minloss 0.9628122499585152
just saved the best current model in epoch82, with acc1:0.7237891429111388, and acc2:0.7250000000000001

Epoch - 86 Train-Loss : 0.4611890007555485

Epoch - 86 Valid-Loss : 0.9722507750988006
micro_f1_86 = 0.7237499999999999 
macro_f1_86 = 0.7163866939124199 
minloss 0.9628122499585152
just saved the best current model in epoch82, with acc1:0.7237891429111388, and acc2:0.7250000000000001

Epoch - 87 Train-Loss : 0.43469064772129057

Epoch - 87 Valid-Loss : 0.9645170737802983
micro_f1_87 = 0.7075 
macro_f1_87 = 0.7028034227692658 
minloss 0.9628122499585152
just saved the best current model in epoch82, with acc1:0.7237891429111388, and acc2:0.7250000000000001

Epoch - 88 Train-Loss : 0.43219330422580243

Epoch - 88 Valid-Loss : 0.9353679889440536
micro_f1_88 = 0.73875 
macro_f1_88 = 0.7336403134444478 
minloss 0.9353679889440536
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 89 Train-Loss : 0.4190877843275666

Epoch - 89 Valid-Loss : 0.9404566517472267
micro_f1_89 = 0.7299999999999999 
macro_f1_89 = 0.7236521320754676 
minloss 0.9353679889440536
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 90 Train-Loss : 0.40904544230550527

Epoch - 90 Valid-Loss : 0.98173972196877
micro_f1_90 = 0.7162499999999999 
macro_f1_90 = 0.7151982708187345 
minloss 0.9353679889440536
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 91 Train-Loss : 0.40694216057658195

Epoch - 91 Valid-Loss : 0.9366772873699665
micro_f1_91 = 0.7137500000000001 
macro_f1_91 = 0.7049474007931282 
minloss 0.9353679889440536
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 92 Train-Loss : 0.3850875246524811

Epoch - 92 Valid-Loss : 0.9486619710922242
micro_f1_92 = 0.72125 
macro_f1_92 = 0.7157808908524824 
minloss 0.9353679889440536
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 93 Train-Loss : 0.39830395139753816

Epoch - 93 Valid-Loss : 0.9410536526143551
micro_f1_93 = 0.7299999999999999 
macro_f1_93 = 0.723372744628066 
minloss 0.9353679889440536
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 94 Train-Loss : 0.3819452460482717

Epoch - 94 Valid-Loss : 0.9662201344966889
micro_f1_94 = 0.7237499999999999 
macro_f1_94 = 0.7187548451676276 
minloss 0.9353679889440536
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 95 Train-Loss : 0.3763540046662092

Epoch - 95 Valid-Loss : 0.9761832504719495
micro_f1_95 = 0.7112499999999999 
macro_f1_95 = 0.7022839323133896 
minloss 0.9353679889440536
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 96 Train-Loss : 0.3684865140542388

Epoch - 96 Valid-Loss : 0.9648958452045917
micro_f1_96 = 0.7175 
macro_f1_96 = 0.7139453138462853 
minloss 0.9353679889440536
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 97 Train-Loss : 0.35970610365271566

Epoch - 97 Valid-Loss : 0.9340020719170571
micro_f1_97 = 0.72125 
macro_f1_97 = 0.7172500914863049 
minloss 0.9340020719170571
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 98 Train-Loss : 0.3383191246166825

Epoch - 98 Valid-Loss : 0.9364022174477578
micro_f1_98 = 0.73125 
macro_f1_98 = 0.7262903143976034 
minloss 0.9340020719170571
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 99 Train-Loss : 0.3461769949644804

Epoch - 99 Valid-Loss : 0.9144479738175869
micro_f1_99 = 0.73125 
macro_f1_99 = 0.7279894128744757 
minloss 0.9144479738175869
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 100 Train-Loss : 0.33305847182869913

Epoch - 100 Valid-Loss : 0.9820540601015091
micro_f1_100 = 0.73625 
macro_f1_100 = 0.7296136552328875 
minloss 0.9144479738175869
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 101 Train-Loss : 0.33539193470031026

Epoch - 101 Valid-Loss : 0.9147804867476225
micro_f1_101 = 0.735 
macro_f1_101 = 0.7299067524269222 
minloss 0.9144479738175869
just saved the best current model in epoch88, with acc1:0.7336403134444478, and acc2:0.73875

Epoch - 102 Train-Loss : 0.31859926965087654

Epoch - 102 Valid-Loss : 0.8842850837856531
micro_f1_102 = 0.7425 
macro_f1_102 = 0.7369978518111432 
minloss 0.8842850837856531
just saved the best current model in epoch102, with acc1:0.7369978518111432, and acc2:0.7425

Epoch - 103 Train-Loss : 0.33224421717226504

Epoch - 103 Valid-Loss : 0.9051578624546528
micro_f1_103 = 0.7237499999999999 
macro_f1_103 = 0.7221022001906126 
minloss 0.8842850837856531
just saved the best current model in epoch102, with acc1:0.7369978518111432, and acc2:0.7425

Epoch - 104 Train-Loss : 0.3111910130828619

Epoch - 104 Valid-Loss : 0.918954489454627
micro_f1_104 = 0.735 
macro_f1_104 = 0.7304400593898995 
minloss 0.8842850837856531
just saved the best current model in epoch102, with acc1:0.7369978518111432, and acc2:0.7425

Epoch - 105 Train-Loss : 0.31657142866402865

Epoch - 105 Valid-Loss : 0.9355057927966118
micro_f1_105 = 0.72875 
macro_f1_105 = 0.7257029910193846 
minloss 0.8842850837856531
just saved the best current model in epoch102, with acc1:0.7369978518111432, and acc2:0.7425

Epoch - 106 Train-Loss : 0.2916433388926089

Epoch - 106 Valid-Loss : 0.9144524887204171
micro_f1_106 = 0.75375 
macro_f1_106 = 0.7473995719995022 
minloss 0.8842850837856531
just saved the best current model in epoch106, with acc1:0.7473995719995022, and acc2:0.75375

Epoch - 107 Train-Loss : 0.2948276264965534

Epoch - 107 Valid-Loss : 0.8863180686533451
micro_f1_107 = 0.7337500000000001 
macro_f1_107 = 0.7304026143360386 
minloss 0.8842850837856531
just saved the best current model in epoch106, with acc1:0.7473995719995022, and acc2:0.75375

Epoch - 108 Train-Loss : 0.2873003824800253

Epoch - 108 Valid-Loss : 1.0236562741547823
micro_f1_108 = 0.72125 
macro_f1_108 = 0.7173779896165506 
minloss 0.8842850837856531
just saved the best current model in epoch106, with acc1:0.7473995719995022, and acc2:0.75375

Epoch - 109 Train-Loss : 0.2901112071424723

Epoch - 109 Valid-Loss : 0.9609763920307159
micro_f1_109 = 0.7225 
macro_f1_109 = 0.7147742034962361 
minloss 0.8842850837856531
just saved the best current model in epoch106, with acc1:0.7473995719995022, and acc2:0.75375

Epoch - 110 Train-Loss : 0.2802516440860927

Epoch - 110 Valid-Loss : 0.9290250926464796
micro_f1_110 = 0.7425 
macro_f1_110 = 0.7356186828248726 
minloss 0.8842850837856531
just saved the best current model in epoch106, with acc1:0.7473995719995022, and acc2:0.75375

Epoch - 111 Train-Loss : 0.2825060515291989

Epoch - 111 Valid-Loss : 0.9146278425678611
micro_f1_111 = 0.7299999999999999 
macro_f1_111 = 0.7227931995435106 
minloss 0.8842850837856531
just saved the best current model in epoch106, with acc1:0.7473995719995022, and acc2:0.75375

Epoch - 112 Train-Loss : 0.276597277186811

Epoch - 112 Valid-Loss : 0.9091857325285673
micro_f1_112 = 0.7487500000000001 
macro_f1_112 = 0.7466575884770801 
minloss 0.8842850837856531
just saved the best current model in epoch106, with acc1:0.7473995719995022, and acc2:0.75375

Epoch - 113 Train-Loss : 0.24463302463293077

Epoch - 113 Valid-Loss : 0.8771072250604629
micro_f1_113 = 0.75875 
macro_f1_113 = 0.7539677183006431 
minloss 0.8771072250604629
just saved the best current model in epoch113, with acc1:0.7539677183006431, and acc2:0.75875

Epoch - 114 Train-Loss : 0.25943659991025925

Epoch - 114 Valid-Loss : 0.988204510435462
micro_f1_114 = 0.74125 
macro_f1_114 = 0.7294371502433262 
minloss 0.8771072250604629
just saved the best current model in epoch113, with acc1:0.7539677183006431, and acc2:0.75875

Epoch - 115 Train-Loss : 0.2485242083668709

Epoch - 115 Valid-Loss : 0.9088224714994431
micro_f1_115 = 0.745 
macro_f1_115 = 0.7378665964579513 
minloss 0.8771072250604629
just saved the best current model in epoch113, with acc1:0.7539677183006431, and acc2:0.75875

Epoch - 116 Train-Loss : 0.25765981547534467

Epoch - 116 Valid-Loss : 0.913278848528862
micro_f1_116 = 0.7425 
macro_f1_116 = 0.7374338558079114 
minloss 0.8771072250604629
just saved the best current model in epoch113, with acc1:0.7539677183006431, and acc2:0.75875

Epoch - 117 Train-Loss : 0.22378885513171554

Epoch - 117 Valid-Loss : 0.9435366611927748
micro_f1_117 = 0.7275000000000001 
macro_f1_117 = 0.7292896147327851 
minloss 0.8771072250604629
just saved the best current model in epoch113, with acc1:0.7539677183006431, and acc2:0.75875

Epoch - 118 Train-Loss : 0.24212007449008524

Epoch - 118 Valid-Loss : 0.9615288499742747
micro_f1_118 = 0.72 
macro_f1_118 = 0.7100497824764342 
minloss 0.8771072250604629
training is terminating so as to prevent further overfitting
just saved the best current model in epoch113, with acc1:0.7539677183006431, and acc2:0.75875
(3, 2)
3
params = 3343378
Experiment's attempt changed to : 2


.......................................................................................................




.......................................................................................................


train_folds:  [1, 2, 3, 5]
valid_fold:  [4]
Loading_features......
Loading_features......
features are loaded
batch_size: 16
model_M2Dcnn2 initialized with total : 3343378 parameters.
 learning_rate = 1e-05, epochs = 135
 epochs = 135
Experiment's attempt no_ : 1
Train started..

Epoch - 1 Train-Loss : 3.8304711413383483

Epoch - 1 Valid-Loss : 3.6788924407958983
micro_f1_1 = 0.115 
macro_f1_1 = 0.04814808571606068 
minloss 3.6788924407958983

Epoch - 2 Train-Loss : 3.6228558349609377

Epoch - 2 Valid-Loss : 3.4654139041900636
micro_f1_2 = 0.18375 
macro_f1_2 = 0.12486304373066857 
minloss 3.4654139041900636

Epoch - 3 Train-Loss : 3.4276413536071777

Epoch - 3 Valid-Loss : 3.242200183868408
micro_f1_3 = 0.23375 
macro_f1_3 = 0.1772760576085759 
minloss 3.242200183868408

Epoch - 4 Train-Loss : 3.239439947605133

Epoch - 4 Valid-Loss : 3.074284677505493
micro_f1_4 = 0.24875 
macro_f1_4 = 0.17549361067805264 
minloss 3.074284677505493

Epoch - 5 Train-Loss : 3.0514728486537934

Epoch - 5 Valid-Loss : 2.8717253017425537
micro_f1_5 = 0.31 
macro_f1_5 = 0.2555657103216736 
minloss 2.8717253017425537

Epoch - 6 Train-Loss : 2.9281052136421204

Epoch - 6 Valid-Loss : 2.7133369922637938
micro_f1_6 = 0.36250000000000004 
macro_f1_6 = 0.3084807612157555 
minloss 2.7133369922637938

Epoch - 7 Train-Loss : 2.7575743556022645

Epoch - 7 Valid-Loss : 2.588641653060913
micro_f1_7 = 0.39125 
macro_f1_7 = 0.3442974553874905 
minloss 2.588641653060913

Epoch - 8 Train-Loss : 2.6326202565431593

Epoch - 8 Valid-Loss : 2.4490444707870482
micro_f1_8 = 0.39625 
macro_f1_8 = 0.3382508200434853 
minloss 2.4490444707870482

Epoch - 9 Train-Loss : 2.5455557465553285

Epoch - 9 Valid-Loss : 2.3754207253456117
micro_f1_9 = 0.4125 
macro_f1_9 = 0.3739420463791606 
minloss 2.3754207253456117

Epoch - 10 Train-Loss : 2.455568352341652

Epoch - 10 Valid-Loss : 2.20443332195282
micro_f1_10 = 0.4525 
macro_f1_10 = 0.39269091989161586 
minloss 2.20443332195282

Epoch - 11 Train-Loss : 2.3534939336776732

Epoch - 11 Valid-Loss : 2.1285929703712463
micro_f1_11 = 0.47875 
macro_f1_11 = 0.4207441767774898 
minloss 2.1285929703712463

Epoch - 12 Train-Loss : 2.275532795190811

Epoch - 12 Valid-Loss : 2.1050212955474854
micro_f1_12 = 0.4575 
macro_f1_12 = 0.4160122052323978 
minloss 2.1050212955474854

Epoch - 13 Train-Loss : 2.1721072494983673

Epoch - 13 Valid-Loss : 1.9870178055763246
micro_f1_13 = 0.49375 
macro_f1_13 = 0.4511112040258324 
minloss 1.9870178055763246

Epoch - 14 Train-Loss : 2.1076195687055588

Epoch - 14 Valid-Loss : 1.9222748804092407
micro_f1_14 = 0.4975 
macro_f1_14 = 0.4479436605607616 
minloss 1.9222748804092407

Epoch - 15 Train-Loss : 2.04783604323864

Epoch - 15 Valid-Loss : 1.8114696955680847
micro_f1_15 = 0.52125 
macro_f1_15 = 0.4660029465118516 
minloss 1.8114696955680847

Epoch - 16 Train-Loss : 1.982960085272789

Epoch - 16 Valid-Loss : 1.8183043956756593
micro_f1_16 = 0.515 
macro_f1_16 = 0.4752795963745416 
minloss 1.8114696955680847

Epoch - 17 Train-Loss : 1.9317230463027955

Epoch - 17 Valid-Loss : 1.7507652759552002
micro_f1_17 = 0.56625 
macro_f1_17 = 0.5243837646664423 
minloss 1.7507652759552002
just saved the best current model in epoch17, with acc1:0.5243837646664423, and acc2:0.56625

Epoch - 18 Train-Loss : 1.8674366921186447

Epoch - 18 Valid-Loss : 1.7339893889427185
micro_f1_18 = 0.54375 
macro_f1_18 = 0.5116252679855701 
minloss 1.7339893889427185
just saved the best current model in epoch17, with acc1:0.5243837646664423, and acc2:0.56625

Epoch - 19 Train-Loss : 1.8365908151865005

Epoch - 19 Valid-Loss : 1.6911139154434205
micro_f1_19 = 0.55 
macro_f1_19 = 0.5180096777090459 
minloss 1.6911139154434205
just saved the best current model in epoch17, with acc1:0.5243837646664423, and acc2:0.56625

Epoch - 20 Train-Loss : 1.7782871180772781

Epoch - 20 Valid-Loss : 1.5794549214839935
micro_f1_20 = 0.575 
macro_f1_20 = 0.5395513788339896 
minloss 1.5794549214839935
just saved the best current model in epoch20, with acc1:0.5395513788339896, and acc2:0.575

Epoch - 21 Train-Loss : 1.7228171837329864

Epoch - 21 Valid-Loss : 1.562163302898407
micro_f1_21 = 0.58375 
macro_f1_21 = 0.5409803822988328 
minloss 1.562163302898407
just saved the best current model in epoch21, with acc1:0.5409803822988328, and acc2:0.58375

Epoch - 22 Train-Loss : 1.675644918680191

Epoch - 22 Valid-Loss : 1.5393438279628753
micro_f1_22 = 0.575 
macro_f1_22 = 0.5385748507944169 
minloss 1.5393438279628753
just saved the best current model in epoch21, with acc1:0.5409803822988328, and acc2:0.58375

Epoch - 23 Train-Loss : 1.6637704437971115

Epoch - 23 Valid-Loss : 1.4816609477996827
micro_f1_23 = 0.575 
macro_f1_23 = 0.5424485527687354 
minloss 1.4816609477996827
just saved the best current model in epoch21, with acc1:0.5409803822988328, and acc2:0.58375

Epoch - 24 Train-Loss : 1.610274815261364

Epoch - 24 Valid-Loss : 1.4580029511451722
micro_f1_24 = 0.585 
macro_f1_24 = 0.5574443966918373 
minloss 1.4580029511451722
just saved the best current model in epoch24, with acc1:0.5574443966918373, and acc2:0.585

Epoch - 25 Train-Loss : 1.5806054469943047

Epoch - 25 Valid-Loss : 1.4178465068340302
micro_f1_25 = 0.5975 
macro_f1_25 = 0.5651812299701034 
minloss 1.4178465068340302
just saved the best current model in epoch25, with acc1:0.5651812299701034, and acc2:0.5975

Epoch - 26 Train-Loss : 1.5269277322292327

Epoch - 26 Valid-Loss : 1.3914500725269319
micro_f1_26 = 0.6125 
macro_f1_26 = 0.5854140061548372 
minloss 1.3914500725269319
just saved the best current model in epoch26, with acc1:0.5854140061548372, and acc2:0.6125

Epoch - 27 Train-Loss : 1.5017666736245154

Epoch - 27 Valid-Loss : 1.3987174499034882
micro_f1_27 = 0.59875 
macro_f1_27 = 0.5700663757724311 
minloss 1.3914500725269319
just saved the best current model in epoch26, with acc1:0.5854140061548372, and acc2:0.6125

Epoch - 28 Train-Loss : 1.490215052962303

Epoch - 28 Valid-Loss : 1.3727058804035186
micro_f1_28 = 0.60875 
macro_f1_28 = 0.5798457965221084 
minloss 1.3727058804035186
just saved the best current model in epoch26, with acc1:0.5854140061548372, and acc2:0.6125

Epoch - 29 Train-Loss : 1.4293582451343536

Epoch - 29 Valid-Loss : 1.343679460287094
micro_f1_29 = 0.63 
macro_f1_29 = 0.5983894481772759 
minloss 1.343679460287094
just saved the best current model in epoch29, with acc1:0.5983894481772759, and acc2:0.63

Epoch - 30 Train-Loss : 1.4077682197093964

Epoch - 30 Valid-Loss : 1.291596553325653
micro_f1_30 = 0.64 
macro_f1_30 = 0.6142193614812392 
minloss 1.291596553325653
just saved the best current model in epoch30, with acc1:0.6142193614812392, and acc2:0.64

Epoch - 31 Train-Loss : 1.36409638017416

Epoch - 31 Valid-Loss : 1.2888912153244019
micro_f1_31 = 0.62125 
macro_f1_31 = 0.5959860652281018 
minloss 1.2888912153244019
just saved the best current model in epoch30, with acc1:0.6142193614812392, and acc2:0.64

Epoch - 32 Train-Loss : 1.341557140350342

Epoch - 32 Valid-Loss : 1.283083015680313
micro_f1_32 = 0.6225 
macro_f1_32 = 0.6071871645759834 
minloss 1.283083015680313
just saved the best current model in epoch30, with acc1:0.6142193614812392, and acc2:0.64

Epoch - 33 Train-Loss : 1.2868659070134163

Epoch - 33 Valid-Loss : 1.2890176808834075
micro_f1_33 = 0.63375 
macro_f1_33 = 0.6133074946964253 
minloss 1.283083015680313
just saved the best current model in epoch30, with acc1:0.6142193614812392, and acc2:0.64

Epoch - 34 Train-Loss : 1.2855594572424889

Epoch - 34 Valid-Loss : 1.2388705241680145
micro_f1_34 = 0.6325 
macro_f1_34 = 0.6097578142107009 
minloss 1.2388705241680145
just saved the best current model in epoch30, with acc1:0.6142193614812392, and acc2:0.64

Epoch - 35 Train-Loss : 1.270452890098095

Epoch - 35 Valid-Loss : 1.2107503283023835
micro_f1_35 = 0.65875 
macro_f1_35 = 0.6389882663335162 
minloss 1.2107503283023835
just saved the best current model in epoch35, with acc1:0.6389882663335162, and acc2:0.65875

Epoch - 36 Train-Loss : 1.2304219153523446

Epoch - 36 Valid-Loss : 1.2022215282917024
micro_f1_36 = 0.65625 
macro_f1_36 = 0.6362863523139664 
minloss 1.2022215282917024
just saved the best current model in epoch35, with acc1:0.6389882663335162, and acc2:0.65875

Epoch - 37 Train-Loss : 1.2234586995840073

Epoch - 37 Valid-Loss : 1.1552057564258575
micro_f1_37 = 0.65625 
macro_f1_37 = 0.6368143025936793 
minloss 1.1552057564258575
just saved the best current model in epoch35, with acc1:0.6389882663335162, and acc2:0.65875

Epoch - 38 Train-Loss : 1.1970383256673813

Epoch - 38 Valid-Loss : 1.1705672466754913
micro_f1_38 = 0.6825 
macro_f1_38 = 0.6648165463705443 
minloss 1.1552057564258575
just saved the best current model in epoch38, with acc1:0.6648165463705443, and acc2:0.6825

Epoch - 39 Train-Loss : 1.1673983004689217

Epoch - 39 Valid-Loss : 1.1269625061750412
micro_f1_39 = 0.6625 
macro_f1_39 = 0.6446055824423221 
minloss 1.1269625061750412
just saved the best current model in epoch38, with acc1:0.6648165463705443, and acc2:0.6825

Epoch - 40 Train-Loss : 1.1309766578674316

Epoch - 40 Valid-Loss : 1.1140395665168763
micro_f1_40 = 0.67375 
macro_f1_40 = 0.656396715701341 
minloss 1.1140395665168763
just saved the best current model in epoch38, with acc1:0.6648165463705443, and acc2:0.6825

Epoch - 41 Train-Loss : 1.1193015223741531

Epoch - 41 Valid-Loss : 1.094701087474823
micro_f1_41 = 0.69 
macro_f1_41 = 0.6694116767477739 
minloss 1.094701087474823
just saved the best current model in epoch41, with acc1:0.6694116767477739, and acc2:0.69

Epoch - 42 Train-Loss : 1.1060877349972724

Epoch - 42 Valid-Loss : 1.1029675543308257
micro_f1_42 = 0.6675 
macro_f1_42 = 0.6479768263441567 
minloss 1.094701087474823
just saved the best current model in epoch41, with acc1:0.6694116767477739, and acc2:0.69

Epoch - 43 Train-Loss : 1.0699658760428428

Epoch - 43 Valid-Loss : 1.1005637407302857
micro_f1_43 = 0.68 
macro_f1_43 = 0.66726559944754 
minloss 1.094701087474823
just saved the best current model in epoch41, with acc1:0.6694116767477739, and acc2:0.69

Epoch - 44 Train-Loss : 1.065004195421934

Epoch - 44 Valid-Loss : 1.051747897863388
micro_f1_44 = 0.69125 
macro_f1_44 = 0.6772024569437869 
minloss 1.051747897863388
just saved the best current model in epoch44, with acc1:0.6772024569437869, and acc2:0.69125

Epoch - 45 Train-Loss : 1.009181724190712

Epoch - 45 Valid-Loss : 1.0334329617023468
micro_f1_45 = 0.69125 
macro_f1_45 = 0.6747150651713182 
minloss 1.0334329617023468
just saved the best current model in epoch44, with acc1:0.6772024569437869, and acc2:0.69125

Epoch - 46 Train-Loss : 1.023520265519619

Epoch - 46 Valid-Loss : 1.0382879960536957
micro_f1_46 = 0.7125 
macro_f1_46 = 0.6977345384872592 
minloss 1.0334329617023468
just saved the best current model in epoch46, with acc1:0.6977345384872592, and acc2:0.7125

Epoch - 47 Train-Loss : 0.9924995656311512

Epoch - 47 Valid-Loss : 1.0634048342704774
micro_f1_47 = 0.70875 
macro_f1_47 = 0.6958267524245592 
minloss 1.0334329617023468
just saved the best current model in epoch46, with acc1:0.6977345384872592, and acc2:0.7125

Epoch - 48 Train-Loss : 0.9915762129426002

Epoch - 48 Valid-Loss : 1.0249758875370025
micro_f1_48 = 0.71875 
macro_f1_48 = 0.702445477034574 
minloss 1.0249758875370025
just saved the best current model in epoch48, with acc1:0.702445477034574, and acc2:0.71875

Epoch - 49 Train-Loss : 0.9597275368869305

Epoch - 49 Valid-Loss : 1.0106710833311081
micro_f1_49 = 0.70875 
macro_f1_49 = 0.6946530933412491 
minloss 1.0106710833311081
just saved the best current model in epoch48, with acc1:0.702445477034574, and acc2:0.71875

Epoch - 50 Train-Loss : 0.9353614726662636

Epoch - 50 Valid-Loss : 1.0088762122392654
micro_f1_50 = 0.7137500000000001 
macro_f1_50 = 0.6968696870865307 
minloss 1.0088762122392654
just saved the best current model in epoch48, with acc1:0.702445477034574, and acc2:0.71875

Epoch - 51 Train-Loss : 0.9421103993058204

Epoch - 51 Valid-Loss : 1.0440397781133652
micro_f1_51 = 0.67125 
macro_f1_51 = 0.6555412573230402 
minloss 1.0088762122392654
just saved the best current model in epoch48, with acc1:0.702445477034574, and acc2:0.71875

Epoch - 52 Train-Loss : 0.9093390892446042

Epoch - 52 Valid-Loss : 0.9800757265090942
micro_f1_52 = 0.7 
macro_f1_52 = 0.6872877698863029 
minloss 0.9800757265090942
just saved the best current model in epoch48, with acc1:0.702445477034574, and acc2:0.71875

Epoch - 53 Train-Loss : 0.8928752539306879

Epoch - 53 Valid-Loss : 0.9850947779417037
micro_f1_53 = 0.7100000000000001 
macro_f1_53 = 0.7023857514961557 
minloss 0.9800757265090942
just saved the best current model in epoch48, with acc1:0.702445477034574, and acc2:0.71875

Epoch - 54 Train-Loss : 0.8715350364148616

Epoch - 54 Valid-Loss : 0.9784475976228714
micro_f1_54 = 0.7275000000000001 
macro_f1_54 = 0.710767037591669 
minloss 0.9784475976228714
just saved the best current model in epoch54, with acc1:0.710767037591669, and acc2:0.7275000000000001

Epoch - 55 Train-Loss : 0.8543985314667225

Epoch - 55 Valid-Loss : 0.9411952608823776
micro_f1_55 = 0.72125 
macro_f1_55 = 0.7049025555787258 
minloss 0.9411952608823776
just saved the best current model in epoch54, with acc1:0.710767037591669, and acc2:0.7275000000000001

Epoch - 56 Train-Loss : 0.8311791707575321

Epoch - 56 Valid-Loss : 0.9647866070270539
micro_f1_56 = 0.70375 
macro_f1_56 = 0.693877781644662 
minloss 0.9411952608823776
just saved the best current model in epoch54, with acc1:0.710767037591669, and acc2:0.7275000000000001

Epoch - 57 Train-Loss : 0.8132417376339436

Epoch - 57 Valid-Loss : 0.9510099685192108
micro_f1_57 = 0.715 
macro_f1_57 = 0.7017170140237322 
minloss 0.9411952608823776
just saved the best current model in epoch54, with acc1:0.710767037591669, and acc2:0.7275000000000001

Epoch - 58 Train-Loss : 0.8111002865433693

Epoch - 58 Valid-Loss : 0.9053912383317947
micro_f1_58 = 0.745 
macro_f1_58 = 0.7352823828450994 
minloss 0.9053912383317947
just saved the best current model in epoch58, with acc1:0.7352823828450994, and acc2:0.745

Epoch - 59 Train-Loss : 0.785788576900959

Epoch - 59 Valid-Loss : 0.8991323322057724
micro_f1_59 = 0.7425 
macro_f1_59 = 0.7296549016336182 
minloss 0.8991323322057724
just saved the best current model in epoch58, with acc1:0.7352823828450994, and acc2:0.745

Epoch - 60 Train-Loss : 0.7922718298435211

Epoch - 60 Valid-Loss : 0.9322658276557922
micro_f1_60 = 0.7250000000000001 
macro_f1_60 = 0.7103206178088115 
minloss 0.8991323322057724
just saved the best current model in epoch58, with acc1:0.7352823828450994, and acc2:0.745

Epoch - 61 Train-Loss : 0.7715898767113686

Epoch - 61 Valid-Loss : 0.9055972439050675
micro_f1_61 = 0.72625 
macro_f1_61 = 0.7167120472968523 
minloss 0.8991323322057724
just saved the best current model in epoch58, with acc1:0.7352823828450994, and acc2:0.745

Epoch - 62 Train-Loss : 0.7509284942597151

Epoch - 62 Valid-Loss : 0.8880657958984375
micro_f1_62 = 0.72875 
macro_f1_62 = 0.7178051936871925 
minloss 0.8880657958984375
just saved the best current model in epoch58, with acc1:0.7352823828450994, and acc2:0.745

Epoch - 63 Train-Loss : 0.7437973658740521

Epoch - 63 Valid-Loss : 0.8875511729717255
micro_f1_63 = 0.74 
macro_f1_63 = 0.7318672132954421 
minloss 0.8875511729717255
just saved the best current model in epoch58, with acc1:0.7352823828450994, and acc2:0.745

Epoch - 64 Train-Loss : 0.7425045193731785

Epoch - 64 Valid-Loss : 0.8761372590065002
micro_f1_64 = 0.7475 
macro_f1_64 = 0.7391673342228128 
minloss 0.8761372590065002
just saved the best current model in epoch64, with acc1:0.7391673342228128, and acc2:0.7475

Epoch - 65 Train-Loss : 0.7128679518401623

Epoch - 65 Valid-Loss : 0.8693436688184738
micro_f1_65 = 0.7337500000000001 
macro_f1_65 = 0.7205553683359543 
minloss 0.8693436688184738
just saved the best current model in epoch64, with acc1:0.7391673342228128, and acc2:0.7475

Epoch - 66 Train-Loss : 0.7114457451552153

Epoch - 66 Valid-Loss : 0.8919669139385223
micro_f1_66 = 0.735 
macro_f1_66 = 0.7214140751786883 
minloss 0.8693436688184738
just saved the best current model in epoch64, with acc1:0.7391673342228128, and acc2:0.7475

Epoch - 67 Train-Loss : 0.6660273534059524

Epoch - 67 Valid-Loss : 0.8508555990457535
micro_f1_67 = 0.7512500000000001 
macro_f1_67 = 0.7437052108357698 
minloss 0.8508555990457535
just saved the best current model in epoch67, with acc1:0.7437052108357698, and acc2:0.7512500000000001

Epoch - 68 Train-Loss : 0.6887134204804898

Epoch - 68 Valid-Loss : 0.8750766277313232
micro_f1_68 = 0.73625 
macro_f1_68 = 0.7276804448532951 
minloss 0.8508555990457535
just saved the best current model in epoch67, with acc1:0.7437052108357698, and acc2:0.7512500000000001

Epoch - 69 Train-Loss : 0.6611811021715402

Epoch - 69 Valid-Loss : 0.8509540033340454
micro_f1_69 = 0.7325 
macro_f1_69 = 0.7211452699710352 
minloss 0.8508555990457535
just saved the best current model in epoch67, with acc1:0.7437052108357698, and acc2:0.7512500000000001

Epoch - 70 Train-Loss : 0.6489492838084697

Epoch - 70 Valid-Loss : 0.8593406629562378
micro_f1_70 = 0.75 
macro_f1_70 = 0.7414757239051701 
minloss 0.8508555990457535
just saved the best current model in epoch67, with acc1:0.7437052108357698, and acc2:0.7512500000000001

Epoch - 71 Train-Loss : 0.6374339164048434

Epoch - 71 Valid-Loss : 0.8438226681947708
micro_f1_71 = 0.7525 
macro_f1_71 = 0.740368215139947 
minloss 0.8438226681947708
just saved the best current model in epoch67, with acc1:0.7437052108357698, and acc2:0.7512500000000001

Epoch - 72 Train-Loss : 0.6252669349312783

Epoch - 72 Valid-Loss : 0.8393031597137451
micro_f1_72 = 0.76375 
macro_f1_72 = 0.7554270437974852 
minloss 0.8393031597137451
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 73 Train-Loss : 0.631557984277606

Epoch - 73 Valid-Loss : 0.8339238995313645
micro_f1_73 = 0.74625 
macro_f1_73 = 0.7347724819679496 
minloss 0.8339238995313645
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 74 Train-Loss : 0.594841128885746

Epoch - 74 Valid-Loss : 0.8356525868177413
micro_f1_74 = 0.755 
macro_f1_74 = 0.7454374489849096 
minloss 0.8339238995313645
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 75 Train-Loss : 0.5901803530007601

Epoch - 75 Valid-Loss : 0.8225952780246735
micro_f1_75 = 0.7487500000000001 
macro_f1_75 = 0.7392432612221408 
minloss 0.8225952780246735
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 76 Train-Loss : 0.5643989037722349

Epoch - 76 Valid-Loss : 0.8225508695840835
micro_f1_76 = 0.75625 
macro_f1_76 = 0.7411101419805494 
minloss 0.8225508695840835
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 77 Train-Loss : 0.567124687731266

Epoch - 77 Valid-Loss : 0.8145178008079529
micro_f1_77 = 0.75875 
macro_f1_77 = 0.7504493443175968 
minloss 0.8145178008079529
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 78 Train-Loss : 0.5640205509960652

Epoch - 78 Valid-Loss : 0.809731525182724
micro_f1_78 = 0.7525 
macro_f1_78 = 0.7466734632669657 
minloss 0.809731525182724
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 79 Train-Loss : 0.5392533508688211

Epoch - 79 Valid-Loss : 0.8098590195178985
micro_f1_79 = 0.755 
macro_f1_79 = 0.7448510591145129 
minloss 0.809731525182724
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 80 Train-Loss : 0.5495217301696539

Epoch - 80 Valid-Loss : 0.8241404837369919
micro_f1_80 = 0.745 
macro_f1_80 = 0.7327325915321034 
minloss 0.809731525182724
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 81 Train-Loss : 0.5205823139846325

Epoch - 81 Valid-Loss : 0.8267512041330337
micro_f1_81 = 0.7575 
macro_f1_81 = 0.7482162481978659 
minloss 0.809731525182724
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 82 Train-Loss : 0.5230567866563797

Epoch - 82 Valid-Loss : 0.7970490616559982
micro_f1_82 = 0.7525 
macro_f1_82 = 0.7431285467022255 
minloss 0.7970490616559982
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 83 Train-Loss : 0.5097524598985911

Epoch - 83 Valid-Loss : 0.8135406422615051
micro_f1_83 = 0.76 
macro_f1_83 = 0.7477672879995267 
minloss 0.7970490616559982
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 84 Train-Loss : 0.5069270747900009

Epoch - 84 Valid-Loss : 0.8315450876951218
micro_f1_84 = 0.76 
macro_f1_84 = 0.7501128893269882 
minloss 0.7970490616559982
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 85 Train-Loss : 0.48432203620672226

Epoch - 85 Valid-Loss : 0.7836805075407028
micro_f1_85 = 0.76 
macro_f1_85 = 0.7551170520655376 
minloss 0.7836805075407028
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 86 Train-Loss : 0.4944763366878033

Epoch - 86 Valid-Loss : 0.7953463196754456
micro_f1_86 = 0.76375 
macro_f1_86 = 0.7539609246421278 
minloss 0.7836805075407028
just saved the best current model in epoch72, with acc1:0.7554270437974852, and acc2:0.76375

Epoch - 87 Train-Loss : 0.48996376037597655

Epoch - 87 Valid-Loss : 0.746543993651867
micro_f1_87 = 0.7699999999999999 
macro_f1_87 = 0.7590248408036381 
minloss 0.746543993651867
just saved the best current model in epoch87, with acc1:0.7590248408036381, and acc2:0.7699999999999999

Epoch - 88 Train-Loss : 0.4732253640145063

Epoch - 88 Valid-Loss : 0.7948777356743812
micro_f1_88 = 0.75625 
macro_f1_88 = 0.75033822484963 
minloss 0.746543993651867
just saved the best current model in epoch87, with acc1:0.7590248408036381, and acc2:0.7699999999999999

Epoch - 89 Train-Loss : 0.46974284388124943

Epoch - 89 Valid-Loss : 0.8007393366098404
micro_f1_89 = 0.76375 
macro_f1_89 = 0.757669806821224 
minloss 0.746543993651867
just saved the best current model in epoch87, with acc1:0.7590248408036381, and acc2:0.7699999999999999

Epoch - 90 Train-Loss : 0.4458921115472913

Epoch - 90 Valid-Loss : 0.7676716268062591
micro_f1_90 = 0.775 
macro_f1_90 = 0.7688821519206825 
minloss 0.746543993651867
just saved the best current model in epoch90, with acc1:0.7688821519206825, and acc2:0.775

Epoch - 91 Train-Loss : 0.45734041549265386

Epoch - 91 Valid-Loss : 0.7798757803440094
micro_f1_91 = 0.76 
macro_f1_91 = 0.7484677630134859 
minloss 0.746543993651867
just saved the best current model in epoch90, with acc1:0.7688821519206825, and acc2:0.775

Epoch - 92 Train-Loss : 0.4310781638324261

Epoch - 92 Valid-Loss : 0.7870836383104325
micro_f1_92 = 0.76 
macro_f1_92 = 0.7519192525569313 
minloss 0.746543993651867
just saved the best current model in epoch90, with acc1:0.7688821519206825, and acc2:0.775

Epoch - 93 Train-Loss : 0.4292719481140375

Epoch - 93 Valid-Loss : 0.7555533623695374
micro_f1_93 = 0.775 
macro_f1_93 = 0.7695863163687495 
minloss 0.746543993651867
just saved the best current model in epoch93, with acc1:0.7695863163687495, and acc2:0.775

Epoch - 94 Train-Loss : 0.41467982400208714

Epoch - 94 Valid-Loss : 0.7827627843618393
micro_f1_94 = 0.77125 
macro_f1_94 = 0.7646399240311407 
minloss 0.746543993651867
just saved the best current model in epoch93, with acc1:0.7695863163687495, and acc2:0.775

Epoch - 95 Train-Loss : 0.41260479189455507

Epoch - 95 Valid-Loss : 0.7379646214842797
micro_f1_95 = 0.77875 
macro_f1_95 = 0.7696740999055096 
minloss 0.7379646214842797
just saved the best current model in epoch95, with acc1:0.7696740999055096, and acc2:0.77875

Epoch - 96 Train-Loss : 0.40065955370664597

Epoch - 96 Valid-Loss : 0.7450095731019973
micro_f1_96 = 0.77125 
macro_f1_96 = 0.7624736668510812 
minloss 0.7379646214842797
just saved the best current model in epoch95, with acc1:0.7696740999055096, and acc2:0.77875

Epoch - 97 Train-Loss : 0.4084567869082093

Epoch - 97 Valid-Loss : 0.7640263414382935
micro_f1_97 = 0.7575 
macro_f1_97 = 0.7469980466397288 
minloss 0.7379646214842797
just saved the best current model in epoch95, with acc1:0.7696740999055096, and acc2:0.77875

Epoch - 98 Train-Loss : 0.4015551771223545

Epoch - 98 Valid-Loss : 0.7804406130313873
micro_f1_98 = 0.7725000000000001 
macro_f1_98 = 0.7625559947162722 
minloss 0.7379646214842797
just saved the best current model in epoch95, with acc1:0.7696740999055096, and acc2:0.77875

Epoch - 99 Train-Loss : 0.3989503760635853

Epoch - 99 Valid-Loss : 0.7206730934977531
micro_f1_99 = 0.78 
macro_f1_99 = 0.7724533339735521 
minloss 0.7206730934977531
just saved the best current model in epoch99, with acc1:0.7724533339735521, and acc2:0.78

Epoch - 100 Train-Loss : 0.3783902185037732

Epoch - 100 Valid-Loss : 0.7479724770784378
micro_f1_100 = 0.7725000000000001 
macro_f1_100 = 0.763240479835957 
minloss 0.7206730934977531
just saved the best current model in epoch99, with acc1:0.7724533339735521, and acc2:0.78

Epoch - 101 Train-Loss : 0.3621017501503229

Epoch - 101 Valid-Loss : 0.7575876691937447
micro_f1_101 = 0.7725000000000001 
macro_f1_101 = 0.7646500246445391 
minloss 0.7206730934977531
just saved the best current model in epoch99, with acc1:0.7724533339735521, and acc2:0.78

Epoch - 102 Train-Loss : 0.3615761211141944

Epoch - 102 Valid-Loss : 0.7135209634900093
micro_f1_102 = 0.7862500000000001 
macro_f1_102 = 0.7808082602939039 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 103 Train-Loss : 0.3518671281635761

Epoch - 103 Valid-Loss : 0.7388142496347427
micro_f1_103 = 0.77875 
macro_f1_103 = 0.7749618707310981 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 104 Train-Loss : 0.3513669489696622

Epoch - 104 Valid-Loss : 0.7575102218985558
micro_f1_104 = 0.775 
macro_f1_104 = 0.7690280716029635 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 105 Train-Loss : 0.338718189522624

Epoch - 105 Valid-Loss : 0.7440282154083252
micro_f1_105 = 0.77375 
macro_f1_105 = 0.7661648816921114 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 106 Train-Loss : 0.3355912054330111

Epoch - 106 Valid-Loss : 0.7421227033436298
micro_f1_106 = 0.78125 
macro_f1_106 = 0.7719852552425184 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 107 Train-Loss : 0.3277467926964164

Epoch - 107 Valid-Loss : 0.7402645286917686
micro_f1_107 = 0.775 
macro_f1_107 = 0.76458007030647 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 108 Train-Loss : 0.3200097130984068

Epoch - 108 Valid-Loss : 0.7518031084537506
micro_f1_108 = 0.775 
macro_f1_108 = 0.7653144403519342 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 109 Train-Loss : 0.3229471298679709

Epoch - 109 Valid-Loss : 0.7399805146455765
micro_f1_109 = 0.78375 
macro_f1_109 = 0.7713044898609627 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 110 Train-Loss : 0.300918409191072

Epoch - 110 Valid-Loss : 0.7678583914041519
micro_f1_110 = 0.7699999999999999 
macro_f1_110 = 0.7624693722770419 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 111 Train-Loss : 0.3106102691963315

Epoch - 111 Valid-Loss : 0.7716281870007515
micro_f1_111 = 0.7762499999999999 
macro_f1_111 = 0.7663977604973726 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 112 Train-Loss : 0.31491382922977207

Epoch - 112 Valid-Loss : 0.7644516879320145
micro_f1_112 = 0.7775 
macro_f1_112 = 0.7712560258192213 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 113 Train-Loss : 0.28471743380650877

Epoch - 113 Valid-Loss : 0.7593858063220977
micro_f1_113 = 0.7775 
macro_f1_113 = 0.7669146780934197 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 114 Train-Loss : 0.2999499324336648

Epoch - 114 Valid-Loss : 0.7223336124420165
micro_f1_114 = 0.7775 
macro_f1_114 = 0.7713303687701776 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 115 Train-Loss : 0.28204952336847783

Epoch - 115 Valid-Loss : 0.7633039724826812
micro_f1_115 = 0.7825 
macro_f1_115 = 0.774184101335092 
minloss 0.7135209634900093
just saved the best current model in epoch102, with acc1:0.7808082602939039, and acc2:0.7862500000000001

Epoch - 116 Train-Loss : 0.28413758536800743

Epoch - 116 Valid-Loss : 0.7328599199652672
micro_f1_116 = 0.79125 
macro_f1_116 = 0.7852656521774368 
minloss 0.7135209634900093
just saved the best current model in epoch116, with acc1:0.7852656521774368, and acc2:0.79125

Epoch - 117 Train-Loss : 0.26769834354519845

Epoch - 117 Valid-Loss : 0.7997962513566017
micro_f1_117 = 0.7675 
macro_f1_117 = 0.7562142614782951 
minloss 0.7135209634900093
just saved the best current model in epoch116, with acc1:0.7852656521774368, and acc2:0.79125

Epoch - 118 Train-Loss : 0.26495960224419834

Epoch - 118 Valid-Loss : 0.7698661923408509
micro_f1_118 = 0.7725000000000001 
macro_f1_118 = 0.7675057753897805 
minloss 0.7135209634900093
just saved the best current model in epoch116, with acc1:0.7852656521774368, and acc2:0.79125

Epoch - 119 Train-Loss : 0.28163722340017555

Epoch - 119 Valid-Loss : 0.7311768940091133
micro_f1_119 = 0.7825 
macro_f1_119 = 0.7777104240377579 
minloss 0.7135209634900093
just saved the best current model in epoch116, with acc1:0.7852656521774368, and acc2:0.79125

Epoch - 120 Train-Loss : 0.2687861051410437

Epoch - 120 Valid-Loss : 0.6830604404211045
micro_f1_120 = 0.77875 
macro_f1_120 = 0.7736674291062413 
minloss 0.6830604404211045
just saved the best current model in epoch116, with acc1:0.7852656521774368, and acc2:0.79125

Epoch - 121 Train-Loss : 0.25346318500116466

Epoch - 121 Valid-Loss : 0.720209347307682
micro_f1_121 = 0.79625 
macro_f1_121 = 0.7895344752755206 
minloss 0.6830604404211045
just saved the best current model in epoch121, with acc1:0.7895344752755206, and acc2:0.79625

Epoch - 122 Train-Loss : 0.24489235846325755

Epoch - 122 Valid-Loss : 0.6936902245879173
micro_f1_122 = 0.7862500000000001 
macro_f1_122 = 0.7785221678961095 
minloss 0.6830604404211045
just saved the best current model in epoch121, with acc1:0.7895344752755206, and acc2:0.79625

Epoch - 123 Train-Loss : 0.2531898291222751

Epoch - 123 Valid-Loss : 0.7160505989193916
micro_f1_123 = 0.79 
macro_f1_123 = 0.7855315635605694 
minloss 0.6830604404211045
just saved the best current model in epoch121, with acc1:0.7895344752755206, and acc2:0.79625

Epoch - 124 Train-Loss : 0.2317144588381052

Epoch - 124 Valid-Loss : 0.6917758828401566
micro_f1_124 = 0.79625 
macro_f1_124 = 0.7882497053541107 
minloss 0.6830604404211045
just saved the best current model in epoch121, with acc1:0.7895344752755206, and acc2:0.79625

Epoch - 125 Train-Loss : 0.24343155642971395

Epoch - 125 Valid-Loss : 0.7039699691534043
micro_f1_125 = 0.7887499999999998 
macro_f1_125 = 0.7819669032109654 
minloss 0.6830604404211045
just saved the best current model in epoch121, with acc1:0.7895344752755206, and acc2:0.79625

Epoch - 126 Train-Loss : 0.22514934880658985

Epoch - 126 Valid-Loss : 0.7012290716171264
micro_f1_126 = 0.79625 
macro_f1_126 = 0.7890465867252687 
minloss 0.6830604404211045
just saved the best current model in epoch121, with acc1:0.7895344752755206, and acc2:0.79625

Epoch - 127 Train-Loss : 0.23281871946528554

Epoch - 127 Valid-Loss : 0.8233645501732826
micro_f1_127 = 0.755 
macro_f1_127 = 0.7481822463317749 
minloss 0.6830604404211045
just saved the best current model in epoch121, with acc1:0.7895344752755206, and acc2:0.79625

Epoch - 128 Train-Loss : 0.22160351099446415

Epoch - 128 Valid-Loss : 0.6720848898589611
micro_f1_128 = 0.79375 
macro_f1_128 = 0.7883262520014308 
minloss 0.6720848898589611
just saved the best current model in epoch121, with acc1:0.7895344752755206, and acc2:0.79625

Epoch - 129 Train-Loss : 0.2156141552887857

Epoch - 129 Valid-Loss : 0.7846278813481331
micro_f1_129 = 0.7699999999999999 
macro_f1_129 = 0.7598948788134698 
minloss 0.6720848898589611
just saved the best current model in epoch121, with acc1:0.7895344752755206, and acc2:0.79625

Epoch - 130 Train-Loss : 0.20075809748843312

Epoch - 130 Valid-Loss : 0.7265783350169659
micro_f1_130 = 0.79375 
macro_f1_130 = 0.7882064756887026 
minloss 0.6720848898589611
just saved the best current model in epoch121, with acc1:0.7895344752755206, and acc2:0.79625

Epoch - 131 Train-Loss : 0.20654895688407124

Epoch - 131 Valid-Loss : 0.7458558687567711
micro_f1_131 = 0.77375 
macro_f1_131 = 0.7678879496951418 
minloss 0.6720848898589611
just saved the best current model in epoch121, with acc1:0.7895344752755206, and acc2:0.79625

Epoch - 132 Train-Loss : 0.18691467823460697

Epoch - 132 Valid-Loss : 0.6845454633235931
micro_f1_132 = 0.79875 
macro_f1_132 = 0.7960154008521053 
minloss 0.6720848898589611
just saved the best current model in epoch132, with acc1:0.7960154008521053, and acc2:0.79875

Epoch - 133 Train-Loss : 0.19656521286815404

Epoch - 133 Valid-Loss : 0.7587289971113205
micro_f1_133 = 0.78375 
macro_f1_133 = 0.7749960884664168 
minloss 0.6720848898589611
just saved the best current model in epoch132, with acc1:0.7960154008521053, and acc2:0.79875

Epoch - 134 Train-Loss : 0.1912297685071826

Epoch - 134 Valid-Loss : 0.7884536525607109
micro_f1_134 = 0.77125 
macro_f1_134 = 0.7621937952335007 
minloss 0.6720848898589611
just saved the best current model in epoch132, with acc1:0.7960154008521053, and acc2:0.79875

Epoch - 135 Train-Loss : 0.19681754102930427

Epoch - 135 Valid-Loss : 0.7072802847623825
micro_f1_135 = 0.79875 
macro_f1_135 = 0.7923614571237454 
minloss 0.6720848898589611
just saved the best current model in epoch132, with acc1:0.7960154008521053, and acc2:0.79875
(3, 3)
3
params = 3343378
Experiment's attempt changed to : 2


.......................................................................................................




.......................................................................................................


train_folds:  [1, 2, 3, 4]
valid_fold:  [5]
Loading_features......
Loading_features......
features are loaded
batch_size: 16
model_M2Dcnn2 initialized with total : 3343378 parameters.
 learning_rate = 1e-05, epochs = 135
 epochs = 135
Experiment's attempt no_ : 1
Train started..

Epoch - 1 Train-Loss : 3.8331345355510713

Epoch - 1 Valid-Loss : 3.6994987821578977
micro_f1_1 = 0.09875 
macro_f1_1 = 0.05292169226666076 
minloss 3.6994987821578977

Epoch - 2 Train-Loss : 3.6280601406097412

Epoch - 2 Valid-Loss : 3.5036210727691652
micro_f1_2 = 0.145 
macro_f1_2 = 0.08475590825671275 
minloss 3.5036210727691652

Epoch - 3 Train-Loss : 3.4384708511829376

Epoch - 3 Valid-Loss : 3.2806628561019897
micro_f1_3 = 0.185 
macro_f1_3 = 0.1252133747478983 
minloss 3.2806628561019897

Epoch - 4 Train-Loss : 3.2285460209846497

Epoch - 4 Valid-Loss : 3.098082995414734
micro_f1_4 = 0.20999999999999996 
macro_f1_4 = 0.1490262781211435 
minloss 3.098082995414734

Epoch - 5 Train-Loss : 3.0497402918338774

Epoch - 5 Valid-Loss : 2.9187268447875976
micro_f1_5 = 0.2425 
macro_f1_5 = 0.17433208772685027 
minloss 2.9187268447875976

Epoch - 6 Train-Loss : 2.8800046956539154

Epoch - 6 Valid-Loss : 2.7920042991638185
micro_f1_6 = 0.29625 
macro_f1_6 = 0.24048721527375544 
minloss 2.7920042991638185

Epoch - 7 Train-Loss : 2.7490687084198

Epoch - 7 Valid-Loss : 2.6461179637908936
micro_f1_7 = 0.3325 
macro_f1_7 = 0.2788187014784001 
minloss 2.6461179637908936

Epoch - 8 Train-Loss : 2.616505229473114

Epoch - 8 Valid-Loss : 2.557980396747589
micro_f1_8 = 0.37375 
macro_f1_8 = 0.3175441619011558 
minloss 2.557980396747589

Epoch - 9 Train-Loss : 2.5105856359004974

Epoch - 9 Valid-Loss : 2.4470190668106078
micro_f1_9 = 0.3775 
macro_f1_9 = 0.33422028309837903 
minloss 2.4470190668106078

Epoch - 10 Train-Loss : 2.382485223412514

Epoch - 10 Valid-Loss : 2.3625561428070068
micro_f1_10 = 0.40875 
macro_f1_10 = 0.3611581868945775 
minloss 2.3625561428070068

Epoch - 11 Train-Loss : 2.303232427239418

Epoch - 11 Valid-Loss : 2.2800817847251893
micro_f1_11 = 0.42999999999999994 
macro_f1_11 = 0.39235615933483553 
minloss 2.2800817847251893

Epoch - 12 Train-Loss : 2.20588052213192

Epoch - 12 Valid-Loss : 2.2176312732696535
micro_f1_12 = 0.415 
macro_f1_12 = 0.3689277207046142 
minloss 2.2176312732696535

Epoch - 13 Train-Loss : 2.119732475876808

Epoch - 13 Valid-Loss : 2.1267012572288513
micro_f1_13 = 0.43375 
macro_f1_13 = 0.39653608402174484 
minloss 2.1267012572288513

Epoch - 14 Train-Loss : 2.0746276247501374

Epoch - 14 Valid-Loss : 2.076770236492157
micro_f1_14 = 0.4425 
macro_f1_14 = 0.40949411165874783 
minloss 2.076770236492157

Epoch - 15 Train-Loss : 1.9770758932828902

Epoch - 15 Valid-Loss : 2.0344609212875366
micro_f1_15 = 0.45625 
macro_f1_15 = 0.41690842602927125 
minloss 2.0344609212875366

Epoch - 16 Train-Loss : 1.938958055973053

Epoch - 16 Valid-Loss : 1.9985214352607727
micro_f1_16 = 0.46875 
macro_f1_16 = 0.431873198428397 
minloss 1.9985214352607727

Epoch - 17 Train-Loss : 1.8758448070287705

Epoch - 17 Valid-Loss : 1.9818053889274596
micro_f1_17 = 0.4825 
macro_f1_17 = 0.44893641035719073 
minloss 1.9818053889274596

Epoch - 18 Train-Loss : 1.8132397758960723

Epoch - 18 Valid-Loss : 1.926274037361145
micro_f1_18 = 0.49125 
macro_f1_18 = 0.45643837762616823 
minloss 1.926274037361145

Epoch - 19 Train-Loss : 1.772641469836235

Epoch - 19 Valid-Loss : 1.881185233592987
micro_f1_19 = 0.50125 
macro_f1_19 = 0.4736451330042562 
minloss 1.881185233592987

Epoch - 20 Train-Loss : 1.7211237508058548

Epoch - 20 Valid-Loss : 1.8329681181907653
micro_f1_20 = 0.5125 
macro_f1_20 = 0.4806708137398379 
minloss 1.8329681181907653

Epoch - 21 Train-Loss : 1.6794984757900238

Epoch - 21 Valid-Loss : 1.7759044468402863
micro_f1_21 = 0.5225 
macro_f1_21 = 0.49156065450617725 
minloss 1.7759044468402863
just saved the best current model in epoch21, with acc1:0.49156065450617725, and acc2:0.5225

Epoch - 22 Train-Loss : 1.647661316394806

Epoch - 22 Valid-Loss : 1.7664489889144896
micro_f1_22 = 0.53 
macro_f1_22 = 0.5043617867960366 
minloss 1.7664489889144896
just saved the best current model in epoch22, with acc1:0.5043617867960366, and acc2:0.53

Epoch - 23 Train-Loss : 1.600906320810318

Epoch - 23 Valid-Loss : 1.7161755061149597
micro_f1_23 = 0.53875 
macro_f1_23 = 0.5099653460391874 
minloss 1.7161755061149597
just saved the best current model in epoch23, with acc1:0.5099653460391874, and acc2:0.53875

Epoch - 24 Train-Loss : 1.5575747022032738

Epoch - 24 Valid-Loss : 1.6968639087677002
micro_f1_24 = 0.54875 
macro_f1_24 = 0.5196390919693047 
minloss 1.6968639087677002
just saved the best current model in epoch24, with acc1:0.5196390919693047, and acc2:0.54875

Epoch - 25 Train-Loss : 1.5411468631029128

Epoch - 25 Valid-Loss : 1.6672476816177368
micro_f1_25 = 0.55625 
macro_f1_25 = 0.5301283500063462 
minloss 1.6672476816177368
just saved the best current model in epoch25, with acc1:0.5301283500063462, and acc2:0.55625

Epoch - 26 Train-Loss : 1.4818173947930335

Epoch - 26 Valid-Loss : 1.6399838364124297
micro_f1_26 = 0.55 
macro_f1_26 = 0.527480154301885 
minloss 1.6399838364124297
just saved the best current model in epoch25, with acc1:0.5301283500063462, and acc2:0.55625

Epoch - 27 Train-Loss : 1.4518221336603165

Epoch - 27 Valid-Loss : 1.6226523077487947
micro_f1_27 = 0.56625 
macro_f1_27 = 0.5378929941961546 
minloss 1.6226523077487947
just saved the best current model in epoch27, with acc1:0.5378929941961546, and acc2:0.56625

Epoch - 28 Train-Loss : 1.408247908949852

Epoch - 28 Valid-Loss : 1.6153993153572082
micro_f1_28 = 0.57875 
macro_f1_28 = 0.5551836879560188 
minloss 1.6153993153572082
just saved the best current model in epoch28, with acc1:0.5551836879560188, and acc2:0.57875

Epoch - 29 Train-Loss : 1.3762022200226784

Epoch - 29 Valid-Loss : 1.6141207659244536
micro_f1_29 = 0.55375 
macro_f1_29 = 0.5279751436775298 
minloss 1.6141207659244536
just saved the best current model in epoch28, with acc1:0.5551836879560188, and acc2:0.57875

Epoch - 30 Train-Loss : 1.3397841361165046

Epoch - 30 Valid-Loss : 1.5524775731563567
micro_f1_30 = 0.5775 
macro_f1_30 = 0.551320044055601 
minloss 1.5524775731563567
just saved the best current model in epoch28, with acc1:0.5551836879560188, and acc2:0.57875

Epoch - 31 Train-Loss : 1.3110771110653878

Epoch - 31 Valid-Loss : 1.5418172729015351
micro_f1_31 = 0.57375 
macro_f1_31 = 0.5503987042774883 
minloss 1.5418172729015351
just saved the best current model in epoch28, with acc1:0.5551836879560188, and acc2:0.57875

Epoch - 32 Train-Loss : 1.2908221888542175

Epoch - 32 Valid-Loss : 1.533956549167633
micro_f1_32 = 0.58875 
macro_f1_32 = 0.5624716965035476 
minloss 1.533956549167633
just saved the best current model in epoch32, with acc1:0.5624716965035476, and acc2:0.58875

Epoch - 33 Train-Loss : 1.2751415050029755

Epoch - 33 Valid-Loss : 1.5062581622600555
micro_f1_33 = 0.5875 
macro_f1_33 = 0.5624868671222931 
minloss 1.5062581622600555
just saved the best current model in epoch32, with acc1:0.5624716965035476, and acc2:0.58875

Epoch - 34 Train-Loss : 1.2234605914354324

Epoch - 34 Valid-Loss : 1.4873717272281646
micro_f1_34 = 0.6 
macro_f1_34 = 0.5764009877122854 
minloss 1.4873717272281646
just saved the best current model in epoch34, with acc1:0.5764009877122854, and acc2:0.6

Epoch - 35 Train-Loss : 1.1873732057213784

Epoch - 35 Valid-Loss : 1.549303274154663
micro_f1_35 = 0.585 
macro_f1_35 = 0.5528049455171574 
minloss 1.4873717272281646
just saved the best current model in epoch34, with acc1:0.5764009877122854, and acc2:0.6

Epoch - 36 Train-Loss : 1.155928595662117

Epoch - 36 Valid-Loss : 1.468689579963684
micro_f1_36 = 0.59875 
macro_f1_36 = 0.5686616640724764 
minloss 1.468689579963684
just saved the best current model in epoch34, with acc1:0.5764009877122854, and acc2:0.6

Epoch - 37 Train-Loss : 1.1668574586510658

Epoch - 37 Valid-Loss : 1.4887200748920442
micro_f1_37 = 0.60875 
macro_f1_37 = 0.5931344261732623 
minloss 1.468689579963684
just saved the best current model in epoch37, with acc1:0.5931344261732623, and acc2:0.60875

Epoch - 38 Train-Loss : 1.1319191285967827

Epoch - 38 Valid-Loss : 1.483378975391388
micro_f1_38 = 0.59625 
macro_f1_38 = 0.5717200989785549 
minloss 1.468689579963684
just saved the best current model in epoch37, with acc1:0.5931344261732623, and acc2:0.60875

Epoch - 39 Train-Loss : 1.1096432852745055

Epoch - 39 Valid-Loss : 1.453237533569336
micro_f1_39 = 0.6025 
macro_f1_39 = 0.5759901460525174 
minloss 1.453237533569336
just saved the best current model in epoch37, with acc1:0.5931344261732623, and acc2:0.60875

Epoch - 40 Train-Loss : 1.1085200029611588

Epoch - 40 Valid-Loss : 1.4402875316143036
micro_f1_40 = 0.605 
macro_f1_40 = 0.5794599146911587 
minloss 1.4402875316143036
just saved the best current model in epoch37, with acc1:0.5931344261732623, and acc2:0.60875

Epoch - 41 Train-Loss : 1.0752744245529176

Epoch - 41 Valid-Loss : 1.4404254496097564
micro_f1_41 = 0.5975 
macro_f1_41 = 0.5753870948098015 
minloss 1.4402875316143036
just saved the best current model in epoch37, with acc1:0.5931344261732623, and acc2:0.60875

Epoch - 42 Train-Loss : 1.0340312440693378

Epoch - 42 Valid-Loss : 1.4260338997840882
micro_f1_42 = 0.6125 
macro_f1_42 = 0.5905344742068948 
minloss 1.4260338997840882
just saved the best current model in epoch42, with acc1:0.5905344742068948, and acc2:0.6125

Epoch - 43 Train-Loss : 1.015844396650791

Epoch - 43 Valid-Loss : 1.4087366676330566
micro_f1_43 = 0.61 
macro_f1_43 = 0.5933032257651516 
minloss 1.4087366676330566
just saved the best current model in epoch43, with acc1:0.5933032257651516, and acc2:0.61

Epoch - 44 Train-Loss : 0.9959327435493469

Epoch - 44 Valid-Loss : 1.3932359313964844
micro_f1_44 = 0.62 
macro_f1_44 = 0.6024628372452699 
minloss 1.3932359313964844
just saved the best current model in epoch44, with acc1:0.6024628372452699, and acc2:0.62

Epoch - 45 Train-Loss : 0.9775976645946503

Epoch - 45 Valid-Loss : 1.4205135941505431
micro_f1_45 = 0.61375 
macro_f1_45 = 0.5979034217844551 
minloss 1.3932359313964844
just saved the best current model in epoch44, with acc1:0.6024628372452699, and acc2:0.62

Epoch - 46 Train-Loss : 0.9463348037004471

Epoch - 46 Valid-Loss : 1.38206662774086
micro_f1_46 = 0.6175 
macro_f1_46 = 0.596963143987573 
minloss 1.38206662774086
just saved the best current model in epoch44, with acc1:0.6024628372452699, and acc2:0.62

Epoch - 47 Train-Loss : 0.9302688606083394

Epoch - 47 Valid-Loss : 1.3663343262672425
micro_f1_47 = 0.62625 
macro_f1_47 = 0.60279319690246 
minloss 1.3663343262672425
just saved the best current model in epoch47, with acc1:0.60279319690246, and acc2:0.62625

Epoch - 48 Train-Loss : 0.9285098597407341

Epoch - 48 Valid-Loss : 1.3398819077014923
micro_f1_48 = 0.62875 
macro_f1_48 = 0.6023328060581742 
minloss 1.3398819077014923
just saved the best current model in epoch48, with acc1:0.6023328060581742, and acc2:0.62875

Epoch - 49 Train-Loss : 0.8945547750592232

Epoch - 49 Valid-Loss : 1.3691625273227692
micro_f1_49 = 0.61625 
macro_f1_49 = 0.5959311504906345 
minloss 1.3398819077014923
just saved the best current model in epoch48, with acc1:0.6023328060581742, and acc2:0.62875

Epoch - 50 Train-Loss : 0.9109078121185302

Epoch - 50 Valid-Loss : 1.340443321466446
micro_f1_50 = 0.6325 
macro_f1_50 = 0.612843526035508 
minloss 1.3398819077014923
just saved the best current model in epoch50, with acc1:0.612843526035508, and acc2:0.6325

Epoch - 51 Train-Loss : 0.8683684232831002

Epoch - 51 Valid-Loss : 1.3890603709220886
micro_f1_51 = 0.61375 
macro_f1_51 = 0.593637099994619 
minloss 1.3398819077014923
just saved the best current model in epoch50, with acc1:0.612843526035508, and acc2:0.6325

Epoch - 52 Train-Loss : 0.84570348829031

Epoch - 52 Valid-Loss : 1.3384782147407532
micro_f1_52 = 0.6225 
macro_f1_52 = 0.6013683721172536 
minloss 1.3384782147407532
just saved the best current model in epoch50, with acc1:0.612843526035508, and acc2:0.6325

Epoch - 53 Train-Loss : 0.8269110576808453

Epoch - 53 Valid-Loss : 1.3195195066928864
micro_f1_53 = 0.64125 
macro_f1_53 = 0.6226399219650183 
minloss 1.3195195066928864
just saved the best current model in epoch53, with acc1:0.6226399219650183, and acc2:0.64125

Epoch - 54 Train-Loss : 0.8248634526133537

Epoch - 54 Valid-Loss : 1.2934794306755066
micro_f1_54 = 0.6275 
macro_f1_54 = 0.6059104066770352 
minloss 1.2934794306755066
just saved the best current model in epoch53, with acc1:0.6226399219650183, and acc2:0.64125

Epoch - 55 Train-Loss : 0.8060971069335937

Epoch - 55 Valid-Loss : 1.3064962434768677
micro_f1_55 = 0.635 
macro_f1_55 = 0.6157225144750677 
minloss 1.2934794306755066
just saved the best current model in epoch53, with acc1:0.6226399219650183, and acc2:0.64125

Epoch - 56 Train-Loss : 0.7734509697556495

Epoch - 56 Valid-Loss : 1.3069287526607514
micro_f1_56 = 0.6325 
macro_f1_56 = 0.6138881336361538 
minloss 1.2934794306755066
just saved the best current model in epoch53, with acc1:0.6226399219650183, and acc2:0.64125

Epoch - 57 Train-Loss : 0.7715020404756069

Epoch - 57 Valid-Loss : 1.329537633061409
micro_f1_57 = 0.63625 
macro_f1_57 = 0.6121987962246213 
minloss 1.2934794306755066
just saved the best current model in epoch53, with acc1:0.6226399219650183, and acc2:0.64125

Epoch - 58 Train-Loss : 0.732907342761755

Epoch - 58 Valid-Loss : 1.297448456287384
micro_f1_58 = 0.64 
macro_f1_58 = 0.6189837130982352 
minloss 1.2934794306755066
just saved the best current model in epoch53, with acc1:0.6226399219650183, and acc2:0.64125

Epoch - 59 Train-Loss : 0.7424700343608857

Epoch - 59 Valid-Loss : 1.3440467405319214
micro_f1_59 = 0.62625 
macro_f1_59 = 0.6026909476157446 
minloss 1.2934794306755066
just saved the best current model in epoch53, with acc1:0.6226399219650183, and acc2:0.64125

Epoch - 60 Train-Loss : 0.7293346673250198

Epoch - 60 Valid-Loss : 1.283581166267395
micro_f1_60 = 0.655 
macro_f1_60 = 0.6404549500079005 
minloss 1.283581166267395
just saved the best current model in epoch60, with acc1:0.6404549500079005, and acc2:0.655

Epoch - 61 Train-Loss : 0.7150126980245113

Epoch - 61 Valid-Loss : 1.3101419770717622
micro_f1_61 = 0.64 
macro_f1_61 = 0.6238852290175999 
minloss 1.283581166267395
just saved the best current model in epoch60, with acc1:0.6404549500079005, and acc2:0.655

Epoch - 62 Train-Loss : 0.6954712690412999

Epoch - 62 Valid-Loss : 1.2941449636220932
micro_f1_62 = 0.645 
macro_f1_62 = 0.6180842506827458 
minloss 1.283581166267395
just saved the best current model in epoch60, with acc1:0.6404549500079005, and acc2:0.655

Epoch - 63 Train-Loss : 0.6929472640156746

Epoch - 63 Valid-Loss : 1.2733953124284745
micro_f1_63 = 0.64875 
macro_f1_63 = 0.6287365942991635 
minloss 1.2733953124284745
just saved the best current model in epoch60, with acc1:0.6404549500079005, and acc2:0.655

Epoch - 64 Train-Loss : 0.6528900487720967

Epoch - 64 Valid-Loss : 1.294577196240425
micro_f1_64 = 0.64625 
macro_f1_64 = 0.6266650608457568 
minloss 1.2733953124284745
just saved the best current model in epoch60, with acc1:0.6404549500079005, and acc2:0.655

Epoch - 65 Train-Loss : 0.6638580313324929

Epoch - 65 Valid-Loss : 1.2668726497888565
micro_f1_65 = 0.65875 
macro_f1_65 = 0.6454794412185376 
minloss 1.2668726497888565
just saved the best current model in epoch65, with acc1:0.6454794412185376, and acc2:0.65875

Epoch - 66 Train-Loss : 0.6519153130054474

Epoch - 66 Valid-Loss : 1.2891453796625136
micro_f1_66 = 0.64 
macro_f1_66 = 0.621416106484849 
minloss 1.2668726497888565
just saved the best current model in epoch65, with acc1:0.6454794412185376, and acc2:0.65875

Epoch - 67 Train-Loss : 0.6327358140051365

Epoch - 67 Valid-Loss : 1.2322269302606583
micro_f1_67 = 0.65375 
macro_f1_67 = 0.6340317202750017 
minloss 1.2322269302606583
just saved the best current model in epoch65, with acc1:0.6454794412185376, and acc2:0.65875

Epoch - 68 Train-Loss : 0.6293765395879746

Epoch - 68 Valid-Loss : 1.264369797706604
micro_f1_68 = 0.64375 
macro_f1_68 = 0.6275663951426724 
minloss 1.2322269302606583
just saved the best current model in epoch65, with acc1:0.6454794412185376, and acc2:0.65875

Epoch - 69 Train-Loss : 0.6153105695545673

Epoch - 69 Valid-Loss : 1.2428744590282441
micro_f1_69 = 0.6525 
macro_f1_69 = 0.6261456446208951 
minloss 1.2322269302606583
just saved the best current model in epoch65, with acc1:0.6454794412185376, and acc2:0.65875

Epoch - 70 Train-Loss : 0.60548105917871

Epoch - 70 Valid-Loss : 1.2333736717700958
micro_f1_70 = 0.64625 
macro_f1_70 = 0.6292284680862358 
minloss 1.2322269302606583
just saved the best current model in epoch65, with acc1:0.6454794412185376, and acc2:0.65875

Epoch - 71 Train-Loss : 0.5864750558137893

Epoch - 71 Valid-Loss : 1.3130716037750245
micro_f1_71 = 0.655 
macro_f1_71 = 0.641188929267065 
minloss 1.2322269302606583
just saved the best current model in epoch65, with acc1:0.6454794412185376, and acc2:0.65875

Epoch - 72 Train-Loss : 0.5682494066655636

Epoch - 72 Valid-Loss : 1.2367457085847855
micro_f1_72 = 0.6675 
macro_f1_72 = 0.6541798618435791 
minloss 1.2322269302606583
just saved the best current model in epoch72, with acc1:0.6541798618435791, and acc2:0.6675

Epoch - 73 Train-Loss : 0.5761502555757761

Epoch - 73 Valid-Loss : 1.1906700187921524
micro_f1_73 = 0.67625 
macro_f1_73 = 0.6617014844567963 
minloss 1.1906700187921524
just saved the best current model in epoch73, with acc1:0.6617014844567963, and acc2:0.67625

Epoch - 74 Train-Loss : 0.5550532309710979

Epoch - 74 Valid-Loss : 1.2782339918613435
micro_f1_74 = 0.65625 
macro_f1_74 = 0.6393108193761248 
minloss 1.1906700187921524
just saved the best current model in epoch73, with acc1:0.6617014844567963, and acc2:0.67625

Epoch - 75 Train-Loss : 0.5454438737034798

Epoch - 75 Valid-Loss : 1.265941231250763
micro_f1_75 = 0.64875 
macro_f1_75 = 0.6265384814687652 
minloss 1.1906700187921524
just saved the best current model in epoch73, with acc1:0.6617014844567963, and acc2:0.67625

Epoch - 76 Train-Loss : 0.5402303891628981

Epoch - 76 Valid-Loss : 1.230930587053299
micro_f1_76 = 0.65625 
macro_f1_76 = 0.638539983623064 
minloss 1.1906700187921524
just saved the best current model in epoch73, with acc1:0.6617014844567963, and acc2:0.67625

Epoch - 77 Train-Loss : 0.5123132200539112

Epoch - 77 Valid-Loss : 1.3133882927894591
micro_f1_77 = 0.66 
macro_f1_77 = 0.6451324278969791 
minloss 1.1906700187921524
just saved the best current model in epoch73, with acc1:0.6617014844567963, and acc2:0.67625

Epoch - 78 Train-Loss : 0.5236218009889125

Epoch - 78 Valid-Loss : 1.292364965081215
micro_f1_78 = 0.65125 
macro_f1_78 = 0.6278296155040664 
minloss 1.1906700187921524
just saved the best current model in epoch73, with acc1:0.6617014844567963, and acc2:0.67625

Epoch - 79 Train-Loss : 0.5093957668915391

Epoch - 79 Valid-Loss : 1.2469464468955993
micro_f1_79 = 0.66875 
macro_f1_79 = 0.6516083551711902 
minloss 1.1906700187921524
just saved the best current model in epoch73, with acc1:0.6617014844567963, and acc2:0.67625

Epoch - 80 Train-Loss : 0.49328412793576715

Epoch - 80 Valid-Loss : 1.2599232077598572
micro_f1_80 = 0.67125 
macro_f1_80 = 0.6500642629013843 
minloss 1.1906700187921524
just saved the best current model in epoch73, with acc1:0.6617014844567963, and acc2:0.67625

Epoch - 81 Train-Loss : 0.48827405847609046

Epoch - 81 Valid-Loss : 1.2416649395227433
micro_f1_81 = 0.67 
macro_f1_81 = 0.6507379386469097 
minloss 1.1906700187921524
just saved the best current model in epoch73, with acc1:0.6617014844567963, and acc2:0.67625

Epoch - 82 Train-Loss : 0.477585061788559

Epoch - 82 Valid-Loss : 1.2281872844696045
micro_f1_82 = 0.665 
macro_f1_82 = 0.652454469812061 
minloss 1.1906700187921524
just saved the best current model in epoch73, with acc1:0.6617014844567963, and acc2:0.67625

Epoch - 83 Train-Loss : 0.4805785046517849

Epoch - 83 Valid-Loss : 1.1899057841300964
micro_f1_83 = 0.6825 
macro_f1_83 = 0.6628388543464796 
minloss 1.1899057841300964
just saved the best current model in epoch83, with acc1:0.6628388543464796, and acc2:0.6825

Epoch - 84 Train-Loss : 0.47100842379033564

Epoch - 84 Valid-Loss : 1.2354156279563904
micro_f1_84 = 0.6625 
macro_f1_84 = 0.6456639163821027 
minloss 1.1899057841300964
just saved the best current model in epoch83, with acc1:0.6628388543464796, and acc2:0.6825

Epoch - 85 Train-Loss : 0.44255558371543885

Epoch - 85 Valid-Loss : 1.1870932668447494
micro_f1_85 = 0.67 
macro_f1_85 = 0.6519614356309994 
minloss 1.1870932668447494
just saved the best current model in epoch83, with acc1:0.6628388543464796, and acc2:0.6825

Epoch - 86 Train-Loss : 0.43167004123330116

Epoch - 86 Valid-Loss : 1.2221055030822754
micro_f1_86 = 0.68 
macro_f1_86 = 0.6660483964805579 
minloss 1.1870932668447494
just saved the best current model in epoch86, with acc1:0.6660483964805579, and acc2:0.68

Epoch - 87 Train-Loss : 0.4564369450882077

Epoch - 87 Valid-Loss : 1.26291876912117
micro_f1_87 = 0.67 
macro_f1_87 = 0.6536576304076259 
minloss 1.1870932668447494
just saved the best current model in epoch86, with acc1:0.6660483964805579, and acc2:0.68

Epoch - 88 Train-Loss : 0.4302150221541524

Epoch - 88 Valid-Loss : 1.2252856388688087
micro_f1_88 = 0.67 
macro_f1_88 = 0.6528503451836987 
minloss 1.1870932668447494
just saved the best current model in epoch86, with acc1:0.6660483964805579, and acc2:0.68

Epoch - 89 Train-Loss : 0.40215428490191696

Epoch - 89 Valid-Loss : 1.2132825326919556
micro_f1_89 = 0.67125 
macro_f1_89 = 0.6557271304530745 
minloss 1.1870932668447494
just saved the best current model in epoch86, with acc1:0.6660483964805579, and acc2:0.68

Epoch - 90 Train-Loss : 0.4143732062727213

Epoch - 90 Valid-Loss : 1.2222408509254457
micro_f1_90 = 0.67125 
macro_f1_90 = 0.6565124728694185 
minloss 1.1870932668447494
just saved the best current model in epoch86, with acc1:0.6660483964805579, and acc2:0.68

Epoch - 91 Train-Loss : 0.3992643813788891

Epoch - 91 Valid-Loss : 1.224446285367012
micro_f1_91 = 0.67875 
macro_f1_91 = 0.6588333243200063 
minloss 1.1870932668447494
just saved the best current model in epoch86, with acc1:0.6660483964805579, and acc2:0.68

Epoch - 92 Train-Loss : 0.38734692223370076

Epoch - 92 Valid-Loss : 1.2039857929944993
micro_f1_92 = 0.6825 
macro_f1_92 = 0.6644917676944072 
minloss 1.1870932668447494
just saved the best current model in epoch92, with acc1:0.6644917676944072, and acc2:0.6825

Epoch - 93 Train-Loss : 0.40087647188454867

Epoch - 93 Valid-Loss : 1.2192284506559372
micro_f1_93 = 0.6675 
macro_f1_93 = 0.6507695183809613 
minloss 1.1870932668447494
just saved the best current model in epoch92, with acc1:0.6644917676944072, and acc2:0.6825

Epoch - 94 Train-Loss : 0.3937159491702914

Epoch - 94 Valid-Loss : 1.2292582559585572
micro_f1_94 = 0.675 
macro_f1_94 = 0.659005780745371 
minloss 1.1870932668447494
just saved the best current model in epoch92, with acc1:0.6644917676944072, and acc2:0.6825

Epoch - 95 Train-Loss : 0.37697096068412067

Epoch - 95 Valid-Loss : 1.2251143115758896
micro_f1_95 = 0.6775 
macro_f1_95 = 0.6621223739220716 
minloss 1.1870932668447494
just saved the best current model in epoch92, with acc1:0.6644917676944072, and acc2:0.6825

Epoch - 96 Train-Loss : 0.3753620940074325

Epoch - 96 Valid-Loss : 1.2175448602437973
micro_f1_96 = 0.67 
macro_f1_96 = 0.6475085625574336 
minloss 1.1870932668447494
just saved the best current model in epoch92, with acc1:0.6644917676944072, and acc2:0.6825

Epoch - 97 Train-Loss : 0.3571917295828462

Epoch - 97 Valid-Loss : 1.1743186604976654
micro_f1_97 = 0.68125 
macro_f1_97 = 0.6658603852973869 
minloss 1.1743186604976654
just saved the best current model in epoch97, with acc1:0.6658603852973869, and acc2:0.68125

Epoch - 98 Train-Loss : 0.3600723749399185

Epoch - 98 Valid-Loss : 1.2655675613880157
micro_f1_98 = 0.665 
macro_f1_98 = 0.6511446478779317 
minloss 1.1743186604976654
just saved the best current model in epoch97, with acc1:0.6658603852973869, and acc2:0.68125

Epoch - 99 Train-Loss : 0.33191621853038666

Epoch - 99 Valid-Loss : 1.1565149819850922
micro_f1_99 = 0.68125 
macro_f1_99 = 0.6675421627889279 
minloss 1.1565149819850922
just saved the best current model in epoch99, with acc1:0.6675421627889279, and acc2:0.68125

Epoch - 100 Train-Loss : 0.33089585725218057

Epoch - 100 Valid-Loss : 1.208418337702751
micro_f1_100 = 0.67625 
macro_f1_100 = 0.6615140169219518 
minloss 1.1565149819850922
just saved the best current model in epoch99, with acc1:0.6675421627889279, and acc2:0.68125

Epoch - 101 Train-Loss : 0.3261836059764028

Epoch - 101 Valid-Loss : 1.1905585283041
micro_f1_101 = 0.6775 
macro_f1_101 = 0.6586100985276861 
minloss 1.1565149819850922
just saved the best current model in epoch99, with acc1:0.6675421627889279, and acc2:0.68125

Epoch - 102 Train-Loss : 0.31781855147331955

Epoch - 102 Valid-Loss : 1.2124752455949783
micro_f1_102 = 0.6825 
macro_f1_102 = 0.659963034802792 
minloss 1.1565149819850922
just saved the best current model in epoch99, with acc1:0.6675421627889279, and acc2:0.68125

Epoch - 103 Train-Loss : 0.3180159618705511

Epoch - 103 Valid-Loss : 1.1908755844831467
micro_f1_103 = 0.6825 
macro_f1_103 = 0.6728540274975168 
minloss 1.1565149819850922
just saved the best current model in epoch103, with acc1:0.6728540274975168, and acc2:0.6825

Epoch - 104 Train-Loss : 0.31683493994176387

Epoch - 104 Valid-Loss : 1.1901794970035553
micro_f1_104 = 0.6825 
macro_f1_104 = 0.6703838676743652 
minloss 1.1565149819850922
just saved the best current model in epoch103, with acc1:0.6728540274975168, and acc2:0.6825

Epoch - 105 Train-Loss : 0.2999956799298525

Epoch - 105 Valid-Loss : 1.2556613081693648
micro_f1_105 = 0.685 
macro_f1_105 = 0.6684868802648232 
minloss 1.1565149819850922
just saved the best current model in epoch103, with acc1:0.6728540274975168, and acc2:0.6825

Epoch - 106 Train-Loss : 0.311433471031487

Epoch - 106 Valid-Loss : 1.1848894229531288
micro_f1_106 = 0.69375 
macro_f1_106 = 0.6768695067460194 
minloss 1.1565149819850922
just saved the best current model in epoch106, with acc1:0.6768695067460194, and acc2:0.69375

Epoch - 107 Train-Loss : 0.2928071434609592

Epoch - 107 Valid-Loss : 1.1808436566591263
micro_f1_107 = 0.67375 
macro_f1_107 = 0.6569442465679768 
minloss 1.1565149819850922
just saved the best current model in epoch106, with acc1:0.6768695067460194, and acc2:0.69375

Epoch - 108 Train-Loss : 0.29236919397488237

Epoch - 108 Valid-Loss : 1.2098148348927498
micro_f1_108 = 0.67875 
macro_f1_108 = 0.6677415081975208 
minloss 1.1565149819850922
just saved the best current model in epoch106, with acc1:0.6768695067460194, and acc2:0.69375

Epoch - 109 Train-Loss : 0.29624658413231375

Epoch - 109 Valid-Loss : 1.1789932906627656
micro_f1_109 = 0.67875 
macro_f1_109 = 0.663879524223648 
minloss 1.1565149819850922
just saved the best current model in epoch106, with acc1:0.6768695067460194, and acc2:0.69375

Epoch - 110 Train-Loss : 0.27270978309214117

Epoch - 110 Valid-Loss : 1.189858981370926
micro_f1_110 = 0.68 
macro_f1_110 = 0.6627948150240581 
minloss 1.1565149819850922
just saved the best current model in epoch106, with acc1:0.6768695067460194, and acc2:0.69375

Epoch - 111 Train-Loss : 0.27779445838183164

Epoch - 111 Valid-Loss : 1.1652637416124343
micro_f1_111 = 0.6775 
macro_f1_111 = 0.6662233579057667 
minloss 1.1565149819850922
just saved the best current model in epoch106, with acc1:0.6768695067460194, and acc2:0.69375

Epoch - 112 Train-Loss : 0.268583558909595

Epoch - 112 Valid-Loss : 1.1876340329647064
micro_f1_112 = 0.6775 
macro_f1_112 = 0.661886933213609 
minloss 1.1565149819850922
just saved the best current model in epoch106, with acc1:0.6768695067460194, and acc2:0.69375

Epoch - 113 Train-Loss : 0.25933585047721863

Epoch - 113 Valid-Loss : 1.2255888962745667
micro_f1_113 = 0.69375 
macro_f1_113 = 0.6789273543545817 
minloss 1.1565149819850922
just saved the best current model in epoch113, with acc1:0.6789273543545817, and acc2:0.69375

Epoch - 114 Train-Loss : 0.2641181832551956

Epoch - 114 Valid-Loss : 1.2567928540706634
micro_f1_114 = 0.68375 
macro_f1_114 = 0.6668220214384533 
minloss 1.1565149819850922
training is terminating so as to prevent further overfitting
just saved the best current model in epoch113, with acc1:0.6789273543545817, and acc2:0.69375
(3, 4)
3
params = 3343378
Experiment's attempt changed to : 2


.......................................................................................................




.......................................................................................................


