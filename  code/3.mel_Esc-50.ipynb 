{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "esc3[kfold].ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2jjwWVAfK6JEtlpBeIkXk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pasquale90/mthesis/blob/master/%20code/3.mel_Esc-50.ipynb%20\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ytm7uEhUlgt"
      },
      "source": [
        "!pip uninstall librosa\n",
        "!pip install librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx7eK7RSgbb6"
      },
      "source": [
        "#prevent from disconnecting --> to console\n",
        "'''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "}setInterval(ClickConnect,60000)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te91VG5RQ_Cp"
      },
      "source": [
        "#Import Google_drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNri9FNoRLMd"
      },
      "source": [
        "#Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms, models\n",
        "from google.colab import files as filez"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lN1TsYQfWQV9"
      },
      "source": [
        "#define device\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device('cuda:0')\n",
        "else:\n",
        "  device=torch.device('cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5cFRYobRXP4"
      },
      "source": [
        "#define experiment mode\n",
        "class mode_class:\n",
        "  def __init__(self, mode_atr):\n",
        "    if (mode_atr == 80):\n",
        "      self.mode='80'\n",
        "    elif (mode_atr == 128):\n",
        "      self.mode = '128'\n",
        "    elif (mode_atr == 360):\n",
        "      self.mode = '360'\n",
        "    else:\n",
        "      print(f'{mode_atr} input attribute is not valid.Please insert 80 or 128 or even 360')\n",
        "  def get_mode(self):\n",
        "    return self.mode\n",
        "\n",
        "mode_id = 80\n",
        "#mode_id = 128\n",
        "mode_id = 360\n",
        "\n",
        "#define mode\n",
        "mode_instance = mode_class(mode_id)\n",
        "mode=mode_instance.get_mode()\n",
        "print('mode:',mode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVv3xCXipppZ"
      },
      "source": [
        "#define expid\n",
        "expid='esc3'\n",
        "\n",
        "#define paths\n",
        "data_path='/content/gdrive/My Drive/dissertation/ESC-50-master/meta/esc50.csv'\n",
        "audio_path='/content/gdrive/My Drive/dissertation/ESC-50-master/audio/'\n",
        "\n",
        "#model.py\t\t\n",
        "model_savepath= \"/content/\"+expid+mode+\"/saved_models/\"\n",
        "\n",
        "#store.py\n",
        "results_path=\"/content/\"+expid+mode+\"/results/\"\n",
        "compare_results_path =\"/content/\"+expid+mode+\"/k_fold_results/\"\t\n",
        "\n",
        "#attempt.py\n",
        "attempt_path=\"/content/\"+expid+mode+\"/expattempt/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEazv5VURdjc"
      },
      "source": [
        "#Import Dataset\n",
        "esc50 = pd.read_csv(data_path)\n",
        "audiofiles = os.listdir(audio_path)\n",
        "print(esc50.shape)\n",
        "print(len(audiofiles))\n",
        "\n",
        "#store_sorted_class_names, in the same way that are returned from dataset_class in data.py\n",
        "esc_classes = sorted(esc50['category'].unique())\n",
        "num_classes = len(esc_classes)\n",
        "print('num_classes: ',num_classes)\n",
        "\n",
        "folds = sorted(esc50['fold'].unique())\n",
        "print('folds: ',num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6je__0xBs4w"
      },
      "source": [
        "#AUDIO AUGMENTATION FUNCTIONS_ synthetic data\n",
        "def audio_augmentation(data, sr, class_conditional, shift_time=True, thresshold=0.5):\n",
        "\n",
        "  #add_white_noise to the signal\n",
        "  def white_noise(data): \n",
        "    noiz = np.random.randn(len(data))\n",
        "    mean_intensity = np.sum(np.square(data))/len(data)\n",
        "    data_wn = data + noiz*0.75* mean_intensity\n",
        "    return data_wn\n",
        "    \n",
        "  #Shift the sound wave by a factor value chosen randomly within [0.5,1,1.5,2] seconds\n",
        "  def time_shift(data,sr):\n",
        "    time_step = np.random.choice([sr//2,sr,sr+sr//2,sr*2])\n",
        "    time_shifted = np.roll(data,time_step)\n",
        "    return time_shifted\n",
        "\n",
        "  #Time-stretching the wave by a factor value of 0.9. Permissible : 0 < x < 1.0\n",
        "  def time_stretch(data):#,factor\n",
        "    factor = 0.90\n",
        "    time_streched = librosa.effects.time_stretch(data,factor)\n",
        "    return time_streched[0:len(data)]\n",
        "\n",
        "  #pitch shifting of wave by a random factor value in the space [-1,1].  Permissible : -5 <= x <= 5\n",
        "  def pitch_shift(data,sr):\n",
        "    detune = np.random.uniform(low=-2.5,high=-1.75,size=None)\n",
        "    overtune = np.random.uniform(low=1.75,high=2.5,size=None)\n",
        "    shift_factor = np.random.choice([detune,overtune])\n",
        "    pitch_shifted = librosa.effects.pitch_shift(data,sr,n_steps=shift_factor)\n",
        "    return pitch_shifted\n",
        "  \n",
        "  def soft_pitch_shift(data,sr):\n",
        "    detune = np.random.uniform(low=-1.0,high=-0.5,size=None)\n",
        "    overtune = np.random.uniform(low=0.5,high=1.0,size=None)\n",
        "    shift_factor = np.random.choice([detune,overtune])\n",
        "    pitch_shifted = librosa.effects.pitch_shift(data,sr,n_steps=shift_factor)\n",
        "    return pitch_shifted\n",
        "\n",
        "  '''\n",
        "  #A rough but very simple segmentation mask function\n",
        "  def envelope(y,rate,threshold):\n",
        "    mask = []\n",
        "    y= pd.Series(y).apply(np.abs)\n",
        "    y_mean = y.rolling(window=int(rate/10), min_periods=1, center=True).mean()#/10 means that win=1/10 sec\n",
        "    for mean in y_mean:\n",
        "      if mean > threshold:#if above the threshold keep\n",
        "        mask.append(True)\n",
        "      else:\n",
        "        mask.append(False)#else drop\n",
        "    return mask\n",
        "  '''\n",
        "  \n",
        "  strong_augs = ['airplane','car_horn','cat',#esc\n",
        "                 'chirping_birds','church_bells',\n",
        "                 'cow','crow','crying_baby',\n",
        "                 'door_wood_creaks','insects',\n",
        "                 'rooster','sheep','siren',\n",
        "                 'car_horn','children_playing','siren']#us8k\n",
        "\n",
        "  medium_augs= ['breathing','brushing_teeth',#esc\n",
        "                'clock_alarm','coughing','dog',\n",
        "                'door_wood_knock','fireworks',\n",
        "                'frog','glass_breaking','hand_saw',\n",
        "                'hen','laughing','pig','pouring_water',\n",
        "                'sneezing','snoring',\n",
        "                'dog_bark','gun_shot','street_music']#us8k\n",
        "                \n",
        "  weak_augs = ['can_opening','chainsaw','clapping',#esc\n",
        "               'clock_tick','crackling_fire','crickets',\n",
        "               'drinking_sipping','engine','footsteps',\n",
        "               'helicopter','keyboard_typing','mouse_click',\n",
        "               'rain','sea_waves','thunderstorm',\n",
        "              'toilet_flush','train','vacuum_cleaner',\n",
        "              'washing_machine','water_drops','wind',  \n",
        "               'air_conditioner','drilling','engine_idling','jackhammer']#us8k\n",
        "   \n",
        "  #prob_wn = np.random.uniform(low=0,high=1)\n",
        "  #if prob_wn>thresshold:\n",
        "  data =  white_noise(data)\n",
        "  \n",
        "  #prob_tsh = np.random.uniform(low=0,high=1)\n",
        "  #if prob_tsh>thresshold:\n",
        "\n",
        "  # if it is padded , don 't shift\n",
        "  if shift_time: \n",
        "    data = time_shift(data,sr)\n",
        "\n",
        "  if class_conditional in strong_augs:\n",
        "    \n",
        "    prob_tst = np.random.uniform(low=0,high=1)\n",
        "    if prob_tst>thresshold:\n",
        "      data = time_stretch(data)\n",
        "\n",
        "    prob_psh = np.random.uniform(low=0,high=1)\n",
        "    if prob_psh>thresshold:\n",
        "      data = pitch_shift(data,sr)\n",
        "  \n",
        "  elif class_conditional in medium_augs:\n",
        "    \n",
        "    prob_spsh = np.random.uniform(low=0,high=1)\n",
        "    if prob_spsh>thresshold:\n",
        "      data = soft_pitch_shift(data,sr)\n",
        "\n",
        "  elif class_conditional in weak_augs:\n",
        "    pass\n",
        "\n",
        "  return data\n",
        "\n",
        "# Data augmentation for training data\n",
        "train_transforms = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.RandomErasing(p=0.5, scale = (0.05,0.05), ratio = (0.3,0.33), value=0, inplace=False)      \n",
        "    ])\n",
        "valid_transforms = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "#Vision Augmentations -  Horizontal Flip, Random Erasing\n",
        "def vision_augmentations(data,transform):\n",
        "    augs = []\n",
        "    for file in data:\n",
        "      image = Image.fromarray(file)\n",
        "      image = transform(image)\n",
        "      augs.append(image)\n",
        "    return augs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQsX_S0xiZmu"
      },
      "source": [
        "def analysis_parameters(mode):\n",
        "  sampling_rate=44100\n",
        "  hop_length=512\n",
        "  fft_points=2048\n",
        "  mel_bands=mode  #80x321 or 128x321\n",
        "  return sampling_rate, hop_length, fft_points, mel_bands\n",
        "  \n",
        "def zero_pad(signal,fs):\n",
        "  shift_time = True\n",
        "  if len(signal)>(4*fs):\n",
        "    signal = signal[0:4*fs]\n",
        "  elif len(signal)<(4*fs):\n",
        "    shift_time = False\n",
        "  num_zeros=4*fs-len(signal)\n",
        "  zp=np.zeros(num_zeros,dtype=float)\n",
        "  padded_signal = np.concatenate((signal,zp),axis=0)\n",
        "  return padded_signal, shift_time\n",
        "\n",
        "#extract features\n",
        "def extract_mel_spectogram(audio_path, df, folds, audiofiles, sr, hop, nfft, nmels):   \n",
        "\n",
        "  def compute_mel_spectogram(raw,sr,hop,nfft,nmels,window='hann'):\n",
        "    S=librosa.feature.melspectrogram(y=raw.astype(float),\n",
        "                                      sr=sr,S=None,\n",
        "                                      n_fft=nfft,\n",
        "                                      hop_length=hop,\n",
        "                                      window=window, \n",
        "                                      power=2,\n",
        "                                      n_mels=nmels) \n",
        "    S = librosa.power_to_db(S, ref=np.max)\n",
        "    return S\n",
        "\n",
        "  def scale_image(spec, eps=1e-6):\n",
        "    mean = spec.mean()\n",
        "    std = spec.std()\n",
        "    spec_norm = (spec - mean) / (std + eps)\n",
        "    spec_min, spec_max = spec_norm.min(), spec_norm.max()\n",
        "    spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n",
        "    spec_scaled = spec_scaled.astype(np.uint8)\n",
        "    return spec_scaled\n",
        "  \n",
        "  features, labels, folders = [], [], []  \n",
        " \n",
        "  extr = True\n",
        "  if extr == True:\n",
        "\n",
        "    shape_print=True\n",
        "\n",
        "    #deterministic random augmentation\n",
        "    np.random.seed(3)\n",
        "\n",
        "    for count_files, file in tqdm(enumerate(audiofiles)):\n",
        "      #label = int(df.loc[df['filename']==file]['target'].to_string(index=False))\n",
        "      #folder = int(df.loc[df['filename']==file]['fold'].to_string(index=False))\n",
        "      name = file.split('.wav')[0]\n",
        "      label = int(file.split('-')[-1].split('.')[0])\n",
        "      folder = int(file.split('-')[0])\n",
        "      #print('name', name)\n",
        "      #print('folder',folder)\n",
        "      #print('label',label) \n",
        "\n",
        "\n",
        "      raw, _ =librosa.load(audio_path+file, sr=sr, mono=True)\n",
        "      #print(f'{file} had length {len(raw)}')\n",
        "\n",
        "      #extract mel_spectogram\n",
        "      S = compute_mel_spectogram(raw,sr,hop,nfft,nmels)\n",
        "      #print(S.shape)\n",
        "\n",
        "      #flip image\n",
        "      flipped = np.flipud(S)\n",
        "\n",
        "      #to gray scale\n",
        "      greyscale = scale_image(flipped)\n",
        "      \n",
        "      features.append(greyscale)\n",
        "      labels.append(label)\n",
        "      folders.append(folder)\n",
        "\n",
        "      #Synthetic data augmentations\n",
        "      '''Probability of a file being augmented is 100% so as to increase to the double the size of the ESC dataset,and to prevent from imbalanced class distribution issue\n",
        "      #fprob = np.random.uniform(low=0,high=1)\n",
        "      #if fprob>0.75:\n",
        "      '''\n",
        "      category = df.loc[df['filename']==file]['category'].to_string(index=False).lstrip()\n",
        "      augmented = audio_augmentation(data=raw,sr=sr,class_conditional=category,shift_time=False, thresshold=0.5)\n",
        "      synthetic = scale_image(np.flipud(compute_mel_spectogram(augmented,sr,hop,nfft,nmels)))\n",
        "\n",
        "      features.append(synthetic)\n",
        "      labels.append(label)\n",
        "      folders.append(folder)\n",
        "\n",
        "      if shape_print:\n",
        "          print('\\nFeature Shape Check\\n')\n",
        "          print(f'raw had len:{len(raw)/sr}')\n",
        "          print(f'Spectogram has shape : {S.shape} with min:{greyscale.min()} and max:{greyscale.max()}')\n",
        "          shape_print = False\n",
        "\n",
        "  #'''\n",
        "  print('len(features)-features',len(features))\n",
        "  print('len(features[0])-freq_domain',len(features[0]))\n",
        "  print('len(features[0][0])-time_domain',len(features[0][0]))\n",
        "  print('labels',len(labels))\n",
        "  print('folders',len(folders))\n",
        "  #'''\n",
        "  return features, labels, folders"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8brhXU0qkYy9"
      },
      "source": [
        "#get analysis parameters\n",
        "sampling_rate, hop_length, fft_points, mel_bands = analysis_parameters(mode)\n",
        "print(f'sampling_rate: {sampling_rate}, hop_length: {hop_length}, fft_points: {fft_points}, mel_bands: {mel_bands}')\n",
        "\n",
        "#extract_features\n",
        "features, labels, folders = extract_mel_spectogram(audio_path,esc50,folds,audiofiles,sampling_rate,hop_length,fft_points,mel_bands)\n",
        "print(f' feature\\'s len : {len(features)}, labels : {len(labels)}, folders : {len(folders)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpxdfqdpkbS8"
      },
      "source": [
        "#train and fold definition\n",
        "\n",
        "def k_fold(vfold,folds):\n",
        "  train_folds = [folds.index(fold)+1 for i,fold in enumerate(folds) if vfold!=folds.index(fold)+1]\n",
        "  #train_folds = [folds.index(fold)+1 for i,fold in enumerate(folds) if (vfold-1)==folds.index(fold)+1]#for testing\n",
        "  valid_folds = [folds.index(fold)+1 for i,fold in enumerate(folds) if vfold==folds.index(fold)+1]\n",
        "  print('train_folds: ',train_folds)\n",
        "  print('valid_folds: ',valid_folds)\n",
        "  return train_folds,valid_folds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_6CbjJ6kpMj"
      },
      "source": [
        "class Data(Dataset): \n",
        "  def __init__(self, features,labels,folders,split,transforms):\n",
        "\n",
        "    print('Loading_features......')\n",
        "\n",
        "    #features, labels, folders\n",
        "    self.indexes = [i for i, val in enumerate(folders) if val in split]\n",
        "    self.data = [features[x] for x in self.indexes]\n",
        "    self.labels = [labels[x] for x in self.indexes]\n",
        "\n",
        "    #normalize\n",
        "    self.data = np.asarray(self.data, dtype=np.float32)/255.0\n",
        " \n",
        "    #image transforms\n",
        "    self.data = vision_augmentations(self.data,transforms)\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  def __getitem__(self, idx):#load data on demand\n",
        "    return self.data[idx], self.labels[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSVwmjpiaxtx"
      },
      "source": [
        "from time import time\n",
        "def reload_data(features,labels,folders,train_folds,train_transforms,valid_transforms):\n",
        "  def pause(secs):\n",
        "    init_time = time()\n",
        "    print(f'paused for {secs} seconds')\n",
        "    while time() < init_time+secs: \n",
        "      pass\n",
        "  \n",
        "  #load spectograms\n",
        "  train_data = Data(features,labels,folders,train_folds,train_transforms)\n",
        "  pause(10)\n",
        "  valid_data = Data(features,labels,folders,valid_folds,valid_transforms)\n",
        "  print('features are loaded') \n",
        "  pause(20)\n",
        "\n",
        "  batch_size = 16\t\n",
        "  print(f'batch_size: {batch_size}')\n",
        "\n",
        "  #data iterator\n",
        "  train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "  valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=False) \n",
        "  return train_loader, valid_loader, batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlnnML9Uk20r"
      },
      "source": [
        "class M2Dcnn2(nn.Module):\n",
        "  def __init__(self, num_cats):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, kernel_size = 3, stride=1, padding=1 )\n",
        "    self.bn1 = nn.BatchNorm2d(32)\n",
        "    self.conv2 = nn.Conv2d(32, 32, kernel_size = 3, stride=1, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(32)\n",
        "    self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.bn3 = nn.BatchNorm2d(64)\n",
        "    self.conv4 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "    self.bn4 = nn.BatchNorm2d(64)\n",
        "    self.conv5 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "    self.bn5 = nn.BatchNorm2d(128)\n",
        "    self.conv6 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "    self.bn6 = nn.BatchNorm2d(128)\n",
        "    self.conv7 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "    self.bn7 = nn.BatchNorm2d(256)\n",
        "    self.conv8 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "    self.bn8 = nn.BatchNorm2d(256)\n",
        "    self.ap = nn.AdaptiveAvgPool2d((256,10))\n",
        "    self.dense1 = nn.Linear(256*10,512)\n",
        "    self.dropout = nn.Dropout(0.25)\n",
        "    self.dense2 = nn.Linear(512, 128)\n",
        "    self.dropout = nn.Dropout(0.25)\n",
        "    self.dense3 = nn.Linear(128, num_cats)\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(self.bn1(x))\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(self.bn2(x)) \n",
        "    x = F.max_pool2d(x, kernel_size=2) \n",
        "    x = self.conv3(x)\n",
        "    x = F.relu(self.bn3(x))\n",
        "    x = self.conv4(x)\n",
        "    x = F.relu(self.bn4(x))\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "    x = self.conv5(x)\n",
        "    x = F.relu(self.bn5(x))\n",
        "    x = self.conv6(x)\n",
        "    x = F.relu(self.bn6(x))\n",
        "    x = F.max_pool2d(x, kernel_size=2)\n",
        "    x = self.conv7(x)\n",
        "    x = F.relu(self.bn7(x))\n",
        "    x = self.conv8(x)\n",
        "    x = F.relu(self.bn8(x))\n",
        "    x = F.max_pool2d(x, kernel_size=3)\n",
        "    x = x.view(x.size(0),x.size(1),x.size(2)*x.size(3))\n",
        "    x = self.ap(x)\n",
        "    x = x.view(x.size(0),-1)\n",
        "    x = F.relu(self.dense1(x))\n",
        "    x = self.dropout(x)\n",
        "    x = self.dense2(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.dense3(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GN6gB11a9As"
      },
      "source": [
        "def reinitialize_model(num_classes):\n",
        "\n",
        "  #introduce reproducibility\n",
        "  seed = 777\n",
        "  torch.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "\n",
        "  #init model\n",
        "  #model =  Mcnn2(num_cats=num_classes).cuda()#to(device)\n",
        "  model =  M2Dcnn2(num_cats=num_classes).to(device)\n",
        "  modelid='M2Dcnn2'\n",
        "  total_params = sum(p.numel() for p in model.parameters())\n",
        "  print(f'model_{modelid} initialized with total : {total_params} parameters.')\n",
        "\n",
        "  #define loss and optimizer\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  learning_rate = 1e-5\n",
        "  epochs = 150\n",
        "  print(f' learning_rate = {learning_rate}, epochs = {epochs}')\n",
        "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  return model, loss_fn, optimizer, learning_rate, epochs, modelid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVpmKsoPk9pb"
      },
      "source": [
        "#define metrics\n",
        "def F1_score(trace_y, trace_yhat, classes):\n",
        "  num_classes = len(classes)\n",
        "  #confussion matrix\n",
        "  confmat = confusion_matrix(trace_y, trace_yhat, num_classes)\n",
        "    \n",
        "  TP=pd.Series(confmat.to_numpy().diagonal())\n",
        "  FP=confmat.sum(axis=0)-TP\n",
        "  FN=confmat.sum(axis=1)-TP\n",
        "\n",
        "  TN=confmat.sum().sum()-TP-FP-FN\n",
        "  \n",
        "  #micro\n",
        "  micro_precision = TP.sum()/(TP.sum()+FP.sum())\n",
        "  micro_recall = TP.sum()/(TP.sum()+FN.sum())\n",
        "  F1_micro=2*micro_precision*micro_recall/(micro_precision+micro_recall)\n",
        "\n",
        "  #macro\n",
        "  macro_precision = pd.Series(np.nan)\n",
        "  macro_recall= pd.Series(np.nan)\n",
        "  macro_f1 = pd.Series(np.nan)\n",
        "  #Avoid Zero-Division\n",
        "  for i in range(num_classes):\n",
        "    if (TP[i]==0 and FP[i]==0):\n",
        "      macro_precision[i]=0\n",
        "    else:\n",
        "      macro_precision[i] = TP[i]/(TP[i]+FP[i]) \n",
        "    if (TP[i]==0 and FN[i]==0):\n",
        "      macro_recall[i]=0\n",
        "    else:\n",
        "      macro_recall[i] = TP[i]/(TP[i]+FN[i])\n",
        "    \n",
        "    if (macro_precision[i]+macro_recall[i]==0.0):\n",
        "      macro_f1[i]=0\n",
        "    else:\n",
        "      macro_f1[i] = 2*macro_precision[i]*macro_recall[i]/(macro_precision[i]+macro_recall[i])\n",
        "      \n",
        "  macro_precision = macro_precision.sum()/num_classes\n",
        "  macro_recall=macro_recall.sum()/num_classes\n",
        "  F1_macro = macro_f1.sum()/num_classes\n",
        "\n",
        "  return micro_recall,micro_precision,F1_micro, macro_recall,macro_precision,F1_macro\n",
        "  \n",
        "#labels, preds\n",
        "def confusion_matrix(trace_y,trace_yhat, num_classes):\n",
        "  confmat=pd.DataFrame(data=np.zeros(shape=(num_classes,num_classes)))\n",
        "  predictions=trace_yhat.argmax(axis=1)\n",
        "\n",
        "  for i,pred in enumerate(predictions):\n",
        "    confmat.iat[trace_y[i],pred]+=1\n",
        "\n",
        "  return confmat\n",
        "\n",
        "\n",
        "def F1_Class(trace_y,trace_yhat,classes):\n",
        "  num_classes = len(classes)\n",
        "  confmat = confusion_matrix(trace_y, trace_yhat, num_classes)\n",
        "  \n",
        "  TP=pd.Series(confmat.to_numpy().diagonal())\n",
        "  FP=confmat.sum(axis=0)-TP\n",
        "  FN=confmat.sum(axis=1)-TP\n",
        "  TN=confmat.sum().sum()-TP-FP-FN\n",
        "\n",
        "  #AVOID ZERO DIVISION\n",
        "  class_precision = pd.Series(np.nan)\n",
        "  class_recall = pd.Series(np.nan)\n",
        "  class_f1 = pd.Series(np.nan)\n",
        "  for i in range(num_classes):\n",
        "    if (TP[i]==0 and FP[i]==0):\n",
        "      class_precision[i]=0\n",
        "    else:\n",
        "      class_precision[i] = TP[i]/(TP[i]+FP[i]) \n",
        "    if (TP[i]==0 and FN[i]==0):\n",
        "      class_recall[i]=0\n",
        "    else:\n",
        "      class_recall[i] = TP[i]/(TP[i]+FN[i])\n",
        "    \n",
        "    if (class_precision[i]==0.0 and class_recall[i]==0.0):\n",
        "      class_f1[i]=0\n",
        "    else:\n",
        "      class_f1[i] = 2*class_precision[i]*class_recall[i]/(class_precision[i]+class_recall[i])\n",
        "    \n",
        "  #create a dict_report\n",
        "  f1_class_report = {}\n",
        "  class_report = {}\n",
        "  class_counts = np.asarray(np.unique(trace_y, return_counts=True)).T\n",
        "\n",
        "  for i,c in enumerate(classes):\n",
        "    f1_class_report[c] = {}\n",
        "    f1_class_report[c]['precision'] = class_precision[i]\n",
        "    f1_class_report[c]['recall'] = class_recall[i]\n",
        "    f1_class_report[c]['f1'] = class_f1[i]\n",
        "    f1_class_report[c]['count'] = class_counts[i][1]\n",
        "\n",
        "  return f1_class_report\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "import sklearn\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "#Keep a backup record of the performance meassures\n",
        "def backup_metrics(labels,preds,classes,path):\n",
        "    if (not os.path.exists(path)):\n",
        "      os.makedirs(path)\n",
        "    report = classification_report(labels, preds.argmax(1), target_names=classes)\n",
        "    report_data = []\n",
        "    lines = report.split('\\n')\n",
        "    for line in lines[2:-5]:\n",
        "        row = {}\n",
        "        row_data = ' '.join(line.split())   \n",
        "        row_data = row_data.split(' ')\n",
        "        row['class'] = row_data[0]\n",
        "        row['precision'] = float(row_data[1])\n",
        "        row['recall'] = float(row_data[2])\n",
        "        row['f1_score'] = float(row_data[3])\n",
        "        row['support'] = float(row_data[4])\n",
        "        report_data.append(row)\n",
        "    dataframe = pd.DataFrame.from_dict(report_data)\n",
        "    dataframe.to_csv(path+'backup_classification_report.csv',index=False)\n",
        "\n",
        "    accuracy= f1_score(labels, preds.argmax(1), average='micro', zero_division='warn')\n",
        "    macro_avg= f1_score(labels, preds.argmax(1), average='macro', zero_division='warn')\n",
        "    weightedf1= f1_score(labels, preds.argmax(1), average='weighted', zero_division='warn')\n",
        "\n",
        "    precision,recall,_,_ =precision_recall_fscore_support(labels, preds.argmax(1), average='macro')\n",
        "    general = pd.DataFrame(data = [accuracy,recall,precision,macro_avg,weightedf1],index=['accuracy','recall','precision','macro_f1','weightedf1'])\n",
        "    general.to_csv(path+'backup_general.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZS1lX0rk_n6"
      },
      "source": [
        "#prevent overfitting custom method\n",
        "class prevent_overfitting:\n",
        "  def __init__(self):\n",
        "    self.tolerance=7  #how many epochs tolerance\n",
        "    self.minloss = 10 \n",
        "    self.lossenvelope =[]\n",
        "    self.thresshold = 0.5\n",
        "    self.avgperformances=[]\n",
        "    self.best_epoch = 0\n",
        "    self.micro_accuracies = []\n",
        "    self.macro_accuracies = []\n",
        "    self.best_curr_model = False#None #model.state_dict save\n",
        "  \n",
        "  #check overfitting by observing 3 last epoch's valid loss\n",
        "  def detect_overfitting(self,validloss,epoch,console_path):\n",
        "    self.lossenvelope.append(validloss)\n",
        "    if validloss<self.minloss:\n",
        "      self.minloss=validloss\n",
        "    print('minloss',self.minloss)\n",
        "    print(f'minloss : {self.minloss}', file=open(console_path, \"a\"))\n",
        "\n",
        "    if (len(self.lossenvelope)>self.tolerance):\n",
        "      self.lossenvelope.pop(0) #reject first value when {tolerance} values are passed\n",
        "\n",
        "    if(len(self.lossenvelope)>=self.tolerance):\n",
        "      overfit=all(earlier <= later for earlier, later in zip(self.lossenvelope, self.lossenvelope[-5:]))#check if ascending\n",
        "      if (overfit and min(self.lossenvelope)>self.minloss):\n",
        "        return True\n",
        "    return False\n",
        "  \n",
        "  #store best model's results by observing mean micro and macro accuracies\n",
        "  def store_best_model(self,micro_accuracy,macro_accuracy,console_path):\n",
        "    self.macro_accuracies.append(micro_accuracy)\n",
        "    self.micro_accuracies.append(macro_accuracy)\n",
        "    \n",
        "    meanaccuracy = (micro_accuracy+macro_accuracy)/2.0\n",
        "    self.avgperformances.append(meanaccuracy)\n",
        "    \n",
        "    self.best_epoch = self.avgperformances.index(max(self.avgperformances))+1\n",
        "    \n",
        "    if meanaccuracy >= self.thresshold:\n",
        "      self.best_curr_model = (meanaccuracy >= max(self.avgperformances))\n",
        "      print(f'just saved the best current model in epoch{self.best_epoch}, with acc1:{self.micro_accuracies[self.best_epoch-1]}, and acc2:{self.macro_accuracies[self.best_epoch-1]}')\n",
        "      print(f'just saved the best current model in epoch{self.best_epoch}, with acc1:{self.micro_accuracies[self.best_epoch-1]}, and acc2:{self.macro_accuracies[self.best_epoch-1]}', file=open(console_path, \"a\"))\n",
        "    else:\n",
        "      self.best_curr_model = False\n",
        "\n",
        "    return self.best_curr_model, self.micro_accuracies[self.best_epoch-1],self.macro_accuracies[self.best_epoch-1]#, self.best_epoch\n",
        "  \n",
        "  def early_stopping(self,epochinst):\n",
        "    print('training is terminating so as to prevent further overfitting')\n",
        "    total_epochs = epochinst.set_total(epochinst.get_step())\n",
        "    return total_epochs\n",
        "\n",
        "class epochs_class:\n",
        "  def __init__(self):\n",
        "    self.total_epochs=50#random value\n",
        "    self.step_epoch=1\n",
        "  def set_total(self,num_epochs):\n",
        "    self.total_epochs=num_epochs\n",
        "    return self.get_total()\n",
        "  def get_total(self):\n",
        "    return self.total_epochs\n",
        "  def next_step(self):\n",
        "    self.step_epoch+=1\n",
        "    return self.step_epoch\n",
        "  def get_step(self):\n",
        "    return self.step_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTiCHepzlDcV"
      },
      "source": [
        "#save results\n",
        "def save_results(content, name):\n",
        "  path = results_path\n",
        "  if (not os.path.exists(path)):\n",
        "    os.mkdir(path)\n",
        "  ovr_results, class_results = content\n",
        "  ovr_results_filename, class_results_filename = name \n",
        "  \n",
        "  ovr_results.to_csv(path+ovr_results_filename) \n",
        "  class_results.to_csv(path + class_results_filename) \n",
        "   \n",
        "#func to define the last argument in the save_results method\n",
        "#PATTERN:dataset&exp_mode_validationfold_attempt_metrics.csv\n",
        "def define_filenames_pattern(expid, mode, vfold, attempt):\n",
        "  classf1_report_filename = expid+'_'+mode+'_v'+str(vfold)+'_a'+str(attempt)+'_classF1.csv'\n",
        "  ovr_results_filename=expid+'_'+mode+'_v'+str(vfold)+'_a'+str(attempt)+'_overalF1.csv'\n",
        "  return ovr_results_filename , classf1_report_filename\n",
        "  \n",
        "  \n",
        "#as the first argument in save_results\n",
        "def define_content(epochs,\n",
        "\t\t               mean_train_losses, \n",
        "                   mean_valid_losses, \n",
        "                   microrecall,\n",
        "                   microprecision,\n",
        "                   microf1, \n",
        "                   macrorecall,\n",
        "                   macroprecision,\n",
        "                   macrof1, \n",
        "\t\t               classf1):\n",
        "                   \n",
        "  #data = [mean_train_losses, mean_valid_losses, accuracies,micro_auroc, macro_auroc microf1, macrof1]#auroc,\n",
        "  data = {'mean_train_loss' : mean_train_losses, 'mean_valid_loss' : mean_valid_losses, \n",
        "  'micro_recall':microrecall, 'micro_precision': microprecision, 'micro_f1' : microf1,\n",
        "  'macro_recall':macrorecall, 'macro_precision': macroprecision, 'macro_f1' : macrof1}\n",
        "  \n",
        "  #index names\n",
        "  epochs_index= (['epoch_'+str(ep+1) for ep in range(epochs)])\n",
        "  \n",
        "  overal_results = pd.DataFrame(data=data, index=epochs_index,  dtype=np.float16)\n",
        "  class_results = pd.DataFrame(data=classf1, index=epochs_index,  dtype=np.float16)\n",
        "  \n",
        "  overal_results.index.name = 'epochs'\n",
        "  class_results.index.name = 'epochs'\n",
        "  \n",
        "  return overal_results, class_results\n",
        "\n",
        " \n",
        "#save general results so as to compare with other systems in a different folder\n",
        "def save_genres(micro, macro, params, validation_fold, filename):\n",
        "  \n",
        "  path = compare_results_path\n",
        "\n",
        "  #if csv exists, overwrite results\n",
        "  if os.path.isfile(path+filename):\n",
        "    general_results=pd.read_csv(path+filename)\n",
        "    general_results.set_index('validation_fold:',inplace=True)\n",
        "    #debug\n",
        "    print(general_results.shape)\n",
        "    new_fold_results = [micro, macro, params]\n",
        "    #debug\n",
        "    print(len(new_fold_results))\n",
        "    general_results[validation_fold] = new_fold_results\n",
        "    general_results.to_csv(path+filename)\n",
        "  #if not, store them into a new csv file\n",
        "  else:\n",
        "    os.makedirs(path)\n",
        "    data = {validation_fold : [micro,macro,params]}\n",
        "    general_results = pd.DataFrame(data=data, index=['micro_f1','macro_f1','params'], dtype=np.float16)\n",
        "    general_results.index.name = 'validation_fold:'\n",
        "    general_results.to_csv(path+filename) \n",
        "    print(general_results)\n",
        "\n",
        "\n",
        "def genres_filename(expid,mode):\n",
        "  filename=expid+'_'+str(mode)+'.csv'\n",
        "  return filename\n",
        "\n",
        "#save the model\n",
        "#PATTERN:exp-num_mode_dataset_'model'_attempt\t\t\n",
        "def save_model(model_name, vfold, state_dict):\n",
        "  path = model_savepath\n",
        "  if not os.path.exists(path):\n",
        "    os.mkdir(path)\n",
        "  torch.save(state_dict, path + model_name+'_'+str(vfold)+'.pt')\n",
        "'''\n",
        "#load model\n",
        "def load_model(model_name, mode):\n",
        "  if (mode=='80'):\n",
        "    model = esc_mel_model_hybrid(input_shape=(1,80,431), batch_size=16, num_cats=50)\n",
        "  elif(mode=='128'):\n",
        "    model = esc_mel_model_hybrid(input_shape=(1,128,431), batch_size=16, num_cats=50)\n",
        "  model.load_state_dict(torch.load(paths.model_savepath+model_name+'.pth'))\n",
        "'''\n",
        "\n",
        "def ZipAnDownload(mode):\n",
        "  if mode == '80':\n",
        "    !zip -r /content/esc380.zip /content/esc380\n",
        "    filez.download(\"/content/esc380.zip\")\n",
        "  elif mode == '128':\n",
        "    !zip -r /content/esc3128.zip /content/esc3128\n",
        "    filez.download(\"/content/esc3128.zip\")\n",
        "  elif mode == '360':\n",
        "    !zip -r /content/esc3360.zip /content/esc3360\n",
        "    filez.download(\"/content/esc3360.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOQwP5BMlMHN"
      },
      "source": [
        "class attempt_class:\n",
        "  #init_attempt\n",
        "  def __init__(self,mode,vfold):\n",
        "\n",
        "    self.attempt_path = attempt_path\n",
        "    self.attempt_file = attempt_path+\"attempts_\"+mode+\"_\"+str(vfold)+\".txt\"\n",
        "\n",
        "    if (not os.path.exists(self.attempt_path)):\n",
        "      os.makedirs(self.attempt_path)\n",
        "      self.init_files()\n",
        "    elif (not os.path.isfile(self.attempt_file)):\n",
        "      self.init_files()\n",
        "    else:\n",
        "      print(f'attempt is already initialized')\n",
        "      print(f'Experiment\\'s _{mode} attempt no_ : {self.get_attempt()}')\n",
        "  \n",
        "  def init_files(self):\n",
        "    attempt = 1\n",
        "    f = open(self.attempt_file,'w')\n",
        "    with open(self.attempt_file, 'a') as out:\n",
        "      out.write(str(attempt))\n",
        "      print(f'Experiment\\'s attempt no_ : {attempt}')\n",
        "  \n",
        "  def add_attempt(self):\n",
        "    with open(self.attempt_file, \"r\") as f:\n",
        "      attempt = f.read()\n",
        "      attempt = int(attempt)\n",
        "      attempt+=1\n",
        "    with open(self.attempt_file, \"w\") as f:\n",
        "      f.write(str(attempt))\n",
        "    print(f'Experiment\\'s attempt changed to : {attempt}')\n",
        "\n",
        "  def get_attempt(self):\n",
        "    with open(self.attempt_file, \"r\") as f:\n",
        "      attempt = int(f.read())\n",
        "    return attempt\n",
        "\n",
        "  def set_attempt(self,attempt_value):\n",
        "    with open(self.attempt_file, \"w\") as f:\n",
        "      f.write(str(attempt_value))\n",
        "    print(f'Experiment\\'s attempt is set to : {attempt_value}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icGXe-XclPYe"
      },
      "source": [
        "#train_model_func\n",
        "def train(model, loss_fn, train_loader, valid_loader,\n",
        "          epochs, optimizer, learning_rate, device, classes, expid, mode, vfold, modelid):\n",
        "\n",
        "  exp_attempt = attempt_class(mode, vfold)\n",
        "  \n",
        "  print('Train started..')\n",
        "\n",
        "  train_losses = []\n",
        "  valid_losses = []\n",
        "  \n",
        "  microrecall = []\n",
        "  macrorecall = []\n",
        "  \n",
        "  microprecision = []\n",
        "  macroprecision = []\n",
        "  \n",
        "  microf1 = []\n",
        "  macrof1 = []\n",
        "  \n",
        "  classf1 = []\n",
        "  \n",
        "  #console print redirect - save output of train to a log_file\n",
        "  console_path = \"/content/\"+expid+mode+'/'+expid+'_'+str(mode)+'_'+str(vfold)+\".txt\"\n",
        "  \n",
        "  epoch_instance = epochs_class()\n",
        "  total_epochs = epoch_instance.set_total(epochs)\n",
        "  epoch = epoch_instance.get_step()\n",
        "\n",
        "  prevent_overfit = prevent_overfitting()\n",
        "  \n",
        "  model_name = expid+'_'+mode+'_v'+str(vfold)+'_a'+str(exp_attempt.get_attempt())+'_'+modelid\n",
        "  \n",
        "  #Train step\n",
        "  while (epoch<=total_epochs):\n",
        "\n",
        "    model.train()\n",
        "    batch_losses=[]\n",
        "\n",
        "    for i,data in tqdm(enumerate(train_loader)):\n",
        "      x, y = data\n",
        "      optimizer.zero_grad()\n",
        "      x = x.to(device, dtype=torch.float32)\n",
        "      y = y.to(device, dtype=torch.long) \n",
        "      y_hat = model(x) \n",
        "      loss = loss_fn(y_hat, y)\n",
        "      loss.backward()\n",
        "      batch_losses.append(loss.item())\t\t\t\t\t\t\n",
        "      optimizer.step()\n",
        "    train_losses.append(batch_losses)\n",
        "    mean_train_losses=([np.mean(l) for l in train_losses])\n",
        "    \n",
        "    print()\n",
        "    print(f'Epoch - {epoch} Train-Loss : {np.mean(train_losses[-1])}')\n",
        "    print(f'\\nEpoch - {epoch} Train-Loss : {np.mean(train_losses[-1])}\\n', file=open(console_path, \"a\"))\n",
        "    print()\n",
        "    \n",
        "    #Validation step\n",
        "    model.eval()\n",
        "     \n",
        "    batch_losses=[]\n",
        "    trace_y = []\n",
        "    trace_yhat = []\n",
        "    \n",
        "    for i, data in enumerate(valid_loader):\n",
        "      x, y = data\n",
        "      x = x.to(device, dtype=torch.float32)\n",
        "      y = y.to(device, dtype=torch.long)\n",
        "      y_hat = model(x)\n",
        "      loss = loss_fn(y_hat, y)\n",
        "      trace_y.append(y.cpu().detach().numpy())\n",
        "      trace_yhat.append(y_hat.cpu().detach().numpy())      \n",
        "      batch_losses.append(loss.item())\n",
        "    valid_losses.append(batch_losses)\n",
        "    mean_valid_losses=([np.mean(l) for l in valid_losses])\n",
        "    \n",
        "    trace_y = np.concatenate(trace_y)\n",
        "    trace_yhat = np.concatenate(trace_yhat)\n",
        "\n",
        "    #f1,micro,macro\n",
        "    micro_recall,micro_precision,micro, macro_recall,macro_precision,macro = F1_score(trace_y,trace_yhat,classes)\n",
        "    \n",
        "    #just to check\n",
        "    print('\\n')\n",
        "    print(precision_recall_fscore_support(trace_y, trace_yhat.argmax(1), average='macro'))\n",
        "    print('\\n')\n",
        "\n",
        "    microrecall.append(micro_recall)\n",
        "    microprecision.append(micro_precision)\n",
        "    microf1.append(micro) \n",
        "\n",
        "    macrorecall.append(macro_recall)\n",
        "    macroprecision.append(macro_precision)\n",
        "    macrof1.append(macro)\n",
        "    \n",
        "    #f1 for each class\n",
        "    f1_class = F1_Class(trace_y,trace_yhat,classes)\n",
        "    classf1.append(f1_class)\n",
        "\n",
        "    #console prints\n",
        "    print(f'Epoch - {epoch} Valid-Loss : {np.mean(valid_losses[-1])}')\n",
        "    print(f'micro_f1_{epoch} = {micro} ')\n",
        "    print(f'macro_f1_{epoch} = {macro} ')\n",
        "    #append log_file\n",
        "    print(f'Epoch - {epoch} Valid-Loss : {np.mean(valid_losses[-1])}', file=open(console_path, \"a\"))\n",
        "    print(f'micro_f1_{epoch} = {micro} ', file=open(console_path, \"a\"))\n",
        "    print(f'macro_f1_{epoch} = {macro} ', file=open(console_path, \"a\"))\n",
        "    '''\n",
        "    #check if overfitting and reduce lr twice, after that, in case loss is ascending terminate train    \n",
        "    early_stop, optimizer = overfit.detect_overfitting(np.mean(valid_losses[-1]),optimizer, learning_rate, epoch_instance)\n",
        "    if early_stop:\n",
        "      total_epochs=epoch_instance.set_total(epoch_instance.get_step())\n",
        "    epoch=epoch_instance.next_step()\n",
        "    '''\n",
        "    #check if overfitting and reduce lr twice, after that, in case loss is ascending terminate train    \n",
        "    early_stop = prevent_overfit.detect_overfitting(np.mean(valid_losses[-1]), epoch, console_path)\n",
        "    if early_stop:\n",
        "      total_epochs=prevent_overfit.early_stopping(epoch_instance)\n",
        "    \n",
        "    #always store the best according to avg_micro_and_macro_F1\n",
        "    current_best, best_micro, best_macro = prevent_overfit.store_best_model(micro,macro,console_path)#, best_epoch\n",
        "    #If achieves current best mean accuracy, Save the model\n",
        "    if current_best:\n",
        "      save_model(model_name, vfold, model.state_dict())\n",
        "      backup_metrics(trace_y,trace_yhat,classes,results_path+'backup/')\n",
        "    #next epoch\n",
        "    epoch=epoch_instance.next_step()\n",
        "  \n",
        "  #calc_trainable_params\n",
        "  #params=sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "  params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "      \n",
        "  #Save analytic results \n",
        "  save_results(define_content(total_epochs,\n",
        "\t\t                                     mean_train_losses,\n",
        "                                         mean_valid_losses, \n",
        "                                         microrecall,\n",
        "                                         microprecision,\n",
        "                                         microf1, \n",
        "                                         macrorecall,\n",
        "                                         macroprecision,\n",
        "                                         macrof1,\n",
        "\t\t                                     classf1),\n",
        "                      define_filenames_pattern(expid, mode, vfold, exp_attempt.get_attempt()))\n",
        "  \n",
        "      \n",
        "  #Save general results so as to quickly compare systems\n",
        "  genresfilename = genres_filename(expid,mode)\n",
        "  save_genres(round(best_micro,3), round(best_macro,3), params, vfold, genresfilename)\n",
        "  print('params =', params)\n",
        "  print('params =', params, file=open(console_path, \"a\"))\n",
        "\n",
        "  exp_attempt.add_attempt()\n",
        "  return  train_losses,valid_losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3Cjb9EtnT64"
      },
      "source": [
        "#k fold training proccedure\n",
        "for vfold in folds:\n",
        "  train_folds,valid_folds = k_fold(vfold,folds)\n",
        "  train_loader, valid_loader, batch_size = reload_data(features,labels,folders,train_folds,train_transforms,valid_transforms)\n",
        "  model, loss_fn, optimizer, learning_rate, epochs, modelid = reinitialize_model(num_classes)\n",
        "  train_losses,valid_losses = train(model, loss_fn, train_loader, valid_loader, epochs, optimizer, learning_rate, device, esc_classes, expid, mode, vfold, modelid)\n",
        "  \n",
        "ZipAnDownload(mode) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrLROLuectdU"
      },
      "source": [
        "if mode =='80':\n",
        "  !rm -r esc380 esc380.zip\n",
        "if mode =='128':\n",
        "  !rm -r esc3128 esc3128.zip"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}